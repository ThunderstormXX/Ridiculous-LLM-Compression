{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43141e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user31/.conda/envs/tinyllama-env/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb57bad29eaf4ae89da3ff8de30beb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "# 1. Инициализация модели и токенизатора\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device\n",
    ")\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "# 2. Подготовка 20 образцов MMLU (dev)\n",
    "ds = load_dataset(\"cais/mmlu\", \"all\",split=\"dev\")\n",
    "smpl = ds.shuffle(seed=42).select(range(20))\n",
    "texts = [\n",
    "    item[\"question\"] + \" Варианты: \" + \" \".join(item[\"choices\"])\n",
    "    for item in smpl\n",
    "]\n",
    "inputs = tokenizer(\n",
    "    texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa282d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_ids'] = inputs['input_ids'].to(device)\n",
    "inputs['attention_mask'] = inputs['attention_mask'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fbea21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем скрытые состояния до каждого слоя\n",
    "with torch.no_grad():\n",
    "    out = model(**inputs, output_hidden_states=True)\n",
    "hidden_states = out.hidden_states  # список длины num_layers+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "885c2242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17ee81d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class LlamaDecoderLayer(GradientCheckpointingLayer):\n",
      "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
      "        super().__init__()\n",
      "        self.hidden_size = config.hidden_size\n",
      "\n",
      "        self.self_attn = LlamaAttention(config=config, layer_idx=layer_idx)\n",
      "\n",
      "        self.mlp = LlamaMLP(config)\n",
      "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
      "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Cache] = None,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n",
      "        **kwargs: Unpack[FlashAttentionKwargs],\n",
      "    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.input_layernorm(hidden_states)\n",
      "\n",
      "        # Self Attention\n",
      "        hidden_states, self_attn_weights = self.self_attn(\n",
      "            hidden_states=hidden_states,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_value=past_key_value,\n",
      "            output_attentions=output_attentions,\n",
      "            use_cache=use_cache,\n",
      "            cache_position=cache_position,\n",
      "            position_embeddings=position_embeddings,\n",
      "            **kwargs,\n",
      "        )\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        # Fully Connected\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "        hidden_states = self.mlp(hidden_states)\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        outputs = (hidden_states,)\n",
      "        if output_attentions:\n",
      "            outputs += (self_attn_weights,)\n",
      "\n",
      "        return outputs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getsource(type(model.model.layers[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9faca922",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "inputs['attention_mask'] = torch.tril(torch.ones(100, 100, dtype=bool, device=device))\n",
    "\n",
    "num_layers = model.config.num_hidden_layers\n",
    "num_heads = model.config.num_attention_heads\n",
    "head_dim = model.config.hidden_size // num_heads\n",
    "layer_idx = num_layers // 2  # средний слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf65869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = torch.arange(\n",
    "    0, 100, dtype = torch.long, device = device\n",
    ")\n",
    "\n",
    "position_embeddings = model.model.rotary_emb(inputs['input_ids'], position_ids[None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c3dad42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaDecoderLayer(\n",
       "  (self_attn): LlamaAttention(\n",
       "    (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "    (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "    (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "  )\n",
       "  (mlp): LlamaMLP(\n",
       "    (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "    (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "    (act_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6540bdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed prompt: Explain the theory of relativity\n",
      "Number of logged operations: 1\n",
      "Input hidden states shape: torch.Size([1, 8, 3072])\n",
      "Output hidden states shape: torch.Size([1, 8, 3072])\n",
      "Processed prompt: What is the capital of France?\n",
      "Number of logged operations: 1\n",
      "Input hidden states shape: torch.Size([1, 8, 3072])\n",
      "Output hidden states shape: torch.Size([1, 8, 3072])\n",
      "Processed prompt: How to bake a chocolate cake?\n",
      "Number of logged operations: 1\n",
      "Input hidden states shape: torch.Size([1, 8, 3072])\n",
      "Output hidden states shape: torch.Size([1, 8, 3072])\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# 3. Implementing the logging function for layer inspection\n",
    "forward_func = deepcopy(model.model.layers[11].forward)  # Copy original forward method of layer 11\n",
    "logger = []\n",
    "\n",
    "def new_forward(*args, **kwargs):\n",
    "    # Log inputs\n",
    "    layer_inputs = {\n",
    "        'hidden_states': args[0] if len(args) > 0 else kwargs.get('hidden_states'),\n",
    "        'attention_mask': kwargs.get('attention_mask'),\n",
    "        'position_ids': kwargs.get('position_ids'),\n",
    "        'past_key_value': kwargs.get('past_key_value'),\n",
    "        'output_attentions': kwargs.get('output_attentions'),\n",
    "        'use_cache': kwargs.get('use_cache')\n",
    "    }\n",
    "    logger.append({'inputs': layer_inputs})\n",
    "    \n",
    "    # Call original forward\n",
    "    outputs = forward_func(*args, **kwargs)\n",
    "    \n",
    "    # Log outputs\n",
    "    layer_outputs = {\n",
    "        'hidden_states': outputs[0],\n",
    "        'attentions': outputs[1] if len(outputs) > 1 else None,\n",
    "        'past_key_value': outputs[2] if len(outputs) > 2 else None\n",
    "    }\n",
    "    logger[-1]['outputs'] = layer_outputs\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Replace the forward method of layer 11 with our logging version\n",
    "model.model.layers[11].forward = new_forward\n",
    "\n",
    "# 4. Process prompts and collect logs\n",
    "prompts = [\n",
    "    \"Explain the theory of relativity\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"How to bake a chocolate cake?\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # You can now analyze the logger content\n",
    "    print(f\"Processed prompt: {prompt}\")\n",
    "    print(f\"Number of logged operations: {len(logger)}\")\n",
    "    \n",
    "    # Example: Print input/output shapes for first logged operation\n",
    "    if logger:\n",
    "        first_log = logger[0]\n",
    "        print(\"Input hidden states shape:\", first_log['inputs']['hidden_states'].shape)\n",
    "        print(\"Output hidden states shape:\", first_log['outputs']['hidden_states'].shape)\n",
    "    \n",
    "    # Clear logger for next prompt\n",
    "    logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f095a5d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlayer_inputs\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layer_inputs' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b862acc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[43mlogger\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m log\u001b[38;5;241m.\u001b[39mkeys()\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "log = logger[0]\n",
    "log.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b4f7e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlog\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m], log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124matt_nmaks\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log' is not defined"
     ]
    }
   ],
   "source": [
    "log['input'], log['att_nmaks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "243f508d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 100, 3072])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = hidden_states[0]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c38c00bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:290\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:235\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    233\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 235\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[1;32m    236\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "model.model.layers[10].forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.model.layers\n",
    "    arr.append(layer.self_attn.q k v )\n",
    "\n",
    "\n",
    "q , k, v --> listq listk listv \n",
    "\n",
    "arrq, arrk,arrv \n",
    "\n",
    "arrq.shape = (num_prompts, num_heads, H,W)\n",
    "\n",
    "arrq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e080d173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b610fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_head_repr_forward (layer_idx, head_idx, position_embeddings, hidden_states, inputs, model):\n",
    "    \"\"\"\n",
    "    Извлекаем представление заданной головы через forward self-attn одного слоя.\n",
    "    \n",
    "    Args:\n",
    "        layer_idx: Индекс слоя\n",
    "        head_idx: Индекс головы\n",
    "        position_embeddings: Позиционные эмбеддинги\n",
    "        hidden_states: Скрытые состояния модели\n",
    "        inputs: Входные данные (содержит attention_mask)\n",
    "        model: Модель трансформера\n",
    "    \n",
    "    Returns:\n",
    "        Векторное представление головы [hidden_size]\n",
    "    \"\"\"\n",
    "    # Получаем модуль слоя\n",
    "    layer = model.model.layers[layer_idx]\n",
    "    inp = hidden_states[layer_idx]  # [batch, seq, hidden_size]\n",
    "    \n",
    "    # Сохраняем оригинальные веса\n",
    "    orig_q_weight = layer.self_attn.q_proj.weight.data.clone()\n",
    "    orig_k_weight = layer.self_attn.k_proj.weight.data.clone()\n",
    "    orig_v_weight = layer.self_attn.v_proj.weight.data.clone()\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            num_heads = 24\n",
    "            head_dim = 128\n",
    "\n",
    "            # Создаем маску для выбранной головы\n",
    "            mask = torch.zeros(num_heads, device= model.device)\n",
    "            mask[head_idx] = 1.0  \n",
    "\n",
    "            # Применяем маски к проекционным матрицам\n",
    "            q_mask = mask.repeat_interleave(head_dim)\n",
    "            print(mask.shape, q_mask.shape, layer.self_attn.q_proj.weight.data.shape)\n",
    "            layer.self_attn.q_proj.weight.data *= q_mask\n",
    "\n",
    "            k_head_dim = layer.self_attn.k_proj.weight.shape[0] // num_heads\n",
    "            v_head_dim = layer.self_attn.v_proj.weight.shape[0] // num_heads\n",
    "            \n",
    "            k_mask = mask.repeat_interleave(k_head_dim)\n",
    "            v_mask = mask.repeat_interleave(v_head_dim)\n",
    "\n",
    "            layer.self_attn.k_proj.weight.data *= k_mask\n",
    "            layer.self_attn.v_proj.weight.data *= v_mask\n",
    "\n",
    "            # Forward pass через self-attention\n",
    "            attn_out = layer.self_attn(\n",
    "                inp,\n",
    "                attention_mask=inputs.get(\"attention_mask\"), \n",
    "                position_embeddings=position_embeddings, \n",
    "            )[0]\n",
    "\n",
    "            # Усредняем по батчу и последовательности\n",
    "            rep = attn_out.mean(dim=(0,1))  # [hidden_size]\n",
    "            \n",
    "    finally:\n",
    "        # Восстанавливаем оригинальные веса\n",
    "        layer.self_attn.q_proj.weight.data.copy_(orig_q_weight)\n",
    "        layer.self_attn.k_proj.weight.data.copy_(orig_k_weight)\n",
    "        layer.self_attn.v_proj.weight.data.copy_(orig_v_weight)\n",
    "\n",
    "    return rep.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "067db239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class LlamaAttention(nn.Module):\n",
      "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
      "\n",
      "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
      "        super().__init__()\n",
      "        self.config = config\n",
      "        self.layer_idx = layer_idx\n",
      "        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
      "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
      "        self.scaling = self.head_dim**-0.5\n",
      "        self.attention_dropout = config.attention_dropout\n",
      "        self.is_causal = True\n",
      "\n",
      "        self.q_proj = nn.Linear(\n",
      "            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n",
      "        )\n",
      "        self.k_proj = nn.Linear(\n",
      "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
      "        )\n",
      "        self.v_proj = nn.Linear(\n",
      "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
      "        )\n",
      "        self.o_proj = nn.Linear(\n",
      "            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n",
      "        )\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
      "        attention_mask: Optional[torch.Tensor],\n",
      "        past_key_value: Optional[Cache] = None,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "        **kwargs: Unpack[FlashAttentionKwargs],\n",
      "    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n",
      "        input_shape = hidden_states.shape[:-1]\n",
      "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
      "\n",
      "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "\n",
      "        cos, sin = position_embeddings\n",
      "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
      "\n",
      "        if past_key_value is not None:\n",
      "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
      "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
      "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
      "\n",
      "        attention_interface: Callable = eager_attention_forward\n",
      "        if self.config._attn_implementation != \"eager\":\n",
      "            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
      "\n",
      "        attn_output, attn_weights = attention_interface(\n",
      "            self,\n",
      "            query_states,\n",
      "            key_states,\n",
      "            value_states,\n",
      "            attention_mask,\n",
      "            dropout=0.0 if not self.training else self.attention_dropout,\n",
      "            scaling=self.scaling,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
      "        attn_output = self.o_proj(attn_output)\n",
      "        return attn_output, attn_weights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getsource(type(model.model.layers[0].self_attn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef105f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing layers:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24]) torch.Size([3072]) torch.Size([3072, 3072])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3072) must match the size of tensor b (1008) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mli\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: extracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreps\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m heads with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreps\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mli\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m li \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(layers, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzing layers\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# Извлекаем представления всех голов\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m         reps \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([\n\u001b[1;32m      9\u001b[0m             extract_head_repr_forward \n\u001b[1;32m     10\u001b[0m             (layer_idx, \n\u001b[1;32m     11\u001b[0m              head_idx, \n\u001b[1;32m     12\u001b[0m              position_embeddings, \n\u001b[1;32m     13\u001b[0m              hidden_states, \n\u001b[1;32m     14\u001b[0m              inputs, \n\u001b[1;32m     15\u001b[0m              model)\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m head_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_heads)\n\u001b[1;32m     17\u001b[0m         ], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m         layer_reprs[li] \u001b[38;5;241m=\u001b[39m reps\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mli\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: extracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreps\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m heads with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreps\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m li \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(layers, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzing layers\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# Извлекаем представления всех голов\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         reps \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([\n\u001b[0;32m----> 9\u001b[0m             \u001b[43mextract_head_repr_forward\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m             \u001b[49m\u001b[43mhead_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m             \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m             \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m             \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m             \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m head_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_heads)\n\u001b[1;32m     17\u001b[0m         ], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m         layer_reprs[li] \u001b[38;5;241m=\u001b[39m reps\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mli\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: extracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreps\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m heads with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreps\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 45\u001b[0m, in \u001b[0;36mextract_head_repr_forward\u001b[0;34m(layer_idx, head_idx, position_embeddings, hidden_states, inputs, model)\u001b[0m\n\u001b[1;32m     42\u001b[0m k_mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mrepeat_interleave(k_head_dim)\n\u001b[1;32m     43\u001b[0m v_mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mrepeat_interleave(v_head_dim)\n\u001b[0;32m---> 45\u001b[0m layer\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mk_proj\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m k_mask\n\u001b[1;32m     46\u001b[0m layer\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mv_proj\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m v_mask\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Forward pass через self-attention\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3072) must match the size of tensor b (1008) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "layers = [0, num_layers//4, num_layers//2, 3*num_layers//4, num_layers-1]\n",
    "layer_reprs = {}\n",
    "\n",
    "for li in tqdm.tqdm(layers, desc=\"Analyzing layers\"):\n",
    "    try:\n",
    "        # Извлекаем представления всех голов\n",
    "        reps = np.stack([\n",
    "            extract_head_repr_forward \n",
    "            (layer_idx, \n",
    "             head_idx, \n",
    "             position_embeddings, \n",
    "             hidden_states, \n",
    "             inputs, \n",
    "             model)\n",
    "            for head_idx in range(num_heads)\n",
    "        ], axis=0)\n",
    "        \n",
    "        layer_reprs[li] = reps\n",
    "        print(f\"Layer {li}: extracted {reps.shape[0]} heads with dim {reps.shape[1]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        print(f\"Error in layer {li}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f47e583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ab9fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m layer_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Анализируем 6-й слой\u001b[39;00m\n\u001b[1;32m      2\u001b[0m head_representations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(\u001b[38;5;28mrange\u001b[39m(num_heads), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mИзвлечение голов слоя \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      5\u001b[0m     head_repr \u001b[38;5;241m=\u001b[39m extract_head_repr(\n\u001b[1;32m      6\u001b[0m         layer_idx\u001b[38;5;241m=\u001b[39mlayer_idx,\n\u001b[1;32m      7\u001b[0m         head_idx\u001b[38;5;241m=\u001b[39mh,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel\n\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m     head_representations\u001b[38;5;241m.\u001b[39mappend(head_repr)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd9de373",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reprs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m norms \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mreprs\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reprs' is not defined"
     ]
    }
   ],
   "source": [
    "norms = np.linalg.norm(reprs, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869d74a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Косинус и L2\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m norms \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(reprs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m cos_sim \u001b[38;5;241m=\u001b[39m (reprs \u001b[38;5;241m@\u001b[39m reprs\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m/\u001b[39m (norms \u001b[38;5;241m@\u001b[39m norms\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m      6\u001b[0m l2_dist \u001b[38;5;241m=\u001b[39m squareform(pdist(reprs, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Косинус и L2\n",
    "\n",
    "norms = np.linalg.norm(reprs, axis=1, keepdims=True)\n",
    "cos_sim = (reprs @ reprs.T) / (norms @ norms.T)\n",
    "\n",
    "l2_dist = squareform(pdist(reprs, metric='euclidean'))\n",
    "\n",
    "# Find similar pairs by threshold\n",
    "threshold = 0.95\n",
    "pairs = []\n",
    "for i in range(num_heads):\n",
    "    for j in range(i+1, num_heads):\n",
    "        if cos_sim[i,j] >= threshold:\n",
    "            pairs.append((i, j, float(cos_sim[i,j]), float(l2_dist[i,j])))\n",
    "\n",
    "pairs_sorted = sorted(pairs, key=lambda x: -x[2])\n",
    "print(f\"Found {len(pairs_sorted)} pairs with cos >= {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b19cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция визуализации\n",
    "def visualize_layer_attention(layer_idx):\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        out_att = model(**inputs, output_attentions=True).attentions[layer_idx]\n",
    "    avg_att = out_att.mean(dim=0).cpu().numpy()  # [heads, seq, seq]\n",
    "\n",
    "    buttons = []\n",
    "    for h in range(avg_att.shape[0]):\n",
    "        btn = widgets.Button(description=f\"Head {h}\", layout=widgets.Layout(width='80px'))\n",
    "        def on_click_factory(mat, li, idx):\n",
    "            def on_click(_):\n",
    "                plt.figure(figsize=(6,5))\n",
    "                sns.heatmap(mat[idx], cmap='magma')\n",
    "                plt.title(f\"Layer {li}, Head {idx}\")\n",
    "                plt.show()\n",
    "            return on_click\n",
    "        btn.on_click(on_click_factory(avg_att, layer_idx, h))\n",
    "        buttons.append(btn)\n",
    "\n",
    "    display(widgets.VBox([widgets.Label(f\"Layer {layer_idx}\"), widgets.HBox(buttons)]))\n",
    "\n",
    "# Вызов визуализации\n",
    "visualize_layer_attention(layer_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyllama-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
