{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 79.368,
  "eval_steps": 500,
  "global_step": 5000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0,
      "iterations": 0,
      "loss": 1.4032502174377441,
      "perplexity": 4.06840181350708,
      "step": 0
    },
    {
      "epoch": 0,
      "iterations": 0,
      "loss": 1.4691506624221802,
      "perplexity": 4.345542907714844,
      "step": 0
    },
    {
      "epoch": 0,
      "iterations": 0,
      "loss": 1.4027290344238281,
      "perplexity": 4.066281795501709,
      "step": 0
    },
    {
      "epoch": 0,
      "iterations": 0,
      "loss": 1.262155532836914,
      "perplexity": 3.5330286026000977,
      "step": 0
    },
    {
      "epoch": 0,
      "iterations": 0,
      "loss": 1.3832377195358276,
      "perplexity": 3.9877917766571045,
      "step": 0
    },
    {
      "epoch": 0,
      "iterations": 0,
      "loss": 1.4425297975540161,
      "perplexity": 4.231386661529541,
      "step": 0
    },
    {
      "epoch": 0,
      "iterations": 0,
      "loss": 1.3477307558059692,
      "perplexity": 3.848681926727295,
      "step": 0
    },
    {
      "epoch": 0,
      "iterations": 0,
      "loss": 1.4627671241760254,
      "perplexity": 4.3178911209106445,
      "step": 0
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.9428915977478027,
      "learning_rate": 9.8e-05,
      "loss": 10.1112,
      "step": 50
    },
    {
      "epoch": 0.8,
      "iterations": 50,
      "loss": 1.4251652956008911,
      "perplexity": 4.158545017242432,
      "step": 50
    },
    {
      "epoch": 0.8,
      "iterations": 50,
      "loss": 1.3074774742126465,
      "perplexity": 3.6968367099761963,
      "step": 50
    },
    {
      "epoch": 0.8,
      "iterations": 50,
      "loss": 0.8762142062187195,
      "perplexity": 2.401789903640747,
      "step": 50
    },
    {
      "epoch": 0.8,
      "iterations": 50,
      "loss": 0.901378870010376,
      "perplexity": 2.4629969596862793,
      "step": 50
    },
    {
      "epoch": 0.8,
      "iterations": 50,
      "loss": 1.4407836198806763,
      "perplexity": 4.224004745483398,
      "step": 50
    },
    {
      "epoch": 0.8,
      "iterations": 50,
      "loss": 1.395168662071228,
      "perplexity": 4.035655498504639,
      "step": 50
    },
    {
      "epoch": 0.8,
      "iterations": 50,
      "loss": 1.394025444984436,
      "perplexity": 4.0310444831848145,
      "step": 50
    },
    {
      "epoch": 0.8,
      "iterations": 50,
      "loss": 0.9518380165100098,
      "perplexity": 2.5904667377471924,
      "step": 50
    },
    {
      "epoch": 1.592,
      "grad_norm": 1.5788218975067139,
      "learning_rate": 0.00019800000000000002,
      "loss": 9.4717,
      "step": 100
    },
    {
      "epoch": 1.592,
      "iterations": 100,
      "loss": 1.1725084781646729,
      "perplexity": 3.2300848960876465,
      "step": 100
    },
    {
      "epoch": 1.592,
      "iterations": 100,
      "loss": 1.4004818201065063,
      "perplexity": 4.057154178619385,
      "step": 100
    },
    {
      "epoch": 1.592,
      "iterations": 100,
      "loss": 1.3764948844909668,
      "perplexity": 3.9609932899475098,
      "step": 100
    },
    {
      "epoch": 1.592,
      "iterations": 100,
      "loss": 0.8800183534622192,
      "perplexity": 2.4109439849853516,
      "step": 100
    },
    {
      "epoch": 1.592,
      "iterations": 100,
      "loss": 0.8788101673126221,
      "perplexity": 2.4080328941345215,
      "step": 100
    },
    {
      "epoch": 1.592,
      "iterations": 100,
      "loss": 1.3055717945098877,
      "perplexity": 3.689798355102539,
      "step": 100
    },
    {
      "epoch": 1.592,
      "iterations": 100,
      "loss": 1.3799082040786743,
      "perplexity": 3.974536657333374,
      "step": 100
    },
    {
      "epoch": 1.592,
      "iterations": 100,
      "loss": 0.7987353801727295,
      "perplexity": 2.2227282524108887,
      "step": 100
    },
    {
      "epoch": 2.384,
      "grad_norm": 1.1847991943359375,
      "learning_rate": 0.00019800000000000002,
      "loss": 9.2286,
      "step": 150
    },
    {
      "epoch": 2.384,
      "iterations": 150,
      "loss": 1.3214707374572754,
      "perplexity": 3.7489309310913086,
      "step": 150
    },
    {
      "epoch": 2.384,
      "iterations": 150,
      "loss": 0.88714200258255,
      "perplexity": 2.428179979324341,
      "step": 150
    },
    {
      "epoch": 2.384,
      "iterations": 150,
      "loss": 1.337649941444397,
      "perplexity": 3.8100790977478027,
      "step": 150
    },
    {
      "epoch": 2.384,
      "iterations": 150,
      "loss": 1.409626841545105,
      "perplexity": 4.094427108764648,
      "step": 150
    },
    {
      "epoch": 2.384,
      "iterations": 150,
      "loss": 1.3922141790390015,
      "perplexity": 4.023749351501465,
      "step": 150
    },
    {
      "epoch": 2.384,
      "iterations": 150,
      "loss": 0.8939285278320312,
      "perplexity": 2.4447150230407715,
      "step": 150
    },
    {
      "epoch": 2.384,
      "iterations": 150,
      "loss": 1.3028721809387207,
      "perplexity": 3.6798505783081055,
      "step": 150
    },
    {
      "epoch": 2.384,
      "iterations": 150,
      "loss": 1.3709322214126587,
      "perplexity": 3.939021110534668,
      "step": 150
    },
    {
      "epoch": 3.176,
      "grad_norm": 2.5830702781677246,
      "learning_rate": 0.0001959591836734694,
      "loss": 9.2839,
      "step": 200
    },
    {
      "epoch": 3.176,
      "iterations": 200,
      "loss": 1.16782546043396,
      "perplexity": 3.214993953704834,
      "step": 200
    },
    {
      "epoch": 3.176,
      "iterations": 200,
      "loss": 0.8032423853874207,
      "perplexity": 2.2327685356140137,
      "step": 200
    },
    {
      "epoch": 3.176,
      "iterations": 200,
      "loss": 1.040866494178772,
      "perplexity": 2.831669330596924,
      "step": 200
    },
    {
      "epoch": 3.176,
      "iterations": 200,
      "loss": 0.9615656137466431,
      "perplexity": 2.615788698196411,
      "step": 200
    },
    {
      "epoch": 3.176,
      "iterations": 200,
      "loss": 1.331108808517456,
      "perplexity": 3.785238265991211,
      "step": 200
    },
    {
      "epoch": 3.176,
      "iterations": 200,
      "loss": 1.1138900518417358,
      "perplexity": 3.04618501663208,
      "step": 200
    },
    {
      "epoch": 3.176,
      "iterations": 200,
      "loss": 0.8619638085365295,
      "perplexity": 2.3678061962127686,
      "step": 200
    },
    {
      "epoch": 3.176,
      "iterations": 200,
      "loss": 1.423330545425415,
      "perplexity": 4.1509222984313965,
      "step": 200
    },
    {
      "epoch": 3.976,
      "grad_norm": 1.6937227249145508,
      "learning_rate": 0.00019391836734693877,
      "loss": 9.1585,
      "step": 250
    },
    {
      "epoch": 3.976,
      "iterations": 250,
      "loss": 1.2161505222320557,
      "perplexity": 3.374173879623413,
      "step": 250
    },
    {
      "epoch": 3.976,
      "iterations": 250,
      "loss": 1.3435673713684082,
      "perplexity": 3.8326919078826904,
      "step": 250
    },
    {
      "epoch": 3.976,
      "iterations": 250,
      "loss": 1.4348334074020386,
      "perplexity": 4.19894552230835,
      "step": 250
    },
    {
      "epoch": 3.976,
      "iterations": 250,
      "loss": 1.1811083555221558,
      "perplexity": 3.2579832077026367,
      "step": 250
    },
    {
      "epoch": 3.976,
      "iterations": 250,
      "loss": 1.0591586828231812,
      "perplexity": 2.883943796157837,
      "step": 250
    },
    {
      "epoch": 3.976,
      "iterations": 250,
      "loss": 0.772737443447113,
      "perplexity": 2.16568660736084,
      "step": 250
    },
    {
      "epoch": 3.976,
      "iterations": 250,
      "loss": 1.1530611515045166,
      "perplexity": 3.1678755283355713,
      "step": 250
    },
    {
      "epoch": 3.976,
      "iterations": 250,
      "loss": 1.427342176437378,
      "perplexity": 4.16760778427124,
      "step": 250
    },
    {
      "epoch": 4.768,
      "grad_norm": 2.8105711936950684,
      "learning_rate": 0.00019187755102040817,
      "loss": 9.1003,
      "step": 300
    },
    {
      "epoch": 4.768,
      "iterations": 300,
      "loss": 0.9503795504570007,
      "perplexity": 2.586691379547119,
      "step": 300
    },
    {
      "epoch": 4.768,
      "iterations": 300,
      "loss": 1.3432626724243164,
      "perplexity": 3.831524133682251,
      "step": 300
    },
    {
      "epoch": 4.768,
      "iterations": 300,
      "loss": 1.3342747688293457,
      "perplexity": 3.797240972518921,
      "step": 300
    },
    {
      "epoch": 4.768,
      "iterations": 300,
      "loss": 1.332563042640686,
      "perplexity": 3.7907466888427734,
      "step": 300
    },
    {
      "epoch": 4.768,
      "iterations": 300,
      "loss": 1.236220121383667,
      "perplexity": 3.4425764083862305,
      "step": 300
    },
    {
      "epoch": 4.768,
      "iterations": 300,
      "loss": 1.3707280158996582,
      "perplexity": 3.9382164478302,
      "step": 300
    },
    {
      "epoch": 4.768,
      "iterations": 300,
      "loss": 1.2078652381896973,
      "perplexity": 3.3463332653045654,
      "step": 300
    },
    {
      "epoch": 4.768,
      "iterations": 300,
      "loss": 1.4296709299087524,
      "perplexity": 4.1773247718811035,
      "step": 300
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 1.3073209524154663,
      "learning_rate": 0.00018983673469387754,
      "loss": 9.1435,
      "step": 350
    },
    {
      "epoch": 5.5600000000000005,
      "iterations": 350,
      "loss": 0.8283052444458008,
      "perplexity": 2.289435386657715,
      "step": 350
    },
    {
      "epoch": 5.5600000000000005,
      "iterations": 350,
      "loss": 1.0442367792129517,
      "perplexity": 2.841229200363159,
      "step": 350
    },
    {
      "epoch": 5.5600000000000005,
      "iterations": 350,
      "loss": 1.247394323348999,
      "perplexity": 3.481260061264038,
      "step": 350
    },
    {
      "epoch": 5.5600000000000005,
      "iterations": 350,
      "loss": 0.8966443538665771,
      "perplexity": 2.4513635635375977,
      "step": 350
    },
    {
      "epoch": 5.5600000000000005,
      "iterations": 350,
      "loss": 0.8414362668991089,
      "perplexity": 2.3196961879730225,
      "step": 350
    },
    {
      "epoch": 5.5600000000000005,
      "iterations": 350,
      "loss": 1.3503040075302124,
      "perplexity": 3.858598470687866,
      "step": 350
    },
    {
      "epoch": 5.5600000000000005,
      "iterations": 350,
      "loss": 0.9196962118148804,
      "perplexity": 2.508528232574463,
      "step": 350
    },
    {
      "epoch": 5.5600000000000005,
      "iterations": 350,
      "loss": 1.003558874130249,
      "perplexity": 2.727973222732544,
      "step": 350
    },
    {
      "epoch": 6.352,
      "grad_norm": 1.801879644393921,
      "learning_rate": 0.00018779591836734695,
      "loss": 8.9344,
      "step": 400
    },
    {
      "epoch": 6.352,
      "iterations": 400,
      "loss": 0.9890929460525513,
      "perplexity": 2.6887943744659424,
      "step": 400
    },
    {
      "epoch": 6.352,
      "iterations": 400,
      "loss": 0.8257430195808411,
      "perplexity": 2.283576726913452,
      "step": 400
    },
    {
      "epoch": 6.352,
      "iterations": 400,
      "loss": 1.4353569746017456,
      "perplexity": 4.201144218444824,
      "step": 400
    },
    {
      "epoch": 6.352,
      "iterations": 400,
      "loss": 1.1903674602508545,
      "perplexity": 3.2882893085479736,
      "step": 400
    },
    {
      "epoch": 6.352,
      "iterations": 400,
      "loss": 0.7985779047012329,
      "perplexity": 2.2223782539367676,
      "step": 400
    },
    {
      "epoch": 6.352,
      "iterations": 400,
      "loss": 0.8403620719909668,
      "perplexity": 2.3172059059143066,
      "step": 400
    },
    {
      "epoch": 6.352,
      "iterations": 400,
      "loss": 0.8841666579246521,
      "perplexity": 2.4209659099578857,
      "step": 400
    },
    {
      "epoch": 6.352,
      "iterations": 400,
      "loss": 0.8775992393493652,
      "perplexity": 2.405118703842163,
      "step": 400
    },
    {
      "epoch": 7.144,
      "grad_norm": 2.713587522506714,
      "learning_rate": 0.00018575510204081635,
      "loss": 8.9931,
      "step": 450
    },
    {
      "epoch": 7.144,
      "iterations": 450,
      "loss": 0.8650326728820801,
      "perplexity": 2.3750836849212646,
      "step": 450
    },
    {
      "epoch": 7.144,
      "iterations": 450,
      "loss": 1.3560400009155273,
      "perplexity": 3.8807947635650635,
      "step": 450
    },
    {
      "epoch": 7.144,
      "iterations": 450,
      "loss": 1.416202425956726,
      "perplexity": 4.121439456939697,
      "step": 450
    },
    {
      "epoch": 7.144,
      "iterations": 450,
      "loss": 1.276135802268982,
      "perplexity": 3.582768440246582,
      "step": 450
    },
    {
      "epoch": 7.144,
      "iterations": 450,
      "loss": 1.3602948188781738,
      "perplexity": 3.8973422050476074,
      "step": 450
    },
    {
      "epoch": 7.144,
      "iterations": 450,
      "loss": 1.3991254568099976,
      "perplexity": 4.051654815673828,
      "step": 450
    },
    {
      "epoch": 7.144,
      "iterations": 450,
      "loss": 1.1452172994613647,
      "perplexity": 3.1431243419647217,
      "step": 450
    },
    {
      "epoch": 7.144,
      "iterations": 450,
      "loss": 1.1776387691497803,
      "perplexity": 3.2466988563537598,
      "step": 450
    },
    {
      "epoch": 7.944,
      "grad_norm": 1.1739039421081543,
      "learning_rate": 0.00018371428571428572,
      "loss": 8.8888,
      "step": 500
    },
    {
      "epoch": 7.944,
      "iterations": 500,
      "loss": 1.4411710500717163,
      "perplexity": 4.225641250610352,
      "step": 500
    },
    {
      "epoch": 7.944,
      "iterations": 500,
      "loss": 1.2748039960861206,
      "perplexity": 3.5779998302459717,
      "step": 500
    },
    {
      "epoch": 7.944,
      "iterations": 500,
      "loss": 0.8663200736045837,
      "perplexity": 2.378143310546875,
      "step": 500
    },
    {
      "epoch": 7.944,
      "iterations": 500,
      "loss": 1.2449281215667725,
      "perplexity": 3.4726853370666504,
      "step": 500
    },
    {
      "epoch": 7.944,
      "iterations": 500,
      "loss": 0.8130440711975098,
      "perplexity": 2.254761219024658,
      "step": 500
    },
    {
      "epoch": 7.944,
      "iterations": 500,
      "loss": 1.3498209714889526,
      "perplexity": 3.8567349910736084,
      "step": 500
    },
    {
      "epoch": 7.944,
      "iterations": 500,
      "loss": 0.8517717123031616,
      "perplexity": 2.3437957763671875,
      "step": 500
    },
    {
      "epoch": 7.944,
      "iterations": 500,
      "loss": 1.296932339668274,
      "perplexity": 3.658057689666748,
      "step": 500
    },
    {
      "epoch": 8.736,
      "grad_norm": 1.5328782796859741,
      "learning_rate": 0.00018167346938775513,
      "loss": 8.8102,
      "step": 550
    },
    {
      "epoch": 8.736,
      "iterations": 550,
      "loss": 0.7592331767082214,
      "perplexity": 2.1366372108459473,
      "step": 550
    },
    {
      "epoch": 8.736,
      "iterations": 550,
      "loss": 1.323154091835022,
      "perplexity": 3.755247116088867,
      "step": 550
    },
    {
      "epoch": 8.736,
      "iterations": 550,
      "loss": 1.3303738832473755,
      "perplexity": 3.7824573516845703,
      "step": 550
    },
    {
      "epoch": 8.736,
      "iterations": 550,
      "loss": 1.011886715888977,
      "perplexity": 2.750786066055298,
      "step": 550
    },
    {
      "epoch": 8.736,
      "iterations": 550,
      "loss": 1.129381537437439,
      "perplexity": 3.0937423706054688,
      "step": 550
    },
    {
      "epoch": 8.736,
      "iterations": 550,
      "loss": 1.398077368736267,
      "perplexity": 4.04741096496582,
      "step": 550
    },
    {
      "epoch": 8.736,
      "iterations": 550,
      "loss": 0.8129807710647583,
      "perplexity": 2.2546184062957764,
      "step": 550
    },
    {
      "epoch": 8.736,
      "iterations": 550,
      "loss": 0.8898286819458008,
      "perplexity": 2.4347124099731445,
      "step": 550
    },
    {
      "epoch": 9.528,
      "grad_norm": 1.5293030738830566,
      "learning_rate": 0.0001796326530612245,
      "loss": 8.7626,
      "step": 600
    },
    {
      "epoch": 9.528,
      "iterations": 600,
      "loss": 1.0979970693588257,
      "perplexity": 2.998154878616333,
      "step": 600
    },
    {
      "epoch": 9.528,
      "iterations": 600,
      "loss": 0.8137285709381104,
      "perplexity": 2.25630521774292,
      "step": 600
    },
    {
      "epoch": 9.528,
      "iterations": 600,
      "loss": 1.2985477447509766,
      "perplexity": 3.6639716625213623,
      "step": 600
    },
    {
      "epoch": 9.528,
      "iterations": 600,
      "loss": 0.8706497550010681,
      "perplexity": 2.3884623050689697,
      "step": 600
    },
    {
      "epoch": 9.528,
      "iterations": 600,
      "loss": 0.9377479553222656,
      "perplexity": 2.554222822189331,
      "step": 600
    },
    {
      "epoch": 9.528,
      "iterations": 600,
      "loss": 1.17672598361969,
      "perplexity": 3.243736743927002,
      "step": 600
    },
    {
      "epoch": 9.528,
      "iterations": 600,
      "loss": 1.386419415473938,
      "perplexity": 4.000500202178955,
      "step": 600
    },
    {
      "epoch": 9.528,
      "iterations": 600,
      "loss": 1.3254402875900269,
      "perplexity": 3.7638421058654785,
      "step": 600
    },
    {
      "epoch": 10.32,
      "grad_norm": 2.4994962215423584,
      "learning_rate": 0.00017759183673469388,
      "loss": 8.7559,
      "step": 650
    },
    {
      "epoch": 10.32,
      "iterations": 650,
      "loss": 0.7259123921394348,
      "perplexity": 2.0666158199310303,
      "step": 650
    },
    {
      "epoch": 10.32,
      "iterations": 650,
      "loss": 1.2694687843322754,
      "perplexity": 3.5589616298675537,
      "step": 650
    },
    {
      "epoch": 10.32,
      "iterations": 650,
      "loss": 0.7202224135398865,
      "perplexity": 2.0548901557922363,
      "step": 650
    },
    {
      "epoch": 10.32,
      "iterations": 650,
      "loss": 0.8148616552352905,
      "perplexity": 2.2588632106781006,
      "step": 650
    },
    {
      "epoch": 10.32,
      "iterations": 650,
      "loss": 1.341884970664978,
      "perplexity": 3.826249122619629,
      "step": 650
    },
    {
      "epoch": 10.32,
      "iterations": 650,
      "loss": 1.343503475189209,
      "perplexity": 3.832447052001953,
      "step": 650
    },
    {
      "epoch": 10.32,
      "iterations": 650,
      "loss": 1.2758983373641968,
      "perplexity": 3.5819175243377686,
      "step": 650
    },
    {
      "epoch": 10.32,
      "iterations": 650,
      "loss": 0.7837826609611511,
      "perplexity": 2.18973970413208,
      "step": 650
    },
    {
      "epoch": 11.112,
      "grad_norm": 2.388397693634033,
      "learning_rate": 0.00017555102040816328,
      "loss": 8.5696,
      "step": 700
    },
    {
      "epoch": 11.112,
      "iterations": 700,
      "loss": 0.9836516976356506,
      "perplexity": 2.674203872680664,
      "step": 700
    },
    {
      "epoch": 11.112,
      "iterations": 700,
      "loss": 1.283292531967163,
      "perplexity": 3.6085011959075928,
      "step": 700
    },
    {
      "epoch": 11.112,
      "iterations": 700,
      "loss": 0.6999504566192627,
      "perplexity": 2.013653039932251,
      "step": 700
    },
    {
      "epoch": 11.112,
      "iterations": 700,
      "loss": 1.3018999099731445,
      "perplexity": 3.676274538040161,
      "step": 700
    },
    {
      "epoch": 11.112,
      "iterations": 700,
      "loss": 1.2664647102355957,
      "perplexity": 3.548286199569702,
      "step": 700
    },
    {
      "epoch": 11.112,
      "iterations": 700,
      "loss": 1.2329356670379639,
      "perplexity": 3.4312877655029297,
      "step": 700
    },
    {
      "epoch": 11.112,
      "iterations": 700,
      "loss": 0.6943029761314392,
      "perplexity": 2.0023128986358643,
      "step": 700
    },
    {
      "epoch": 11.112,
      "iterations": 700,
      "loss": 0.844424307346344,
      "perplexity": 2.3266379833221436,
      "step": 700
    },
    {
      "epoch": 11.912,
      "grad_norm": 2.0324223041534424,
      "learning_rate": 0.00017351020408163265,
      "loss": 8.7126,
      "step": 750
    },
    {
      "epoch": 11.912,
      "iterations": 750,
      "loss": 0.6661199927330017,
      "perplexity": 1.9466694593429565,
      "step": 750
    },
    {
      "epoch": 11.912,
      "iterations": 750,
      "loss": 0.7100012302398682,
      "perplexity": 2.033993721008301,
      "step": 750
    },
    {
      "epoch": 11.912,
      "iterations": 750,
      "loss": 0.696071445941925,
      "perplexity": 2.005857229232788,
      "step": 750
    },
    {
      "epoch": 11.912,
      "iterations": 750,
      "loss": 0.9964914321899414,
      "perplexity": 2.708761215209961,
      "step": 750
    },
    {
      "epoch": 11.912,
      "iterations": 750,
      "loss": 1.27153480052948,
      "perplexity": 3.566322088241577,
      "step": 750
    },
    {
      "epoch": 11.912,
      "iterations": 750,
      "loss": 1.3353185653686523,
      "perplexity": 3.801206588745117,
      "step": 750
    },
    {
      "epoch": 11.912,
      "iterations": 750,
      "loss": 0.7350172996520996,
      "perplexity": 2.0855181217193604,
      "step": 750
    },
    {
      "epoch": 11.912,
      "iterations": 750,
      "loss": 1.3278729915618896,
      "perplexity": 3.7730095386505127,
      "step": 750
    },
    {
      "epoch": 12.704,
      "grad_norm": 3.406967878341675,
      "learning_rate": 0.00017146938775510203,
      "loss": 8.4398,
      "step": 800
    },
    {
      "epoch": 12.704,
      "iterations": 800,
      "loss": 0.6269766688346863,
      "perplexity": 1.8719425201416016,
      "step": 800
    },
    {
      "epoch": 12.704,
      "iterations": 800,
      "loss": 1.1874520778656006,
      "perplexity": 3.278716564178467,
      "step": 800
    },
    {
      "epoch": 12.704,
      "iterations": 800,
      "loss": 0.9771456718444824,
      "perplexity": 2.6568617820739746,
      "step": 800
    },
    {
      "epoch": 12.704,
      "iterations": 800,
      "loss": 0.7235382199287415,
      "perplexity": 2.0617151260375977,
      "step": 800
    },
    {
      "epoch": 12.704,
      "iterations": 800,
      "loss": 1.395439863204956,
      "perplexity": 4.036749839782715,
      "step": 800
    },
    {
      "epoch": 12.704,
      "iterations": 800,
      "loss": 1.1168209314346313,
      "perplexity": 3.055126190185547,
      "step": 800
    },
    {
      "epoch": 12.704,
      "iterations": 800,
      "loss": 0.9065678119659424,
      "perplexity": 2.4758105278015137,
      "step": 800
    },
    {
      "epoch": 12.704,
      "iterations": 800,
      "loss": 0.9662123322486877,
      "perplexity": 2.627971887588501,
      "step": 800
    },
    {
      "epoch": 13.496,
      "grad_norm": 2.1148693561553955,
      "learning_rate": 0.00016942857142857146,
      "loss": 8.3797,
      "step": 850
    },
    {
      "epoch": 13.496,
      "iterations": 850,
      "loss": 0.721360981464386,
      "perplexity": 2.0572311878204346,
      "step": 850
    },
    {
      "epoch": 13.496,
      "iterations": 850,
      "loss": 1.4117069244384766,
      "perplexity": 4.10295295715332,
      "step": 850
    },
    {
      "epoch": 13.496,
      "iterations": 850,
      "loss": 0.8962063193321228,
      "perplexity": 2.4502899646759033,
      "step": 850
    },
    {
      "epoch": 13.496,
      "iterations": 850,
      "loss": 0.7291890382766724,
      "perplexity": 2.0733983516693115,
      "step": 850
    },
    {
      "epoch": 13.496,
      "iterations": 850,
      "loss": 1.325915813446045,
      "perplexity": 3.765632390975952,
      "step": 850
    },
    {
      "epoch": 13.496,
      "iterations": 850,
      "loss": 0.9512767791748047,
      "perplexity": 2.5890133380889893,
      "step": 850
    },
    {
      "epoch": 13.496,
      "iterations": 850,
      "loss": 0.8797624111175537,
      "perplexity": 2.4103269577026367,
      "step": 850
    },
    {
      "epoch": 13.496,
      "iterations": 850,
      "loss": 0.8233275413513184,
      "perplexity": 2.2780675888061523,
      "step": 850
    },
    {
      "epoch": 14.288,
      "grad_norm": 1.618030309677124,
      "learning_rate": 0.00016738775510204083,
      "loss": 8.364,
      "step": 900
    },
    {
      "epoch": 14.288,
      "iterations": 900,
      "loss": 1.1965059041976929,
      "perplexity": 3.3085362911224365,
      "step": 900
    },
    {
      "epoch": 14.288,
      "iterations": 900,
      "loss": 1.4118331670761108,
      "perplexity": 4.103470802307129,
      "step": 900
    },
    {
      "epoch": 14.288,
      "iterations": 900,
      "loss": 0.6244611144065857,
      "perplexity": 1.8672394752502441,
      "step": 900
    },
    {
      "epoch": 14.288,
      "iterations": 900,
      "loss": 1.3377306461334229,
      "perplexity": 3.8103866577148438,
      "step": 900
    },
    {
      "epoch": 14.288,
      "iterations": 900,
      "loss": 0.9609332084655762,
      "perplexity": 2.6141350269317627,
      "step": 900
    },
    {
      "epoch": 14.288,
      "iterations": 900,
      "loss": 1.275619626045227,
      "perplexity": 3.5809195041656494,
      "step": 900
    },
    {
      "epoch": 14.288,
      "iterations": 900,
      "loss": 1.3343851566314697,
      "perplexity": 3.7976601123809814,
      "step": 900
    },
    {
      "epoch": 14.288,
      "iterations": 900,
      "loss": 0.7668776512145996,
      "perplexity": 2.1530332565307617,
      "step": 900
    },
    {
      "epoch": 15.08,
      "grad_norm": 1.588842749595642,
      "learning_rate": 0.0001653469387755102,
      "loss": 8.2573,
      "step": 950
    },
    {
      "epoch": 15.08,
      "iterations": 950,
      "loss": 1.1166179180145264,
      "perplexity": 3.0545060634613037,
      "step": 950
    },
    {
      "epoch": 15.08,
      "iterations": 950,
      "loss": 1.2599539756774902,
      "perplexity": 3.525259256362915,
      "step": 950
    },
    {
      "epoch": 15.08,
      "iterations": 950,
      "loss": 0.779222846031189,
      "perplexity": 2.1797776222229004,
      "step": 950
    },
    {
      "epoch": 15.08,
      "iterations": 950,
      "loss": 1.438791036605835,
      "perplexity": 4.2155961990356445,
      "step": 950
    },
    {
      "epoch": 15.08,
      "iterations": 950,
      "loss": 1.3168689012527466,
      "perplexity": 3.7317187786102295,
      "step": 950
    },
    {
      "epoch": 15.08,
      "iterations": 950,
      "loss": 0.626968502998352,
      "perplexity": 1.871927261352539,
      "step": 950
    },
    {
      "epoch": 15.08,
      "iterations": 950,
      "loss": 0.9754181504249573,
      "perplexity": 2.652276039123535,
      "step": 950
    },
    {
      "epoch": 15.08,
      "iterations": 950,
      "loss": 1.0741238594055176,
      "perplexity": 2.927427053451538,
      "step": 950
    },
    {
      "epoch": 15.88,
      "grad_norm": 1.8524723052978516,
      "learning_rate": 0.0001633061224489796,
      "loss": 8.1491,
      "step": 1000
    },
    {
      "epoch": 15.88,
      "iterations": 1000,
      "loss": 0.6367197632789612,
      "perplexity": 1.8902702331542969,
      "step": 1000
    },
    {
      "epoch": 15.88,
      "iterations": 1000,
      "loss": 1.0081853866577148,
      "perplexity": 2.7406232357025146,
      "step": 1000
    },
    {
      "epoch": 15.88,
      "iterations": 1000,
      "loss": 1.2472440004348755,
      "perplexity": 3.4807369709014893,
      "step": 1000
    },
    {
      "epoch": 15.88,
      "iterations": 1000,
      "loss": 1.3410508632659912,
      "perplexity": 3.82305908203125,
      "step": 1000
    },
    {
      "epoch": 15.88,
      "iterations": 1000,
      "loss": 0.6334425806999207,
      "perplexity": 1.8840855360031128,
      "step": 1000
    },
    {
      "epoch": 15.88,
      "iterations": 1000,
      "loss": 0.956955075263977,
      "perplexity": 2.6037559509277344,
      "step": 1000
    },
    {
      "epoch": 15.88,
      "iterations": 1000,
      "loss": 1.4074878692626953,
      "perplexity": 4.085679054260254,
      "step": 1000
    },
    {
      "epoch": 15.88,
      "iterations": 1000,
      "loss": 1.2073198556900024,
      "perplexity": 3.3445088863372803,
      "step": 1000
    },
    {
      "epoch": 16.672,
      "grad_norm": 2.4213225841522217,
      "learning_rate": 0.00016126530612244899,
      "loss": 7.9357,
      "step": 1050
    },
    {
      "epoch": 16.672,
      "iterations": 1050,
      "loss": 0.6090465784072876,
      "perplexity": 1.8386775255203247,
      "step": 1050
    },
    {
      "epoch": 16.672,
      "iterations": 1050,
      "loss": 0.644220769405365,
      "perplexity": 1.904502272605896,
      "step": 1050
    },
    {
      "epoch": 16.672,
      "iterations": 1050,
      "loss": 0.6428782343864441,
      "perplexity": 1.901947259902954,
      "step": 1050
    },
    {
      "epoch": 16.672,
      "iterations": 1050,
      "loss": 0.7456284761428833,
      "perplexity": 2.1077656745910645,
      "step": 1050
    },
    {
      "epoch": 16.672,
      "iterations": 1050,
      "loss": 1.4331036806106567,
      "perplexity": 4.191688537597656,
      "step": 1050
    },
    {
      "epoch": 16.672,
      "iterations": 1050,
      "loss": 1.0038741827011108,
      "perplexity": 2.7288334369659424,
      "step": 1050
    },
    {
      "epoch": 16.672,
      "iterations": 1050,
      "loss": 1.2503092288970947,
      "perplexity": 3.491422414779663,
      "step": 1050
    },
    {
      "epoch": 16.672,
      "iterations": 1050,
      "loss": 1.2379844188690186,
      "perplexity": 3.448655128479004,
      "step": 1050
    },
    {
      "epoch": 17.464,
      "grad_norm": 1.8159329891204834,
      "learning_rate": 0.00015922448979591836,
      "loss": 8.0828,
      "step": 1100
    },
    {
      "epoch": 17.464,
      "iterations": 1100,
      "loss": 0.9521985650062561,
      "perplexity": 2.5914008617401123,
      "step": 1100
    },
    {
      "epoch": 17.464,
      "iterations": 1100,
      "loss": 1.3583035469055176,
      "perplexity": 3.8895890712738037,
      "step": 1100
    },
    {
      "epoch": 17.464,
      "iterations": 1100,
      "loss": 0.6009587049484253,
      "perplexity": 1.8238664865493774,
      "step": 1100
    },
    {
      "epoch": 17.464,
      "iterations": 1100,
      "loss": 1.4008947610855103,
      "perplexity": 4.058830261230469,
      "step": 1100
    },
    {
      "epoch": 17.464,
      "iterations": 1100,
      "loss": 0.9125901460647583,
      "perplexity": 2.4907655715942383,
      "step": 1100
    },
    {
      "epoch": 17.464,
      "iterations": 1100,
      "loss": 1.3807820081710815,
      "perplexity": 3.978011131286621,
      "step": 1100
    },
    {
      "epoch": 17.464,
      "iterations": 1100,
      "loss": 0.647940456867218,
      "perplexity": 1.9115997552871704,
      "step": 1100
    },
    {
      "epoch": 17.464,
      "iterations": 1100,
      "loss": 0.5636405348777771,
      "perplexity": 1.757057547569275,
      "step": 1100
    },
    {
      "epoch": 18.256,
      "grad_norm": 1.8228684663772583,
      "learning_rate": 0.00015718367346938776,
      "loss": 7.9691,
      "step": 1150
    },
    {
      "epoch": 18.256,
      "iterations": 1150,
      "loss": 1.0565581321716309,
      "perplexity": 2.8764536380767822,
      "step": 1150
    },
    {
      "epoch": 18.256,
      "iterations": 1150,
      "loss": 0.8571454286575317,
      "perplexity": 2.356424570083618,
      "step": 1150
    },
    {
      "epoch": 18.256,
      "iterations": 1150,
      "loss": 1.112743616104126,
      "perplexity": 3.0426950454711914,
      "step": 1150
    },
    {
      "epoch": 18.256,
      "iterations": 1150,
      "loss": 0.8329395055770874,
      "perplexity": 2.30007004737854,
      "step": 1150
    },
    {
      "epoch": 18.256,
      "iterations": 1150,
      "loss": 0.8326234221458435,
      "perplexity": 2.2993431091308594,
      "step": 1150
    },
    {
      "epoch": 18.256,
      "iterations": 1150,
      "loss": 1.083107590675354,
      "perplexity": 2.9538447856903076,
      "step": 1150
    },
    {
      "epoch": 18.256,
      "iterations": 1150,
      "loss": 0.5377579927444458,
      "perplexity": 1.7121638059616089,
      "step": 1150
    },
    {
      "epoch": 18.256,
      "iterations": 1150,
      "loss": 0.5492587685585022,
      "perplexity": 1.7319687604904175,
      "step": 1150
    },
    {
      "epoch": 19.048,
      "grad_norm": 1.8482674360275269,
      "learning_rate": 0.00015514285714285714,
      "loss": 7.8865,
      "step": 1200
    },
    {
      "epoch": 19.048,
      "iterations": 1200,
      "loss": 0.5769485831260681,
      "perplexity": 1.7805968523025513,
      "step": 1200
    },
    {
      "epoch": 19.048,
      "iterations": 1200,
      "loss": 1.2112786769866943,
      "perplexity": 3.3577754497528076,
      "step": 1200
    },
    {
      "epoch": 19.048,
      "iterations": 1200,
      "loss": 0.99701327085495,
      "perplexity": 2.7101752758026123,
      "step": 1200
    },
    {
      "epoch": 19.048,
      "iterations": 1200,
      "loss": 1.2543889284133911,
      "perplexity": 3.505695343017578,
      "step": 1200
    },
    {
      "epoch": 19.048,
      "iterations": 1200,
      "loss": 0.594965398311615,
      "perplexity": 1.8129682540893555,
      "step": 1200
    },
    {
      "epoch": 19.048,
      "iterations": 1200,
      "loss": 1.1606930494308472,
      "perplexity": 3.1921448707580566,
      "step": 1200
    },
    {
      "epoch": 19.048,
      "iterations": 1200,
      "loss": 0.9751089811325073,
      "perplexity": 2.651456117630005,
      "step": 1200
    },
    {
      "epoch": 19.048,
      "iterations": 1200,
      "loss": 1.3984960317611694,
      "perplexity": 4.049105644226074,
      "step": 1200
    },
    {
      "epoch": 19.848,
      "grad_norm": 1.669064998626709,
      "learning_rate": 0.00015310204081632654,
      "loss": 7.7887,
      "step": 1250
    },
    {
      "epoch": 19.848,
      "iterations": 1250,
      "loss": 0.5829055905342102,
      "perplexity": 1.7912355661392212,
      "step": 1250
    },
    {
      "epoch": 19.848,
      "iterations": 1250,
      "loss": 0.6918845772743225,
      "perplexity": 1.99747633934021,
      "step": 1250
    },
    {
      "epoch": 19.848,
      "iterations": 1250,
      "loss": 1.1336259841918945,
      "perplexity": 3.1069018840789795,
      "step": 1250
    },
    {
      "epoch": 19.848,
      "iterations": 1250,
      "loss": 0.5803927779197693,
      "perplexity": 1.7867401838302612,
      "step": 1250
    },
    {
      "epoch": 19.848,
      "iterations": 1250,
      "loss": 0.6167545914649963,
      "perplexity": 1.8529047966003418,
      "step": 1250
    },
    {
      "epoch": 19.848,
      "iterations": 1250,
      "loss": 1.311985969543457,
      "perplexity": 3.713541269302368,
      "step": 1250
    },
    {
      "epoch": 19.848,
      "iterations": 1250,
      "loss": 1.2057090997695923,
      "perplexity": 3.339125871658325,
      "step": 1250
    },
    {
      "epoch": 19.848,
      "iterations": 1250,
      "loss": 0.7527640461921692,
      "perplexity": 2.1228597164154053,
      "step": 1250
    },
    {
      "epoch": 20.64,
      "grad_norm": 2.2815122604370117,
      "learning_rate": 0.00015106122448979592,
      "loss": 7.6079,
      "step": 1300
    },
    {
      "epoch": 20.64,
      "iterations": 1300,
      "loss": 0.5391091704368591,
      "perplexity": 1.714478850364685,
      "step": 1300
    },
    {
      "epoch": 20.64,
      "iterations": 1300,
      "loss": 0.557958722114563,
      "perplexity": 1.7471024990081787,
      "step": 1300
    },
    {
      "epoch": 20.64,
      "iterations": 1300,
      "loss": 0.6017357707023621,
      "perplexity": 1.8252843618392944,
      "step": 1300
    },
    {
      "epoch": 20.64,
      "iterations": 1300,
      "loss": 1.1034533977508545,
      "perplexity": 3.0145585536956787,
      "step": 1300
    },
    {
      "epoch": 20.64,
      "iterations": 1300,
      "loss": 0.5978987812995911,
      "perplexity": 1.8182941675186157,
      "step": 1300
    },
    {
      "epoch": 20.64,
      "iterations": 1300,
      "loss": 1.3020514249801636,
      "perplexity": 3.6768314838409424,
      "step": 1300
    },
    {
      "epoch": 20.64,
      "iterations": 1300,
      "loss": 0.534480631351471,
      "perplexity": 1.7065616846084595,
      "step": 1300
    },
    {
      "epoch": 20.64,
      "iterations": 1300,
      "loss": 0.8274655342102051,
      "perplexity": 2.2875137329101562,
      "step": 1300
    },
    {
      "epoch": 21.432,
      "grad_norm": 4.184293270111084,
      "learning_rate": 0.00014902040816326532,
      "loss": 7.7086,
      "step": 1350
    },
    {
      "epoch": 21.432,
      "iterations": 1350,
      "loss": 1.0635403394699097,
      "perplexity": 2.8966078758239746,
      "step": 1350
    },
    {
      "epoch": 21.432,
      "iterations": 1350,
      "loss": 1.2026433944702148,
      "perplexity": 3.328904867172241,
      "step": 1350
    },
    {
      "epoch": 21.432,
      "iterations": 1350,
      "loss": 1.365219235420227,
      "perplexity": 3.916581630706787,
      "step": 1350
    },
    {
      "epoch": 21.432,
      "iterations": 1350,
      "loss": 0.5812026858329773,
      "perplexity": 1.788187861442566,
      "step": 1350
    },
    {
      "epoch": 21.432,
      "iterations": 1350,
      "loss": 1.3491274118423462,
      "perplexity": 3.8540611267089844,
      "step": 1350
    },
    {
      "epoch": 21.432,
      "iterations": 1350,
      "loss": 1.4019315242767334,
      "perplexity": 4.063040256500244,
      "step": 1350
    },
    {
      "epoch": 21.432,
      "iterations": 1350,
      "loss": 1.302033543586731,
      "perplexity": 3.6767659187316895,
      "step": 1350
    },
    {
      "epoch": 21.432,
      "iterations": 1350,
      "loss": 0.5830033421516418,
      "perplexity": 1.7914106845855713,
      "step": 1350
    },
    {
      "epoch": 22.224,
      "grad_norm": 4.826366901397705,
      "learning_rate": 0.00014697959183673472,
      "loss": 7.596,
      "step": 1400
    },
    {
      "epoch": 22.224,
      "iterations": 1400,
      "loss": 0.5358403921127319,
      "perplexity": 1.7088837623596191,
      "step": 1400
    },
    {
      "epoch": 22.224,
      "iterations": 1400,
      "loss": 1.042484998703003,
      "perplexity": 2.836256265640259,
      "step": 1400
    },
    {
      "epoch": 22.224,
      "iterations": 1400,
      "loss": 1.3919323682785034,
      "perplexity": 4.022615909576416,
      "step": 1400
    },
    {
      "epoch": 22.224,
      "iterations": 1400,
      "loss": 0.6378576755523682,
      "perplexity": 1.8924223184585571,
      "step": 1400
    },
    {
      "epoch": 22.224,
      "iterations": 1400,
      "loss": 1.0706130266189575,
      "perplexity": 2.9171671867370605,
      "step": 1400
    },
    {
      "epoch": 22.224,
      "iterations": 1400,
      "loss": 0.5228120684623718,
      "perplexity": 1.6867643594741821,
      "step": 1400
    },
    {
      "epoch": 22.224,
      "iterations": 1400,
      "loss": 0.803387463092804,
      "perplexity": 2.2330925464630127,
      "step": 1400
    },
    {
      "epoch": 22.224,
      "iterations": 1400,
      "loss": 0.7539895176887512,
      "perplexity": 2.125462770462036,
      "step": 1400
    },
    {
      "epoch": 23.016,
      "grad_norm": 2.4067859649658203,
      "learning_rate": 0.0001449387755102041,
      "loss": 7.5005,
      "step": 1450
    },
    {
      "epoch": 23.016,
      "iterations": 1450,
      "loss": 0.5486997365951538,
      "perplexity": 1.7310007810592651,
      "step": 1450
    },
    {
      "epoch": 23.016,
      "iterations": 1450,
      "loss": 1.093252420425415,
      "perplexity": 2.9839632511138916,
      "step": 1450
    },
    {
      "epoch": 23.016,
      "iterations": 1450,
      "loss": 0.5222764611244202,
      "perplexity": 1.6858611106872559,
      "step": 1450
    },
    {
      "epoch": 23.016,
      "iterations": 1450,
      "loss": 1.3117355108261108,
      "perplexity": 3.712611198425293,
      "step": 1450
    },
    {
      "epoch": 23.016,
      "iterations": 1450,
      "loss": 0.45949453115463257,
      "perplexity": 1.5832735300064087,
      "step": 1450
    },
    {
      "epoch": 23.016,
      "iterations": 1450,
      "loss": 0.5163171887397766,
      "perplexity": 1.675844430923462,
      "step": 1450
    },
    {
      "epoch": 23.016,
      "iterations": 1450,
      "loss": 1.3302329778671265,
      "perplexity": 3.781924247741699,
      "step": 1450
    },
    {
      "epoch": 23.016,
      "iterations": 1450,
      "loss": 0.893937885761261,
      "perplexity": 2.4447379112243652,
      "step": 1450
    },
    {
      "epoch": 23.816,
      "grad_norm": 1.912050724029541,
      "learning_rate": 0.00014289795918367347,
      "loss": 7.4977,
      "step": 1500
    },
    {
      "epoch": 23.816,
      "iterations": 1500,
      "loss": 1.3621437549591064,
      "perplexity": 3.904554605484009,
      "step": 1500
    },
    {
      "epoch": 23.816,
      "iterations": 1500,
      "loss": 0.9330029487609863,
      "perplexity": 2.5421316623687744,
      "step": 1500
    },
    {
      "epoch": 23.816,
      "iterations": 1500,
      "loss": 0.5013000965118408,
      "perplexity": 1.650866150856018,
      "step": 1500
    },
    {
      "epoch": 23.816,
      "iterations": 1500,
      "loss": 0.5491482019424438,
      "perplexity": 1.731777310371399,
      "step": 1500
    },
    {
      "epoch": 23.816,
      "iterations": 1500,
      "loss": 0.825995683670044,
      "perplexity": 2.284153938293457,
      "step": 1500
    },
    {
      "epoch": 23.816,
      "iterations": 1500,
      "loss": 1.3977034091949463,
      "perplexity": 4.045897483825684,
      "step": 1500
    },
    {
      "epoch": 23.816,
      "iterations": 1500,
      "loss": 1.1331760883331299,
      "perplexity": 3.105504274368286,
      "step": 1500
    },
    {
      "epoch": 23.816,
      "iterations": 1500,
      "loss": 1.1477775573730469,
      "perplexity": 3.151181936264038,
      "step": 1500
    },
    {
      "epoch": 24.608,
      "grad_norm": 2.7294955253601074,
      "learning_rate": 0.00014085714285714287,
      "loss": 7.3278,
      "step": 1550
    },
    {
      "epoch": 24.608,
      "iterations": 1550,
      "loss": 0.5246846675872803,
      "perplexity": 1.6899259090423584,
      "step": 1550
    },
    {
      "epoch": 24.608,
      "iterations": 1550,
      "loss": 0.5451394319534302,
      "perplexity": 1.7248488664627075,
      "step": 1550
    },
    {
      "epoch": 24.608,
      "iterations": 1550,
      "loss": 0.5564219355583191,
      "perplexity": 1.7444196939468384,
      "step": 1550
    },
    {
      "epoch": 24.608,
      "iterations": 1550,
      "loss": 1.333707571029663,
      "perplexity": 3.795088052749634,
      "step": 1550
    },
    {
      "epoch": 24.608,
      "iterations": 1550,
      "loss": 1.3802685737609863,
      "perplexity": 3.975969076156616,
      "step": 1550
    },
    {
      "epoch": 24.608,
      "iterations": 1550,
      "loss": 0.7486528158187866,
      "perplexity": 2.114150047302246,
      "step": 1550
    },
    {
      "epoch": 24.608,
      "iterations": 1550,
      "loss": 1.2958673238754272,
      "perplexity": 3.6541638374328613,
      "step": 1550
    },
    {
      "epoch": 24.608,
      "iterations": 1550,
      "loss": 0.4843631684780121,
      "perplexity": 1.6231410503387451,
      "step": 1550
    },
    {
      "epoch": 25.4,
      "grad_norm": 3.401135206222534,
      "learning_rate": 0.00013881632653061225,
      "loss": 7.2327,
      "step": 1600
    },
    {
      "epoch": 25.4,
      "iterations": 1600,
      "loss": 1.100433349609375,
      "perplexity": 3.0054683685302734,
      "step": 1600
    },
    {
      "epoch": 25.4,
      "iterations": 1600,
      "loss": 0.6969457864761353,
      "perplexity": 2.0076117515563965,
      "step": 1600
    },
    {
      "epoch": 25.4,
      "iterations": 1600,
      "loss": 0.4361754059791565,
      "perplexity": 1.5467801094055176,
      "step": 1600
    },
    {
      "epoch": 25.4,
      "iterations": 1600,
      "loss": 0.4635510742664337,
      "perplexity": 1.5897091627120972,
      "step": 1600
    },
    {
      "epoch": 25.4,
      "iterations": 1600,
      "loss": 1.2658424377441406,
      "perplexity": 3.546078681945801,
      "step": 1600
    },
    {
      "epoch": 25.4,
      "iterations": 1600,
      "loss": 1.1028883457183838,
      "perplexity": 3.0128555297851562,
      "step": 1600
    },
    {
      "epoch": 25.4,
      "iterations": 1600,
      "loss": 0.5295355319976807,
      "perplexity": 1.6981433629989624,
      "step": 1600
    },
    {
      "epoch": 25.4,
      "iterations": 1600,
      "loss": 0.6808142066001892,
      "perplexity": 1.9754854440689087,
      "step": 1600
    },
    {
      "epoch": 26.192,
      "grad_norm": 2.5513782501220703,
      "learning_rate": 0.00013677551020408162,
      "loss": 7.4827,
      "step": 1650
    },
    {
      "epoch": 26.192,
      "iterations": 1650,
      "loss": 1.2685704231262207,
      "perplexity": 3.5557658672332764,
      "step": 1650
    },
    {
      "epoch": 26.192,
      "iterations": 1650,
      "loss": 0.43727579712867737,
      "perplexity": 1.5484830141067505,
      "step": 1650
    },
    {
      "epoch": 26.192,
      "iterations": 1650,
      "loss": 0.9880886673927307,
      "perplexity": 2.686095714569092,
      "step": 1650
    },
    {
      "epoch": 26.192,
      "iterations": 1650,
      "loss": 1.3548104763031006,
      "perplexity": 3.876026153564453,
      "step": 1650
    },
    {
      "epoch": 26.192,
      "iterations": 1650,
      "loss": 1.2873340845108032,
      "perplexity": 3.623114585876465,
      "step": 1650
    },
    {
      "epoch": 26.192,
      "iterations": 1650,
      "loss": 1.0964717864990234,
      "perplexity": 2.9935853481292725,
      "step": 1650
    },
    {
      "epoch": 26.192,
      "iterations": 1650,
      "loss": 1.3203551769256592,
      "perplexity": 3.7447509765625,
      "step": 1650
    },
    {
      "epoch": 26.192,
      "iterations": 1650,
      "loss": 0.513969361782074,
      "perplexity": 1.6719144582748413,
      "step": 1650
    },
    {
      "epoch": 26.992,
      "grad_norm": 2.3748068809509277,
      "learning_rate": 0.00013473469387755103,
      "loss": 7.1984,
      "step": 1700
    },
    {
      "epoch": 26.992,
      "iterations": 1700,
      "loss": 0.9872845411300659,
      "perplexity": 2.683936595916748,
      "step": 1700
    },
    {
      "epoch": 26.992,
      "iterations": 1700,
      "loss": 2.0109996795654297,
      "perplexity": 7.470782279968262,
      "step": 1700
    },
    {
      "epoch": 26.992,
      "iterations": 1700,
      "loss": 2.0434024333953857,
      "perplexity": 7.71682071685791,
      "step": 1700
    },
    {
      "epoch": 26.992,
      "iterations": 1700,
      "loss": 2.13818621635437,
      "perplexity": 8.484036445617676,
      "step": 1700
    },
    {
      "epoch": 27.784,
      "grad_norm": 2.0457022190093994,
      "learning_rate": 0.0001326938775510204,
      "loss": 7.1295,
      "step": 1750
    },
    {
      "epoch": 27.784,
      "iterations": 1750,
      "loss": 1.1343990564346313,
      "perplexity": 3.109304428100586,
      "step": 1750
    },
    {
      "epoch": 27.784,
      "iterations": 1750,
      "loss": 1.3760267496109009,
      "perplexity": 3.959139585494995,
      "step": 1750
    },
    {
      "epoch": 27.784,
      "iterations": 1750,
      "loss": 1.15541672706604,
      "perplexity": 3.175346612930298,
      "step": 1750
    },
    {
      "epoch": 27.784,
      "iterations": 1750,
      "loss": 0.9468733668327332,
      "perplexity": 2.5776379108428955,
      "step": 1750
    },
    {
      "epoch": 27.784,
      "iterations": 1750,
      "loss": 1.1200618743896484,
      "perplexity": 3.0650439262390137,
      "step": 1750
    },
    {
      "epoch": 27.784,
      "iterations": 1750,
      "loss": 1.2813547849655151,
      "perplexity": 3.601515531539917,
      "step": 1750
    },
    {
      "epoch": 27.784,
      "iterations": 1750,
      "loss": 0.48250052332878113,
      "perplexity": 1.6201205253601074,
      "step": 1750
    },
    {
      "epoch": 27.784,
      "iterations": 1750,
      "loss": 1.3930168151855469,
      "perplexity": 4.026980400085449,
      "step": 1750
    },
    {
      "epoch": 28.576,
      "grad_norm": 2.0825204849243164,
      "learning_rate": 0.0001306530612244898,
      "loss": 7.1793,
      "step": 1800
    },
    {
      "epoch": 28.576,
      "iterations": 1800,
      "loss": 0.5059148669242859,
      "perplexity": 1.6585021018981934,
      "step": 1800
    },
    {
      "epoch": 28.576,
      "iterations": 1800,
      "loss": 0.4530651569366455,
      "perplexity": 1.5731266736984253,
      "step": 1800
    },
    {
      "epoch": 28.576,
      "iterations": 1800,
      "loss": 0.4853505492210388,
      "perplexity": 1.6247445344924927,
      "step": 1800
    },
    {
      "epoch": 28.576,
      "iterations": 1800,
      "loss": 1.0065430402755737,
      "perplexity": 2.736125946044922,
      "step": 1800
    },
    {
      "epoch": 28.576,
      "iterations": 1800,
      "loss": 1.0867568254470825,
      "perplexity": 2.9646434783935547,
      "step": 1800
    },
    {
      "epoch": 28.576,
      "iterations": 1800,
      "loss": 0.6400924324989319,
      "perplexity": 1.8966561555862427,
      "step": 1800
    },
    {
      "epoch": 28.576,
      "iterations": 1800,
      "loss": 1.1150124073028564,
      "perplexity": 3.0496058464050293,
      "step": 1800
    },
    {
      "epoch": 28.576,
      "iterations": 1800,
      "loss": 0.652862012386322,
      "perplexity": 1.921030879020691,
      "step": 1800
    },
    {
      "epoch": 29.368,
      "grad_norm": 2.9187309741973877,
      "learning_rate": 0.0001286122448979592,
      "loss": 7.0969,
      "step": 1850
    },
    {
      "epoch": 29.368,
      "iterations": 1850,
      "loss": 1.1451704502105713,
      "perplexity": 3.142976999282837,
      "step": 1850
    },
    {
      "epoch": 29.368,
      "iterations": 1850,
      "loss": 0.9996665716171265,
      "perplexity": 2.7173757553100586,
      "step": 1850
    },
    {
      "epoch": 29.368,
      "iterations": 1850,
      "loss": 1.3858723640441895,
      "perplexity": 3.99831223487854,
      "step": 1850
    },
    {
      "epoch": 29.368,
      "iterations": 1850,
      "loss": 1.1835777759552002,
      "perplexity": 3.266038417816162,
      "step": 1850
    },
    {
      "epoch": 29.368,
      "iterations": 1850,
      "loss": 0.41287457942962646,
      "perplexity": 1.5111554861068726,
      "step": 1850
    },
    {
      "epoch": 29.368,
      "iterations": 1850,
      "loss": 0.4473784565925598,
      "perplexity": 1.5642061233520508,
      "step": 1850
    },
    {
      "epoch": 29.368,
      "iterations": 1850,
      "loss": 0.3779611885547638,
      "perplexity": 1.4593063592910767,
      "step": 1850
    },
    {
      "epoch": 29.368,
      "iterations": 1850,
      "loss": 1.0014541149139404,
      "perplexity": 2.7222373485565186,
      "step": 1850
    },
    {
      "epoch": 30.16,
      "grad_norm": 2.0348496437072754,
      "learning_rate": 0.00012657142857142858,
      "loss": 7.0619,
      "step": 1900
    },
    {
      "epoch": 30.16,
      "iterations": 1900,
      "loss": 1.0667712688446045,
      "perplexity": 2.9059815406799316,
      "step": 1900
    },
    {
      "epoch": 30.16,
      "iterations": 1900,
      "loss": 0.9960535168647766,
      "perplexity": 2.7075753211975098,
      "step": 1900
    },
    {
      "epoch": 30.16,
      "iterations": 1900,
      "loss": 1.3391709327697754,
      "perplexity": 3.815878391265869,
      "step": 1900
    },
    {
      "epoch": 30.16,
      "iterations": 1900,
      "loss": 0.9661340117454529,
      "perplexity": 2.6277658939361572,
      "step": 1900
    },
    {
      "epoch": 30.16,
      "iterations": 1900,
      "loss": 0.4421072006225586,
      "perplexity": 1.5559825897216797,
      "step": 1900
    },
    {
      "epoch": 30.16,
      "iterations": 1900,
      "loss": 0.40567219257354736,
      "perplexity": 1.5003106594085693,
      "step": 1900
    },
    {
      "epoch": 30.16,
      "iterations": 1900,
      "loss": 0.4532291889190674,
      "perplexity": 1.5733847618103027,
      "step": 1900
    },
    {
      "epoch": 30.16,
      "iterations": 1900,
      "loss": 0.4634707570075989,
      "perplexity": 1.5895814895629883,
      "step": 1900
    },
    {
      "epoch": 30.96,
      "grad_norm": 2.4210896492004395,
      "learning_rate": 0.00012453061224489798,
      "loss": 6.9907,
      "step": 1950
    },
    {
      "epoch": 30.96,
      "iterations": 1950,
      "loss": 0.5676205158233643,
      "perplexity": 1.7640644311904907,
      "step": 1950
    },
    {
      "epoch": 30.96,
      "iterations": 1950,
      "loss": 1.4263570308685303,
      "perplexity": 4.163504123687744,
      "step": 1950
    },
    {
      "epoch": 30.96,
      "iterations": 1950,
      "loss": 0.4654618203639984,
      "perplexity": 1.5927495956420898,
      "step": 1950
    },
    {
      "epoch": 30.96,
      "iterations": 1950,
      "loss": 1.043026089668274,
      "perplexity": 2.8377914428710938,
      "step": 1950
    },
    {
      "epoch": 30.96,
      "iterations": 1950,
      "loss": 0.8958601951599121,
      "perplexity": 2.449441909790039,
      "step": 1950
    },
    {
      "epoch": 30.96,
      "iterations": 1950,
      "loss": 0.5741626024246216,
      "perplexity": 1.775642991065979,
      "step": 1950
    },
    {
      "epoch": 30.96,
      "iterations": 1950,
      "loss": 0.7922784686088562,
      "perplexity": 2.2084226608276367,
      "step": 1950
    },
    {
      "epoch": 30.96,
      "iterations": 1950,
      "loss": 0.9708977341651917,
      "perplexity": 2.6403136253356934,
      "step": 1950
    },
    {
      "epoch": 31.752,
      "grad_norm": 2.895207643508911,
      "learning_rate": 0.00012248979591836736,
      "loss": 6.7862,
      "step": 2000
    },
    {
      "epoch": 31.752,
      "iterations": 2000,
      "loss": 1.268844485282898,
      "perplexity": 3.5567402839660645,
      "step": 2000
    },
    {
      "epoch": 31.752,
      "iterations": 2000,
      "loss": 0.9010182619094849,
      "perplexity": 2.462108850479126,
      "step": 2000
    },
    {
      "epoch": 31.752,
      "iterations": 2000,
      "loss": 0.4372745454311371,
      "perplexity": 1.5484811067581177,
      "step": 2000
    },
    {
      "epoch": 31.752,
      "iterations": 2000,
      "loss": 1.166054606437683,
      "perplexity": 3.20930552482605,
      "step": 2000
    },
    {
      "epoch": 31.752,
      "iterations": 2000,
      "loss": 0.41010910272598267,
      "perplexity": 1.5069822072982788,
      "step": 2000
    },
    {
      "epoch": 31.752,
      "iterations": 2000,
      "loss": 1.0877445936203003,
      "perplexity": 2.967573404312134,
      "step": 2000
    },
    {
      "epoch": 31.752,
      "iterations": 2000,
      "loss": 0.42530620098114014,
      "perplexity": 1.5300588607788086,
      "step": 2000
    },
    {
      "epoch": 31.752,
      "iterations": 2000,
      "loss": 0.4103706479072571,
      "perplexity": 1.5073764324188232,
      "step": 2000
    },
    {
      "epoch": 32.544,
      "grad_norm": 2.8867316246032715,
      "learning_rate": 0.00012044897959183675,
      "loss": 6.9652,
      "step": 2050
    },
    {
      "epoch": 32.544,
      "iterations": 2050,
      "loss": 0.42659881711006165,
      "perplexity": 1.5320378541946411,
      "step": 2050
    },
    {
      "epoch": 32.544,
      "iterations": 2050,
      "loss": 0.3752068877220154,
      "perplexity": 1.4552924633026123,
      "step": 2050
    },
    {
      "epoch": 32.544,
      "iterations": 2050,
      "loss": 1.2185808420181274,
      "perplexity": 3.3823840618133545,
      "step": 2050
    },
    {
      "epoch": 32.544,
      "iterations": 2050,
      "loss": 1.05925714969635,
      "perplexity": 2.884227752685547,
      "step": 2050
    },
    {
      "epoch": 32.544,
      "iterations": 2050,
      "loss": 1.3124258518218994,
      "perplexity": 3.715175151824951,
      "step": 2050
    },
    {
      "epoch": 32.544,
      "iterations": 2050,
      "loss": 1.142109990119934,
      "perplexity": 3.1333727836608887,
      "step": 2050
    },
    {
      "epoch": 32.544,
      "iterations": 2050,
      "loss": 0.7112906575202942,
      "perplexity": 2.036618232727051,
      "step": 2050
    },
    {
      "epoch": 32.544,
      "iterations": 2050,
      "loss": 1.3160045146942139,
      "perplexity": 3.72849440574646,
      "step": 2050
    },
    {
      "epoch": 33.336,
      "grad_norm": 2.72001314163208,
      "learning_rate": 0.00011840816326530612,
      "loss": 7.0355,
      "step": 2100
    },
    {
      "epoch": 33.336,
      "iterations": 2100,
      "loss": 0.5757951140403748,
      "perplexity": 1.7785440683364868,
      "step": 2100
    },
    {
      "epoch": 33.336,
      "iterations": 2100,
      "loss": 1.118854284286499,
      "perplexity": 3.061344623565674,
      "step": 2100
    },
    {
      "epoch": 33.336,
      "iterations": 2100,
      "loss": 0.8300866484642029,
      "perplexity": 2.293517589569092,
      "step": 2100
    },
    {
      "epoch": 33.336,
      "iterations": 2100,
      "loss": 0.6419961452484131,
      "perplexity": 1.9002702236175537,
      "step": 2100
    },
    {
      "epoch": 33.336,
      "iterations": 2100,
      "loss": 1.1658862829208374,
      "perplexity": 3.2087655067443848,
      "step": 2100
    },
    {
      "epoch": 33.336,
      "iterations": 2100,
      "loss": 1.2736115455627441,
      "perplexity": 3.5737359523773193,
      "step": 2100
    },
    {
      "epoch": 33.336,
      "iterations": 2100,
      "loss": 1.0347943305969238,
      "perplexity": 2.8145272731781006,
      "step": 2100
    },
    {
      "epoch": 33.336,
      "iterations": 2100,
      "loss": 0.7570005655288696,
      "perplexity": 2.1318721771240234,
      "step": 2100
    },
    {
      "epoch": 34.128,
      "grad_norm": 1.9045355319976807,
      "learning_rate": 0.00011636734693877551,
      "loss": 6.7174,
      "step": 2150
    },
    {
      "epoch": 34.128,
      "iterations": 2150,
      "loss": 0.4020758867263794,
      "perplexity": 1.494924783706665,
      "step": 2150
    },
    {
      "epoch": 34.128,
      "iterations": 2150,
      "loss": 0.3265365660190582,
      "perplexity": 1.38615882396698,
      "step": 2150
    },
    {
      "epoch": 34.128,
      "iterations": 2150,
      "loss": 0.9310849905014038,
      "perplexity": 2.5372605323791504,
      "step": 2150
    },
    {
      "epoch": 34.128,
      "iterations": 2150,
      "loss": 0.7361894845962524,
      "perplexity": 2.0879640579223633,
      "step": 2150
    },
    {
      "epoch": 34.128,
      "iterations": 2150,
      "loss": 1.1722790002822876,
      "perplexity": 3.229343891143799,
      "step": 2150
    },
    {
      "epoch": 34.128,
      "iterations": 2150,
      "loss": 0.993541955947876,
      "perplexity": 2.7007837295532227,
      "step": 2150
    },
    {
      "epoch": 34.128,
      "iterations": 2150,
      "loss": 1.0906524658203125,
      "perplexity": 2.976215124130249,
      "step": 2150
    },
    {
      "epoch": 34.128,
      "iterations": 2150,
      "loss": 1.3566725254058838,
      "perplexity": 3.8832502365112305,
      "step": 2150
    },
    {
      "epoch": 34.928,
      "grad_norm": 2.5227243900299072,
      "learning_rate": 0.0001143265306122449,
      "loss": 6.7596,
      "step": 2200
    },
    {
      "epoch": 34.928,
      "iterations": 2200,
      "loss": 0.3968872129917145,
      "perplexity": 1.4871881008148193,
      "step": 2200
    },
    {
      "epoch": 34.928,
      "iterations": 2200,
      "loss": 0.6211181282997131,
      "perplexity": 1.8610076904296875,
      "step": 2200
    },
    {
      "epoch": 34.928,
      "iterations": 2200,
      "loss": 1.096990704536438,
      "perplexity": 2.9951391220092773,
      "step": 2200
    },
    {
      "epoch": 34.928,
      "iterations": 2200,
      "loss": 1.1338399648666382,
      "perplexity": 3.1075665950775146,
      "step": 2200
    },
    {
      "epoch": 34.928,
      "iterations": 2200,
      "loss": 1.2800700664520264,
      "perplexity": 3.5968918800354004,
      "step": 2200
    },
    {
      "epoch": 34.928,
      "iterations": 2200,
      "loss": 1.4123740196228027,
      "perplexity": 4.105690956115723,
      "step": 2200
    },
    {
      "epoch": 34.928,
      "iterations": 2200,
      "loss": 1.080051064491272,
      "perplexity": 2.9448299407958984,
      "step": 2200
    },
    {
      "epoch": 34.928,
      "iterations": 2200,
      "loss": 0.8849406838417053,
      "perplexity": 2.4228408336639404,
      "step": 2200
    },
    {
      "epoch": 35.72,
      "grad_norm": 4.0831098556518555,
      "learning_rate": 0.00011228571428571429,
      "loss": 6.7583,
      "step": 2250
    },
    {
      "epoch": 35.72,
      "iterations": 2250,
      "loss": 1.1902378797531128,
      "perplexity": 3.287863254547119,
      "step": 2250
    },
    {
      "epoch": 35.72,
      "iterations": 2250,
      "loss": 1.296094298362732,
      "perplexity": 3.6549935340881348,
      "step": 2250
    },
    {
      "epoch": 35.72,
      "iterations": 2250,
      "loss": 0.34492993354797363,
      "perplexity": 1.411890983581543,
      "step": 2250
    },
    {
      "epoch": 35.72,
      "iterations": 2250,
      "loss": 0.3830319046974182,
      "perplexity": 1.4667248725891113,
      "step": 2250
    },
    {
      "epoch": 35.72,
      "iterations": 2250,
      "loss": 0.977666437625885,
      "perplexity": 2.658245801925659,
      "step": 2250
    },
    {
      "epoch": 35.72,
      "iterations": 2250,
      "loss": 0.8191357254981995,
      "perplexity": 2.268538475036621,
      "step": 2250
    },
    {
      "epoch": 35.72,
      "iterations": 2250,
      "loss": 0.35027408599853516,
      "perplexity": 1.4194566011428833,
      "step": 2250
    },
    {
      "epoch": 35.72,
      "iterations": 2250,
      "loss": 1.3472635746002197,
      "perplexity": 3.8468844890594482,
      "step": 2250
    },
    {
      "epoch": 36.512,
      "grad_norm": 2.49626088142395,
      "learning_rate": 0.00011024489795918366,
      "loss": 6.845,
      "step": 2300
    },
    {
      "epoch": 36.512,
      "iterations": 2300,
      "loss": 1.2075239419937134,
      "perplexity": 3.345191478729248,
      "step": 2300
    },
    {
      "epoch": 36.512,
      "iterations": 2300,
      "loss": 0.3322460353374481,
      "perplexity": 1.394095778465271,
      "step": 2300
    },
    {
      "epoch": 36.512,
      "iterations": 2300,
      "loss": 0.40247833728790283,
      "perplexity": 1.4955264329910278,
      "step": 2300
    },
    {
      "epoch": 36.512,
      "iterations": 2300,
      "loss": 1.1546417474746704,
      "perplexity": 3.172886610031128,
      "step": 2300
    },
    {
      "epoch": 36.512,
      "iterations": 2300,
      "loss": 1.1410974264144897,
      "perplexity": 3.1302013397216797,
      "step": 2300
    },
    {
      "epoch": 36.512,
      "iterations": 2300,
      "loss": 0.3712734580039978,
      "perplexity": 1.4495794773101807,
      "step": 2300
    },
    {
      "epoch": 36.512,
      "iterations": 2300,
      "loss": 0.3315871059894562,
      "perplexity": 1.3931773900985718,
      "step": 2300
    },
    {
      "epoch": 36.512,
      "iterations": 2300,
      "loss": 0.447131484746933,
      "perplexity": 1.5638198852539062,
      "step": 2300
    },
    {
      "epoch": 37.304,
      "grad_norm": 2.3822360038757324,
      "learning_rate": 0.00010820408163265308,
      "loss": 6.4155,
      "step": 2350
    },
    {
      "epoch": 37.304,
      "iterations": 2350,
      "loss": 0.37160736322402954,
      "perplexity": 1.4500634670257568,
      "step": 2350
    },
    {
      "epoch": 37.304,
      "iterations": 2350,
      "loss": 1.1186881065368652,
      "perplexity": 3.06083607673645,
      "step": 2350
    },
    {
      "epoch": 37.304,
      "iterations": 2350,
      "loss": 0.6104300022125244,
      "perplexity": 1.8412230014801025,
      "step": 2350
    },
    {
      "epoch": 37.304,
      "iterations": 2350,
      "loss": 1.3033281564712524,
      "perplexity": 3.6815290451049805,
      "step": 2350
    },
    {
      "epoch": 37.304,
      "iterations": 2350,
      "loss": 0.45799702405929565,
      "perplexity": 1.5809043645858765,
      "step": 2350
    },
    {
      "epoch": 37.304,
      "iterations": 2350,
      "loss": 0.33664625883102417,
      "perplexity": 1.4002436399459839,
      "step": 2350
    },
    {
      "epoch": 37.304,
      "iterations": 2350,
      "loss": 0.5308142304420471,
      "perplexity": 1.7003161907196045,
      "step": 2350
    },
    {
      "epoch": 37.304,
      "iterations": 2350,
      "loss": 0.9236958026885986,
      "perplexity": 2.5185813903808594,
      "step": 2350
    },
    {
      "epoch": 38.096,
      "grad_norm": 1.8110482692718506,
      "learning_rate": 0.00010616326530612246,
      "loss": 6.696,
      "step": 2400
    },
    {
      "epoch": 38.096,
      "iterations": 2400,
      "loss": 1.28285551071167,
      "perplexity": 3.6069247722625732,
      "step": 2400
    },
    {
      "epoch": 38.096,
      "iterations": 2400,
      "loss": 1.0870033502578735,
      "perplexity": 2.96537446975708,
      "step": 2400
    },
    {
      "epoch": 38.096,
      "iterations": 2400,
      "loss": 1.384895920753479,
      "perplexity": 3.9944100379943848,
      "step": 2400
    },
    {
      "epoch": 38.096,
      "iterations": 2400,
      "loss": 1.1794676780700684,
      "perplexity": 3.2526423931121826,
      "step": 2400
    },
    {
      "epoch": 38.096,
      "iterations": 2400,
      "loss": 1.0114136934280396,
      "perplexity": 2.7494852542877197,
      "step": 2400
    },
    {
      "epoch": 38.096,
      "iterations": 2400,
      "loss": 0.32974758744239807,
      "perplexity": 1.3906170129776,
      "step": 2400
    },
    {
      "epoch": 38.096,
      "iterations": 2400,
      "loss": 0.5661184191703796,
      "perplexity": 1.7614166736602783,
      "step": 2400
    },
    {
      "epoch": 38.096,
      "iterations": 2400,
      "loss": 0.9166823029518127,
      "perplexity": 2.50097918510437,
      "step": 2400
    },
    {
      "epoch": 38.896,
      "grad_norm": 2.5145890712738037,
      "learning_rate": 0.00010412244897959184,
      "loss": 6.6738,
      "step": 2450
    },
    {
      "epoch": 38.896,
      "iterations": 2450,
      "loss": 0.654381275177002,
      "perplexity": 1.9239517450332642,
      "step": 2450
    },
    {
      "epoch": 38.896,
      "iterations": 2450,
      "loss": 0.42643648386001587,
      "perplexity": 1.5317893028259277,
      "step": 2450
    },
    {
      "epoch": 38.896,
      "iterations": 2450,
      "loss": 1.2386049032211304,
      "perplexity": 3.450795888900757,
      "step": 2450
    },
    {
      "epoch": 38.896,
      "iterations": 2450,
      "loss": 0.4214322566986084,
      "perplexity": 1.524142861366272,
      "step": 2450
    },
    {
      "epoch": 38.896,
      "iterations": 2450,
      "loss": 1.2604633569717407,
      "perplexity": 3.527055263519287,
      "step": 2450
    },
    {
      "epoch": 38.896,
      "iterations": 2450,
      "loss": 0.8258011937141418,
      "perplexity": 2.2837095260620117,
      "step": 2450
    },
    {
      "epoch": 38.896,
      "iterations": 2450,
      "loss": 1.1071144342422485,
      "perplexity": 3.0256149768829346,
      "step": 2450
    },
    {
      "epoch": 38.896,
      "iterations": 2450,
      "loss": 0.33689847588539124,
      "perplexity": 1.4005968570709229,
      "step": 2450
    },
    {
      "epoch": 39.688,
      "grad_norm": 2.911667585372925,
      "learning_rate": 0.00010208163265306123,
      "loss": 6.3612,
      "step": 2500
    },
    {
      "epoch": 39.688,
      "iterations": 2500,
      "loss": 1.378870964050293,
      "perplexity": 3.970416307449341,
      "step": 2500
    },
    {
      "epoch": 39.688,
      "iterations": 2500,
      "loss": 0.35198602080345154,
      "perplexity": 1.4218887090682983,
      "step": 2500
    },
    {
      "epoch": 39.688,
      "iterations": 2500,
      "loss": 0.7995550036430359,
      "perplexity": 2.22455096244812,
      "step": 2500
    },
    {
      "epoch": 39.688,
      "iterations": 2500,
      "loss": 1.0081772804260254,
      "perplexity": 2.740601062774658,
      "step": 2500
    },
    {
      "epoch": 39.688,
      "iterations": 2500,
      "loss": 0.4081290662288666,
      "perplexity": 1.504001259803772,
      "step": 2500
    },
    {
      "epoch": 39.688,
      "iterations": 2500,
      "loss": 0.3321061432361603,
      "perplexity": 1.393900752067566,
      "step": 2500
    },
    {
      "epoch": 39.688,
      "iterations": 2500,
      "loss": 1.2056052684783936,
      "perplexity": 3.3387792110443115,
      "step": 2500
    },
    {
      "epoch": 39.688,
      "iterations": 2500,
      "loss": 1.053544521331787,
      "perplexity": 2.867798089981079,
      "step": 2500
    },
    {
      "epoch": 40.48,
      "grad_norm": 2.9669220447540283,
      "learning_rate": 0.00010004081632653062,
      "loss": 6.5314,
      "step": 2550
    },
    {
      "epoch": 40.48,
      "iterations": 2550,
      "loss": 0.3438749313354492,
      "perplexity": 1.4104022979736328,
      "step": 2550
    },
    {
      "epoch": 40.48,
      "iterations": 2550,
      "loss": 1.1934458017349243,
      "perplexity": 3.2984273433685303,
      "step": 2550
    },
    {
      "epoch": 40.48,
      "iterations": 2550,
      "loss": 0.3220255374908447,
      "perplexity": 1.3799200057983398,
      "step": 2550
    },
    {
      "epoch": 40.48,
      "iterations": 2550,
      "loss": 1.338175892829895,
      "perplexity": 3.8120834827423096,
      "step": 2550
    },
    {
      "epoch": 40.48,
      "iterations": 2550,
      "loss": 0.6927327513694763,
      "perplexity": 1.999171257019043,
      "step": 2550
    },
    {
      "epoch": 40.48,
      "iterations": 2550,
      "loss": 1.1132150888442993,
      "perplexity": 3.0441298484802246,
      "step": 2550
    },
    {
      "epoch": 40.48,
      "iterations": 2550,
      "loss": 1.3752267360687256,
      "perplexity": 3.9559736251831055,
      "step": 2550
    },
    {
      "epoch": 40.48,
      "iterations": 2550,
      "loss": 1.0385935306549072,
      "perplexity": 2.8252406120300293,
      "step": 2550
    },
    {
      "epoch": 41.272,
      "grad_norm": 1.6814812421798706,
      "learning_rate": 9.8e-05,
      "loss": 6.5792,
      "step": 2600
    },
    {
      "epoch": 41.272,
      "iterations": 2600,
      "loss": 0.8204245567321777,
      "perplexity": 2.2714638710021973,
      "step": 2600
    },
    {
      "epoch": 41.272,
      "iterations": 2600,
      "loss": 1.1831966638565063,
      "perplexity": 3.264793872833252,
      "step": 2600
    },
    {
      "epoch": 41.272,
      "iterations": 2600,
      "loss": 0.5311887264251709,
      "perplexity": 1.7009531259536743,
      "step": 2600
    },
    {
      "epoch": 41.272,
      "iterations": 2600,
      "loss": 0.2900431752204895,
      "perplexity": 1.3364851474761963,
      "step": 2600
    },
    {
      "epoch": 41.272,
      "iterations": 2600,
      "loss": 0.29275989532470703,
      "perplexity": 1.3401209115982056,
      "step": 2600
    },
    {
      "epoch": 41.272,
      "iterations": 2600,
      "loss": 1.0173184871673584,
      "perplexity": 2.765768527984619,
      "step": 2600
    },
    {
      "epoch": 41.272,
      "iterations": 2600,
      "loss": 0.7089006900787354,
      "perplexity": 2.0317564010620117,
      "step": 2600
    },
    {
      "epoch": 41.272,
      "iterations": 2600,
      "loss": 0.29602256417274475,
      "perplexity": 1.3445005416870117,
      "step": 2600
    },
    {
      "epoch": 42.064,
      "grad_norm": 2.724421501159668,
      "learning_rate": 9.595918367346939e-05,
      "loss": 6.4984,
      "step": 2650
    },
    {
      "epoch": 42.064,
      "iterations": 2650,
      "loss": 1.103438377380371,
      "perplexity": 3.0145132541656494,
      "step": 2650
    },
    {
      "epoch": 42.064,
      "iterations": 2650,
      "loss": 0.43346652388572693,
      "perplexity": 1.5425957441329956,
      "step": 2650
    },
    {
      "epoch": 42.064,
      "iterations": 2650,
      "loss": 0.8470590710639954,
      "perplexity": 2.3327763080596924,
      "step": 2650
    },
    {
      "epoch": 42.064,
      "iterations": 2650,
      "loss": 1.2158666849136353,
      "perplexity": 3.373216152191162,
      "step": 2650
    },
    {
      "epoch": 42.064,
      "iterations": 2650,
      "loss": 0.3031228184700012,
      "perplexity": 1.3540807962417603,
      "step": 2650
    },
    {
      "epoch": 42.064,
      "iterations": 2650,
      "loss": 0.30044877529144287,
      "perplexity": 1.3504648208618164,
      "step": 2650
    },
    {
      "epoch": 42.064,
      "iterations": 2650,
      "loss": 0.2764381468296051,
      "perplexity": 1.3184254169464111,
      "step": 2650
    },
    {
      "epoch": 42.064,
      "iterations": 2650,
      "loss": 1.1023138761520386,
      "perplexity": 3.011125326156616,
      "step": 2650
    },
    {
      "epoch": 42.864,
      "grad_norm": 4.353557586669922,
      "learning_rate": 9.391836734693879e-05,
      "loss": 6.4816,
      "step": 2700
    },
    {
      "epoch": 42.864,
      "iterations": 2700,
      "loss": 0.31930628418922424,
      "perplexity": 1.3761727809906006,
      "step": 2700
    },
    {
      "epoch": 42.864,
      "iterations": 2700,
      "loss": 0.2981424629688263,
      "perplexity": 1.3473536968231201,
      "step": 2700
    },
    {
      "epoch": 42.864,
      "iterations": 2700,
      "loss": 0.4949515461921692,
      "perplexity": 1.6404187679290771,
      "step": 2700
    },
    {
      "epoch": 42.864,
      "iterations": 2700,
      "loss": 1.1962032318115234,
      "perplexity": 3.307535171508789,
      "step": 2700
    },
    {
      "epoch": 42.864,
      "iterations": 2700,
      "loss": 0.323558509349823,
      "perplexity": 1.3820369243621826,
      "step": 2700
    },
    {
      "epoch": 42.864,
      "iterations": 2700,
      "loss": 0.29238003492355347,
      "perplexity": 1.3396120071411133,
      "step": 2700
    },
    {
      "epoch": 42.864,
      "iterations": 2700,
      "loss": 0.2889639735221863,
      "perplexity": 1.3350435495376587,
      "step": 2700
    },
    {
      "epoch": 42.864,
      "iterations": 2700,
      "loss": 0.33366718888282776,
      "perplexity": 1.3960784673690796,
      "step": 2700
    },
    {
      "epoch": 43.656,
      "grad_norm": 1.840234398841858,
      "learning_rate": 9.187755102040818e-05,
      "loss": 6.2016,
      "step": 2750
    },
    {
      "epoch": 43.656,
      "iterations": 2750,
      "loss": 0.2772681415081024,
      "perplexity": 1.319520115852356,
      "step": 2750
    },
    {
      "epoch": 43.656,
      "iterations": 2750,
      "loss": 1.0049657821655273,
      "perplexity": 2.731813907623291,
      "step": 2750
    },
    {
      "epoch": 43.656,
      "iterations": 2750,
      "loss": 1.23862886428833,
      "perplexity": 3.450878620147705,
      "step": 2750
    },
    {
      "epoch": 43.656,
      "iterations": 2750,
      "loss": 0.7609844207763672,
      "perplexity": 2.1403822898864746,
      "step": 2750
    },
    {
      "epoch": 43.656,
      "iterations": 2750,
      "loss": 0.3049612045288086,
      "perplexity": 1.3565723896026611,
      "step": 2750
    },
    {
      "epoch": 43.656,
      "iterations": 2750,
      "loss": 0.5810871720314026,
      "perplexity": 1.7879812717437744,
      "step": 2750
    },
    {
      "epoch": 43.656,
      "iterations": 2750,
      "loss": 0.251554399728775,
      "perplexity": 1.2860229015350342,
      "step": 2750
    },
    {
      "epoch": 43.656,
      "iterations": 2750,
      "loss": 0.34734147787094116,
      "perplexity": 1.415299892425537,
      "step": 2750
    },
    {
      "epoch": 44.448,
      "grad_norm": 2.5284619331359863,
      "learning_rate": 8.983673469387755e-05,
      "loss": 6.4288,
      "step": 2800
    },
    {
      "epoch": 44.448,
      "iterations": 2800,
      "loss": 0.9867525696754456,
      "perplexity": 2.682509183883667,
      "step": 2800
    },
    {
      "epoch": 44.448,
      "iterations": 2800,
      "loss": 0.8926606178283691,
      "perplexity": 2.441617250442505,
      "step": 2800
    },
    {
      "epoch": 44.448,
      "iterations": 2800,
      "loss": 1.3355116844177246,
      "perplexity": 3.801940679550171,
      "step": 2800
    },
    {
      "epoch": 44.448,
      "iterations": 2800,
      "loss": 0.2852993309497833,
      "perplexity": 1.330160140991211,
      "step": 2800
    },
    {
      "epoch": 44.448,
      "iterations": 2800,
      "loss": 0.3023141026496887,
      "perplexity": 1.3529860973358154,
      "step": 2800
    },
    {
      "epoch": 44.448,
      "iterations": 2800,
      "loss": 1.1815725564956665,
      "perplexity": 3.259495973587036,
      "step": 2800
    },
    {
      "epoch": 44.448,
      "iterations": 2800,
      "loss": 0.43259570002555847,
      "perplexity": 1.5412529706954956,
      "step": 2800
    },
    {
      "epoch": 44.448,
      "iterations": 2800,
      "loss": 0.2999016344547272,
      "perplexity": 1.3497260808944702,
      "step": 2800
    },
    {
      "epoch": 45.24,
      "grad_norm": 2.799603223800659,
      "learning_rate": 8.779591836734694e-05,
      "loss": 6.3281,
      "step": 2850
    },
    {
      "epoch": 45.24,
      "iterations": 2850,
      "loss": 0.3052060306072235,
      "perplexity": 1.3569045066833496,
      "step": 2850
    },
    {
      "epoch": 45.24,
      "iterations": 2850,
      "loss": 0.24314582347869873,
      "perplexity": 1.2752546072006226,
      "step": 2850
    },
    {
      "epoch": 45.24,
      "iterations": 2850,
      "loss": 0.3011242747306824,
      "perplexity": 1.3513773679733276,
      "step": 2850
    },
    {
      "epoch": 45.24,
      "iterations": 2850,
      "loss": 0.3872861862182617,
      "perplexity": 1.4729779958724976,
      "step": 2850
    },
    {
      "epoch": 45.24,
      "iterations": 2850,
      "loss": 0.26868772506713867,
      "perplexity": 1.3082464933395386,
      "step": 2850
    },
    {
      "epoch": 45.24,
      "iterations": 2850,
      "loss": 1.0258327722549438,
      "perplexity": 2.789417266845703,
      "step": 2850
    },
    {
      "epoch": 45.24,
      "iterations": 2850,
      "loss": 0.273303359746933,
      "perplexity": 1.3142988681793213,
      "step": 2850
    },
    {
      "epoch": 45.24,
      "iterations": 2850,
      "loss": 1.09503173828125,
      "perplexity": 2.9892773628234863,
      "step": 2850
    },
    {
      "epoch": 46.032,
      "grad_norm": 2.9153573513031006,
      "learning_rate": 8.575510204081633e-05,
      "loss": 6.1963,
      "step": 2900
    },
    {
      "epoch": 46.032,
      "iterations": 2900,
      "loss": 0.3204839527606964,
      "perplexity": 1.3777943849563599,
      "step": 2900
    },
    {
      "epoch": 46.032,
      "iterations": 2900,
      "loss": 1.2500981092453003,
      "perplexity": 3.49068546295166,
      "step": 2900
    },
    {
      "epoch": 46.032,
      "iterations": 2900,
      "loss": 0.3649519681930542,
      "perplexity": 1.4404449462890625,
      "step": 2900
    },
    {
      "epoch": 46.032,
      "iterations": 2900,
      "loss": 0.2577422261238098,
      "perplexity": 1.2940051555633545,
      "step": 2900
    },
    {
      "epoch": 46.032,
      "iterations": 2900,
      "loss": 1.0033543109893799,
      "perplexity": 2.727415084838867,
      "step": 2900
    },
    {
      "epoch": 46.032,
      "iterations": 2900,
      "loss": 0.9973037242889404,
      "perplexity": 2.7109625339508057,
      "step": 2900
    },
    {
      "epoch": 46.032,
      "iterations": 2900,
      "loss": 0.46492063999176025,
      "perplexity": 1.5918878316879272,
      "step": 2900
    },
    {
      "epoch": 46.032,
      "iterations": 2900,
      "loss": 1.2917031049728394,
      "perplexity": 3.638978958129883,
      "step": 2900
    },
    {
      "epoch": 46.832,
      "grad_norm": 1.6286619901657104,
      "learning_rate": 8.371428571428572e-05,
      "loss": 6.283,
      "step": 2950
    },
    {
      "epoch": 46.832,
      "iterations": 2950,
      "loss": 0.6815657019615173,
      "perplexity": 1.9769705533981323,
      "step": 2950
    },
    {
      "epoch": 46.832,
      "iterations": 2950,
      "loss": 1.180118203163147,
      "perplexity": 3.2547590732574463,
      "step": 2950
    },
    {
      "epoch": 46.832,
      "iterations": 2950,
      "loss": 0.5622034072875977,
      "perplexity": 1.7545342445373535,
      "step": 2950
    },
    {
      "epoch": 46.832,
      "iterations": 2950,
      "loss": 1.433685064315796,
      "perplexity": 4.194126129150391,
      "step": 2950
    },
    {
      "epoch": 46.832,
      "iterations": 2950,
      "loss": 0.9546670913696289,
      "perplexity": 2.5978057384490967,
      "step": 2950
    },
    {
      "epoch": 46.832,
      "iterations": 2950,
      "loss": 0.35839784145355225,
      "perplexity": 1.431034803390503,
      "step": 2950
    },
    {
      "epoch": 46.832,
      "iterations": 2950,
      "loss": 0.23236031830310822,
      "perplexity": 1.261574149131775,
      "step": 2950
    },
    {
      "epoch": 46.832,
      "iterations": 2950,
      "loss": 0.314433217048645,
      "perplexity": 1.3694828748703003,
      "step": 2950
    },
    {
      "epoch": 47.624,
      "grad_norm": 2.51935076713562,
      "learning_rate": 8.167346938775511e-05,
      "loss": 6.2347,
      "step": 3000
    },
    {
      "epoch": 47.624,
      "iterations": 3000,
      "loss": 1.0283541679382324,
      "perplexity": 2.796459436416626,
      "step": 3000
    },
    {
      "epoch": 47.624,
      "iterations": 3000,
      "loss": 0.2684698700904846,
      "perplexity": 1.3079614639282227,
      "step": 3000
    },
    {
      "epoch": 47.624,
      "iterations": 3000,
      "loss": 0.7835467457771301,
      "perplexity": 2.189223051071167,
      "step": 3000
    },
    {
      "epoch": 47.624,
      "iterations": 3000,
      "loss": 1.297601342201233,
      "perplexity": 3.660506010055542,
      "step": 3000
    },
    {
      "epoch": 47.624,
      "iterations": 3000,
      "loss": 0.6314204931259155,
      "perplexity": 1.8802796602249146,
      "step": 3000
    },
    {
      "epoch": 47.624,
      "iterations": 3000,
      "loss": 0.8198632001876831,
      "perplexity": 2.2701892852783203,
      "step": 3000
    },
    {
      "epoch": 47.624,
      "iterations": 3000,
      "loss": 0.2704973816871643,
      "perplexity": 1.310616135597229,
      "step": 3000
    },
    {
      "epoch": 47.624,
      "iterations": 3000,
      "loss": 0.2824925482273102,
      "perplexity": 1.3264318704605103,
      "step": 3000
    },
    {
      "epoch": 48.416,
      "grad_norm": 3.7497777938842773,
      "learning_rate": 7.96326530612245e-05,
      "loss": 6.1383,
      "step": 3050
    },
    {
      "epoch": 48.416,
      "iterations": 3050,
      "loss": 1.2953935861587524,
      "perplexity": 3.652433156967163,
      "step": 3050
    },
    {
      "epoch": 48.416,
      "iterations": 3050,
      "loss": 0.19706755876541138,
      "perplexity": 1.217826247215271,
      "step": 3050
    },
    {
      "epoch": 48.416,
      "iterations": 3050,
      "loss": 0.25855886936187744,
      "perplexity": 1.2950624227523804,
      "step": 3050
    },
    {
      "epoch": 48.416,
      "iterations": 3050,
      "loss": 1.2944045066833496,
      "perplexity": 3.648822546005249,
      "step": 3050
    },
    {
      "epoch": 48.416,
      "iterations": 3050,
      "loss": 0.9740549921989441,
      "perplexity": 2.64866304397583,
      "step": 3050
    },
    {
      "epoch": 48.416,
      "iterations": 3050,
      "loss": 0.4445429742336273,
      "perplexity": 1.5597771406173706,
      "step": 3050
    },
    {
      "epoch": 48.416,
      "iterations": 3050,
      "loss": 1.3740129470825195,
      "perplexity": 3.95117449760437,
      "step": 3050
    },
    {
      "epoch": 48.416,
      "iterations": 3050,
      "loss": 0.3035314679145813,
      "perplexity": 1.354634165763855,
      "step": 3050
    },
    {
      "epoch": 49.208,
      "grad_norm": 2.9975903034210205,
      "learning_rate": 7.759183673469388e-05,
      "loss": 6.3159,
      "step": 3100
    },
    {
      "epoch": 49.208,
      "iterations": 3100,
      "loss": 1.3217402696609497,
      "perplexity": 3.74994158744812,
      "step": 3100
    },
    {
      "epoch": 49.208,
      "iterations": 3100,
      "loss": 0.3084864318370819,
      "perplexity": 1.3613630533218384,
      "step": 3100
    },
    {
      "epoch": 49.208,
      "iterations": 3100,
      "loss": 0.3680371344089508,
      "perplexity": 1.4448957443237305,
      "step": 3100
    },
    {
      "epoch": 49.208,
      "iterations": 3100,
      "loss": 1.1893393993377686,
      "perplexity": 3.2849106788635254,
      "step": 3100
    },
    {
      "epoch": 49.208,
      "iterations": 3100,
      "loss": 1.34385085105896,
      "perplexity": 3.8337786197662354,
      "step": 3100
    },
    {
      "epoch": 49.208,
      "iterations": 3100,
      "loss": 1.2232016324996948,
      "perplexity": 3.3980495929718018,
      "step": 3100
    },
    {
      "epoch": 49.208,
      "iterations": 3100,
      "loss": 0.23101642727851868,
      "perplexity": 1.2598799467086792,
      "step": 3100
    },
    {
      "epoch": 49.208,
      "iterations": 3100,
      "loss": 0.2682011127471924,
      "perplexity": 1.3076101541519165,
      "step": 3100
    },
    {
      "epoch": 50.0,
      "grad_norm": 3.3090853691101074,
      "learning_rate": 7.555102040816326e-05,
      "loss": 6.147,
      "step": 3150
    },
    {
      "epoch": 50.0,
      "iterations": 3150,
      "loss": 0.6356580853462219,
      "perplexity": 1.8882644176483154,
      "step": 3150
    },
    {
      "epoch": 50.0,
      "iterations": 3150,
      "loss": 1.35953950881958,
      "perplexity": 3.894399404525757,
      "step": 3150
    },
    {
      "epoch": 50.0,
      "iterations": 3150,
      "loss": 0.8804760575294495,
      "perplexity": 2.4120476245880127,
      "step": 3150
    },
    {
      "epoch": 50.0,
      "iterations": 3150,
      "loss": 1.3708668947219849,
      "perplexity": 3.9387636184692383,
      "step": 3150
    },
    {
      "epoch": 50.0,
      "iterations": 3150,
      "loss": 1.0769221782684326,
      "perplexity": 2.9356303215026855,
      "step": 3150
    },
    {
      "epoch": 50.0,
      "iterations": 3150,
      "loss": 0.6985310316085815,
      "perplexity": 2.0107967853546143,
      "step": 3150
    },
    {
      "epoch": 50.0,
      "iterations": 3150,
      "loss": 1.2218810319900513,
      "perplexity": 3.3935651779174805,
      "step": 3150
    },
    {
      "epoch": 50.0,
      "iterations": 3150,
      "loss": 0.233549565076828,
      "perplexity": 1.2630754709243774,
      "step": 3150
    },
    {
      "epoch": 50.8,
      "grad_norm": 2.088336944580078,
      "learning_rate": 7.351020408163266e-05,
      "loss": 6.1841,
      "step": 3200
    },
    {
      "epoch": 50.8,
      "iterations": 3200,
      "loss": 0.6610938906669617,
      "perplexity": 1.9369099140167236,
      "step": 3200
    },
    {
      "epoch": 50.8,
      "iterations": 3200,
      "loss": 0.5582146644592285,
      "perplexity": 1.7475497722625732,
      "step": 3200
    },
    {
      "epoch": 50.8,
      "iterations": 3200,
      "loss": 0.7215964198112488,
      "perplexity": 2.057715654373169,
      "step": 3200
    },
    {
      "epoch": 50.8,
      "iterations": 3200,
      "loss": 0.19750748574733734,
      "perplexity": 1.2183620929718018,
      "step": 3200
    },
    {
      "epoch": 50.8,
      "iterations": 3200,
      "loss": 0.4885254502296448,
      "perplexity": 1.6299110651016235,
      "step": 3200
    },
    {
      "epoch": 50.8,
      "iterations": 3200,
      "loss": 0.9726163744926453,
      "perplexity": 2.644855499267578,
      "step": 3200
    },
    {
      "epoch": 50.8,
      "iterations": 3200,
      "loss": 0.24624954164028168,
      "perplexity": 1.2792187929153442,
      "step": 3200
    },
    {
      "epoch": 50.8,
      "iterations": 3200,
      "loss": 0.23280325531959534,
      "perplexity": 1.2621331214904785,
      "step": 3200
    },
    {
      "epoch": 51.592,
      "grad_norm": 2.344987154006958,
      "learning_rate": 7.146938775510205e-05,
      "loss": 5.9874,
      "step": 3250
    },
    {
      "epoch": 51.592,
      "iterations": 3250,
      "loss": 1.1805347204208374,
      "perplexity": 3.256114959716797,
      "step": 3250
    },
    {
      "epoch": 51.592,
      "iterations": 3250,
      "loss": 1.321668267250061,
      "perplexity": 3.749671459197998,
      "step": 3250
    },
    {
      "epoch": 51.592,
      "iterations": 3250,
      "loss": 1.185608983039856,
      "perplexity": 3.272679090499878,
      "step": 3250
    },
    {
      "epoch": 51.592,
      "iterations": 3250,
      "loss": 1.3871898651123047,
      "perplexity": 4.0035834312438965,
      "step": 3250
    },
    {
      "epoch": 51.592,
      "iterations": 3250,
      "loss": 0.22275042533874512,
      "perplexity": 1.2495086193084717,
      "step": 3250
    },
    {
      "epoch": 51.592,
      "iterations": 3250,
      "loss": 0.34987470507621765,
      "perplexity": 1.4188897609710693,
      "step": 3250
    },
    {
      "epoch": 51.592,
      "iterations": 3250,
      "loss": 0.9858918190002441,
      "perplexity": 2.6802010536193848,
      "step": 3250
    },
    {
      "epoch": 51.592,
      "iterations": 3250,
      "loss": 0.23161736130714417,
      "perplexity": 1.2606372833251953,
      "step": 3250
    },
    {
      "epoch": 52.384,
      "grad_norm": 3.5899038314819336,
      "learning_rate": 6.942857142857143e-05,
      "loss": 6.2419,
      "step": 3300
    },
    {
      "epoch": 52.384,
      "iterations": 3300,
      "loss": 0.21962493658065796,
      "perplexity": 1.2456094026565552,
      "step": 3300
    },
    {
      "epoch": 52.384,
      "iterations": 3300,
      "loss": 0.887100338935852,
      "perplexity": 2.4280788898468018,
      "step": 3300
    },
    {
      "epoch": 52.384,
      "iterations": 3300,
      "loss": 0.17879609763622284,
      "perplexity": 1.1957768201828003,
      "step": 3300
    },
    {
      "epoch": 52.384,
      "iterations": 3300,
      "loss": 0.22385412454605103,
      "perplexity": 1.2508885860443115,
      "step": 3300
    },
    {
      "epoch": 52.384,
      "iterations": 3300,
      "loss": 1.1004829406738281,
      "perplexity": 3.005617141723633,
      "step": 3300
    },
    {
      "epoch": 52.384,
      "iterations": 3300,
      "loss": 1.3410712480545044,
      "perplexity": 3.823136806488037,
      "step": 3300
    },
    {
      "epoch": 52.384,
      "iterations": 3300,
      "loss": 1.062764048576355,
      "perplexity": 2.894360065460205,
      "step": 3300
    },
    {
      "epoch": 52.384,
      "iterations": 3300,
      "loss": 0.2129293829202652,
      "perplexity": 1.2372972965240479,
      "step": 3300
    },
    {
      "epoch": 53.176,
      "grad_norm": 3.0280208587646484,
      "learning_rate": 6.738775510204081e-05,
      "loss": 5.9738,
      "step": 3350
    },
    {
      "epoch": 53.176,
      "iterations": 3350,
      "loss": 1.2870689630508423,
      "perplexity": 3.622154474258423,
      "step": 3350
    },
    {
      "epoch": 53.176,
      "iterations": 3350,
      "loss": 0.649559497833252,
      "perplexity": 1.914697289466858,
      "step": 3350
    },
    {
      "epoch": 53.176,
      "iterations": 3350,
      "loss": 0.6884886622428894,
      "perplexity": 1.9907045364379883,
      "step": 3350
    },
    {
      "epoch": 53.176,
      "iterations": 3350,
      "loss": 0.5195109844207764,
      "perplexity": 1.6812052726745605,
      "step": 3350
    },
    {
      "epoch": 53.176,
      "iterations": 3350,
      "loss": 1.28434419631958,
      "perplexity": 3.6122982501983643,
      "step": 3350
    },
    {
      "epoch": 53.176,
      "iterations": 3350,
      "loss": 1.391678810119629,
      "perplexity": 4.0215959548950195,
      "step": 3350
    },
    {
      "epoch": 53.176,
      "iterations": 3350,
      "loss": 1.0213699340820312,
      "perplexity": 2.776996374130249,
      "step": 3350
    },
    {
      "epoch": 53.176,
      "iterations": 3350,
      "loss": 1.1001121997833252,
      "perplexity": 3.004503011703491,
      "step": 3350
    },
    {
      "epoch": 53.976,
      "grad_norm": 2.788973331451416,
      "learning_rate": 6.53469387755102e-05,
      "loss": 5.9925,
      "step": 3400
    },
    {
      "epoch": 53.976,
      "iterations": 3400,
      "loss": 1.3739700317382812,
      "perplexity": 3.951005220413208,
      "step": 3400
    },
    {
      "epoch": 53.976,
      "iterations": 3400,
      "loss": 1.1737940311431885,
      "perplexity": 3.2342400550842285,
      "step": 3400
    },
    {
      "epoch": 53.976,
      "iterations": 3400,
      "loss": 1.2906850576400757,
      "perplexity": 3.6352760791778564,
      "step": 3400
    },
    {
      "epoch": 53.976,
      "iterations": 3400,
      "loss": 1.0852490663528442,
      "perplexity": 2.960176944732666,
      "step": 3400
    },
    {
      "epoch": 53.976,
      "iterations": 3400,
      "loss": 0.1714891791343689,
      "perplexity": 1.1870713233947754,
      "step": 3400
    },
    {
      "epoch": 53.976,
      "iterations": 3400,
      "loss": 0.954407811164856,
      "perplexity": 2.5971322059631348,
      "step": 3400
    },
    {
      "epoch": 53.976,
      "iterations": 3400,
      "loss": 0.41015371680259705,
      "perplexity": 1.5070494413375854,
      "step": 3400
    },
    {
      "epoch": 53.976,
      "iterations": 3400,
      "loss": 1.3621976375579834,
      "perplexity": 3.9047651290893555,
      "step": 3400
    },
    {
      "epoch": 54.768,
      "grad_norm": 2.435955286026001,
      "learning_rate": 6.330612244897959e-05,
      "loss": 5.8554,
      "step": 3450
    },
    {
      "epoch": 54.768,
      "iterations": 3450,
      "loss": 0.8753597736358643,
      "perplexity": 2.3997385501861572,
      "step": 3450
    },
    {
      "epoch": 54.768,
      "iterations": 3450,
      "loss": 0.22505724430084229,
      "perplexity": 1.252394437789917,
      "step": 3450
    },
    {
      "epoch": 54.768,
      "iterations": 3450,
      "loss": 0.6002281308174133,
      "perplexity": 1.822534441947937,
      "step": 3450
    },
    {
      "epoch": 54.768,
      "iterations": 3450,
      "loss": 1.2293081283569336,
      "perplexity": 3.41886305809021,
      "step": 3450
    },
    {
      "epoch": 54.768,
      "iterations": 3450,
      "loss": 0.5835893154144287,
      "perplexity": 1.792460560798645,
      "step": 3450
    },
    {
      "epoch": 54.768,
      "iterations": 3450,
      "loss": 0.7405146956443787,
      "perplexity": 2.0970146656036377,
      "step": 3450
    },
    {
      "epoch": 54.768,
      "iterations": 3450,
      "loss": 1.4125620126724243,
      "perplexity": 4.1064629554748535,
      "step": 3450
    },
    {
      "epoch": 54.768,
      "iterations": 3450,
      "loss": 0.7077719569206238,
      "perplexity": 2.0294644832611084,
      "step": 3450
    },
    {
      "epoch": 55.56,
      "grad_norm": 2.208984136581421,
      "learning_rate": 6.126530612244898e-05,
      "loss": 6.3205,
      "step": 3500
    },
    {
      "epoch": 55.56,
      "iterations": 3500,
      "loss": 0.48541051149368286,
      "perplexity": 1.6248419284820557,
      "step": 3500
    },
    {
      "epoch": 55.56,
      "iterations": 3500,
      "loss": 0.6020033955574036,
      "perplexity": 1.8257728815078735,
      "step": 3500
    },
    {
      "epoch": 55.56,
      "iterations": 3500,
      "loss": 0.962264895439148,
      "perplexity": 2.6176183223724365,
      "step": 3500
    },
    {
      "epoch": 55.56,
      "iterations": 3500,
      "loss": 0.2035817801952362,
      "perplexity": 1.225785493850708,
      "step": 3500
    },
    {
      "epoch": 55.56,
      "iterations": 3500,
      "loss": 1.2457382678985596,
      "perplexity": 3.4754996299743652,
      "step": 3500
    },
    {
      "epoch": 55.56,
      "iterations": 3500,
      "loss": 0.20063726603984833,
      "perplexity": 1.2221814393997192,
      "step": 3500
    },
    {
      "epoch": 55.56,
      "iterations": 3500,
      "loss": 0.2568696141242981,
      "perplexity": 1.2928766012191772,
      "step": 3500
    },
    {
      "epoch": 55.56,
      "iterations": 3500,
      "loss": 1.1903808116912842,
      "perplexity": 3.2883331775665283,
      "step": 3500
    },
    {
      "epoch": 56.352,
      "grad_norm": 2.7033371925354004,
      "learning_rate": 5.922448979591837e-05,
      "loss": 6.032,
      "step": 3550
    },
    {
      "epoch": 56.352,
      "iterations": 3550,
      "loss": 0.1950119584798813,
      "perplexity": 1.2153254747390747,
      "step": 3550
    },
    {
      "epoch": 56.352,
      "iterations": 3550,
      "loss": 0.6598159074783325,
      "perplexity": 1.9344362020492554,
      "step": 3550
    },
    {
      "epoch": 56.352,
      "iterations": 3550,
      "loss": 1.0060005187988281,
      "perplexity": 2.7346420288085938,
      "step": 3550
    },
    {
      "epoch": 56.352,
      "iterations": 3550,
      "loss": 0.2253442406654358,
      "perplexity": 1.2527538537979126,
      "step": 3550
    },
    {
      "epoch": 56.352,
      "iterations": 3550,
      "loss": 1.097335696220398,
      "perplexity": 2.9961726665496826,
      "step": 3550
    },
    {
      "epoch": 56.352,
      "iterations": 3550,
      "loss": 1.0574349164962769,
      "perplexity": 2.878976821899414,
      "step": 3550
    },
    {
      "epoch": 56.352,
      "iterations": 3550,
      "loss": 1.3858646154403687,
      "perplexity": 3.998281478881836,
      "step": 3550
    },
    {
      "epoch": 56.352,
      "iterations": 3550,
      "loss": 1.0082141160964966,
      "perplexity": 2.7407021522521973,
      "step": 3550
    },
    {
      "epoch": 57.144,
      "grad_norm": 2.3838162422180176,
      "learning_rate": 5.718367346938775e-05,
      "loss": 5.8127,
      "step": 3600
    },
    {
      "epoch": 57.144,
      "iterations": 3600,
      "loss": 1.214456558227539,
      "perplexity": 3.3684630393981934,
      "step": 3600
    },
    {
      "epoch": 57.144,
      "iterations": 3600,
      "loss": 1.3639822006225586,
      "perplexity": 3.9117395877838135,
      "step": 3600
    },
    {
      "epoch": 57.144,
      "iterations": 3600,
      "loss": 0.9172038435935974,
      "perplexity": 2.502283811569214,
      "step": 3600
    },
    {
      "epoch": 57.144,
      "iterations": 3600,
      "loss": 0.19091688096523285,
      "perplexity": 1.210358738899231,
      "step": 3600
    },
    {
      "epoch": 57.144,
      "iterations": 3600,
      "loss": 0.9517214298248291,
      "perplexity": 2.5901646614074707,
      "step": 3600
    },
    {
      "epoch": 57.144,
      "iterations": 3600,
      "loss": 1.0239933729171753,
      "perplexity": 2.7842912673950195,
      "step": 3600
    },
    {
      "epoch": 57.144,
      "iterations": 3600,
      "loss": 0.1798809915781021,
      "perplexity": 1.1970748901367188,
      "step": 3600
    },
    {
      "epoch": 57.144,
      "iterations": 3600,
      "loss": 0.5407522916793823,
      "perplexity": 1.7172983884811401,
      "step": 3600
    },
    {
      "epoch": 57.944,
      "grad_norm": 2.312293291091919,
      "learning_rate": 5.514285714285714e-05,
      "loss": 5.9102,
      "step": 3650
    },
    {
      "epoch": 57.944,
      "iterations": 3650,
      "loss": 1.1344469785690308,
      "perplexity": 3.1094534397125244,
      "step": 3650
    },
    {
      "epoch": 57.944,
      "iterations": 3650,
      "loss": 1.0958210229873657,
      "perplexity": 2.991637706756592,
      "step": 3650
    },
    {
      "epoch": 57.944,
      "iterations": 3650,
      "loss": 1.0987813472747803,
      "perplexity": 3.000507354736328,
      "step": 3650
    },
    {
      "epoch": 57.944,
      "iterations": 3650,
      "loss": 0.16377508640289307,
      "perplexity": 1.17794930934906,
      "step": 3650
    },
    {
      "epoch": 57.944,
      "iterations": 3650,
      "loss": 0.1745612472295761,
      "perplexity": 1.1907236576080322,
      "step": 3650
    },
    {
      "epoch": 57.944,
      "iterations": 3650,
      "loss": 0.2626277208328247,
      "perplexity": 1.3003425598144531,
      "step": 3650
    },
    {
      "epoch": 57.944,
      "iterations": 3650,
      "loss": 0.9113882780075073,
      "perplexity": 2.487773895263672,
      "step": 3650
    },
    {
      "epoch": 57.944,
      "iterations": 3650,
      "loss": 0.23049215972423553,
      "perplexity": 1.2592196464538574,
      "step": 3650
    },
    {
      "epoch": 58.736,
      "grad_norm": 1.4119086265563965,
      "learning_rate": 5.3102040816326536e-05,
      "loss": 5.9392,
      "step": 3700
    },
    {
      "epoch": 58.736,
      "iterations": 3700,
      "loss": 1.094544768333435,
      "perplexity": 2.9878222942352295,
      "step": 3700
    },
    {
      "epoch": 58.736,
      "iterations": 3700,
      "loss": 0.9447456002235413,
      "perplexity": 2.5721590518951416,
      "step": 3700
    },
    {
      "epoch": 58.736,
      "iterations": 3700,
      "loss": 0.18481168150901794,
      "perplexity": 1.2029918432235718,
      "step": 3700
    },
    {
      "epoch": 58.736,
      "iterations": 3700,
      "loss": 1.0645564794540405,
      "perplexity": 2.899552583694458,
      "step": 3700
    },
    {
      "epoch": 58.736,
      "iterations": 3700,
      "loss": 0.7796384692192078,
      "perplexity": 2.1806836128234863,
      "step": 3700
    },
    {
      "epoch": 58.736,
      "iterations": 3700,
      "loss": 0.18348243832588196,
      "perplexity": 1.2013938426971436,
      "step": 3700
    },
    {
      "epoch": 58.736,
      "iterations": 3700,
      "loss": 1.2725701332092285,
      "perplexity": 3.570016384124756,
      "step": 3700
    },
    {
      "epoch": 58.736,
      "iterations": 3700,
      "loss": 1.0554282665252686,
      "perplexity": 2.8732054233551025,
      "step": 3700
    },
    {
      "epoch": 59.528,
      "grad_norm": 2.568715810775757,
      "learning_rate": 5.1061224489795925e-05,
      "loss": 5.9941,
      "step": 3750
    },
    {
      "epoch": 59.528,
      "iterations": 3750,
      "loss": 0.7442765235900879,
      "perplexity": 2.1049180030822754,
      "step": 3750
    },
    {
      "epoch": 59.528,
      "iterations": 3750,
      "loss": 1.2421765327453613,
      "perplexity": 3.4631428718566895,
      "step": 3750
    },
    {
      "epoch": 59.528,
      "iterations": 3750,
      "loss": 0.7879959940910339,
      "perplexity": 2.1989850997924805,
      "step": 3750
    },
    {
      "epoch": 59.528,
      "iterations": 3750,
      "loss": 0.9251824021339417,
      "perplexity": 2.5223283767700195,
      "step": 3750
    },
    {
      "epoch": 59.528,
      "iterations": 3750,
      "loss": 0.573698103427887,
      "perplexity": 1.7748184204101562,
      "step": 3750
    },
    {
      "epoch": 59.528,
      "iterations": 3750,
      "loss": 0.8862353563308716,
      "perplexity": 2.4259796142578125,
      "step": 3750
    },
    {
      "epoch": 59.528,
      "iterations": 3750,
      "loss": 1.3433527946472168,
      "perplexity": 3.83186936378479,
      "step": 3750
    },
    {
      "epoch": 59.528,
      "iterations": 3750,
      "loss": 0.8963460922241211,
      "perplexity": 2.450632333755493,
      "step": 3750
    },
    {
      "epoch": 60.32,
      "grad_norm": 3.051065683364868,
      "learning_rate": 4.902040816326531e-05,
      "loss": 5.7897,
      "step": 3800
    },
    {
      "epoch": 60.32,
      "iterations": 3800,
      "loss": 1.149035096168518,
      "perplexity": 3.155147075653076,
      "step": 3800
    },
    {
      "epoch": 60.32,
      "iterations": 3800,
      "loss": 1.09389328956604,
      "perplexity": 2.9858763217926025,
      "step": 3800
    },
    {
      "epoch": 60.32,
      "iterations": 3800,
      "loss": 1.3419135808944702,
      "perplexity": 3.8263583183288574,
      "step": 3800
    },
    {
      "epoch": 60.32,
      "iterations": 3800,
      "loss": 0.13771939277648926,
      "perplexity": 1.147653579711914,
      "step": 3800
    },
    {
      "epoch": 60.32,
      "iterations": 3800,
      "loss": 1.1382241249084473,
      "perplexity": 3.121220350265503,
      "step": 3800
    },
    {
      "epoch": 60.32,
      "iterations": 3800,
      "loss": 1.1936323642730713,
      "perplexity": 3.2990427017211914,
      "step": 3800
    },
    {
      "epoch": 60.32,
      "iterations": 3800,
      "loss": 1.212741494178772,
      "perplexity": 3.3626906871795654,
      "step": 3800
    },
    {
      "epoch": 60.32,
      "iterations": 3800,
      "loss": 1.371050477027893,
      "perplexity": 3.9394867420196533,
      "step": 3800
    },
    {
      "epoch": 61.112,
      "grad_norm": 2.1942543983459473,
      "learning_rate": 4.6979591836734696e-05,
      "loss": 5.8726,
      "step": 3850
    },
    {
      "epoch": 61.112,
      "iterations": 3850,
      "loss": 1.0084683895111084,
      "perplexity": 2.741399049758911,
      "step": 3850
    },
    {
      "epoch": 61.112,
      "iterations": 3850,
      "loss": 0.1773718297481537,
      "perplexity": 1.1940749883651733,
      "step": 3850
    },
    {
      "epoch": 61.112,
      "iterations": 3850,
      "loss": 0.6176568865776062,
      "perplexity": 1.8545775413513184,
      "step": 3850
    },
    {
      "epoch": 61.112,
      "iterations": 3850,
      "loss": 0.4311400055885315,
      "perplexity": 1.5390111207962036,
      "step": 3850
    },
    {
      "epoch": 61.112,
      "iterations": 3850,
      "loss": 1.1706067323684692,
      "perplexity": 3.2239480018615723,
      "step": 3850
    },
    {
      "epoch": 61.112,
      "iterations": 3850,
      "loss": 0.9396286010742188,
      "perplexity": 2.5590310096740723,
      "step": 3850
    },
    {
      "epoch": 61.112,
      "iterations": 3850,
      "loss": 1.0611320734024048,
      "perplexity": 2.8896405696868896,
      "step": 3850
    },
    {
      "epoch": 61.112,
      "iterations": 3850,
      "loss": 0.1934470385313034,
      "perplexity": 1.2134251594543457,
      "step": 3850
    },
    {
      "epoch": 61.912,
      "grad_norm": 2.1471357345581055,
      "learning_rate": 4.4938775510204084e-05,
      "loss": 5.8385,
      "step": 3900
    },
    {
      "epoch": 61.912,
      "iterations": 3900,
      "loss": 0.18311841785907745,
      "perplexity": 1.2009565830230713,
      "step": 3900
    },
    {
      "epoch": 61.912,
      "iterations": 3900,
      "loss": 0.9183496236801147,
      "perplexity": 2.505152463912964,
      "step": 3900
    },
    {
      "epoch": 61.912,
      "iterations": 3900,
      "loss": 0.2333078235387802,
      "perplexity": 1.2627700567245483,
      "step": 3900
    },
    {
      "epoch": 61.912,
      "iterations": 3900,
      "loss": 0.569586992263794,
      "perplexity": 1.7675368785858154,
      "step": 3900
    },
    {
      "epoch": 61.912,
      "iterations": 3900,
      "loss": 0.1502837985754013,
      "perplexity": 1.1621640920639038,
      "step": 3900
    },
    {
      "epoch": 61.912,
      "iterations": 3900,
      "loss": 0.163321852684021,
      "perplexity": 1.1774156093597412,
      "step": 3900
    },
    {
      "epoch": 61.912,
      "iterations": 3900,
      "loss": 0.170150488615036,
      "perplexity": 1.1854832172393799,
      "step": 3900
    },
    {
      "epoch": 61.912,
      "iterations": 3900,
      "loss": 1.419120192527771,
      "perplexity": 4.133481979370117,
      "step": 3900
    },
    {
      "epoch": 62.704,
      "grad_norm": 1.7810606956481934,
      "learning_rate": 4.2897959183673466e-05,
      "loss": 5.7153,
      "step": 3950
    },
    {
      "epoch": 62.704,
      "iterations": 3950,
      "loss": 0.6717859506607056,
      "perplexity": 1.9577306509017944,
      "step": 3950
    },
    {
      "epoch": 62.704,
      "iterations": 3950,
      "loss": 1.2821674346923828,
      "perplexity": 3.6044435501098633,
      "step": 3950
    },
    {
      "epoch": 62.704,
      "iterations": 3950,
      "loss": 0.13732369244098663,
      "perplexity": 1.1471995115280151,
      "step": 3950
    },
    {
      "epoch": 62.704,
      "iterations": 3950,
      "loss": 1.3712352514266968,
      "perplexity": 3.9402146339416504,
      "step": 3950
    },
    {
      "epoch": 62.704,
      "iterations": 3950,
      "loss": 1.3175122737884521,
      "perplexity": 3.7341203689575195,
      "step": 3950
    },
    {
      "epoch": 62.704,
      "iterations": 3950,
      "loss": 1.2194384336471558,
      "perplexity": 3.3852860927581787,
      "step": 3950
    },
    {
      "epoch": 62.704,
      "iterations": 3950,
      "loss": 0.773058295249939,
      "perplexity": 2.166381597518921,
      "step": 3950
    },
    {
      "epoch": 62.704,
      "iterations": 3950,
      "loss": 0.9206500053405762,
      "perplexity": 2.5109219551086426,
      "step": 3950
    },
    {
      "epoch": 63.496,
      "grad_norm": 1.9284619092941284,
      "learning_rate": 4.085714285714286e-05,
      "loss": 5.9924,
      "step": 4000
    },
    {
      "epoch": 63.496,
      "iterations": 4000,
      "loss": 0.15185900032520294,
      "perplexity": 1.1639961004257202,
      "step": 4000
    },
    {
      "epoch": 63.496,
      "iterations": 4000,
      "loss": 0.6776745319366455,
      "perplexity": 1.9692928791046143,
      "step": 4000
    },
    {
      "epoch": 63.496,
      "iterations": 4000,
      "loss": 1.2225704193115234,
      "perplexity": 3.3959054946899414,
      "step": 4000
    },
    {
      "epoch": 63.496,
      "iterations": 4000,
      "loss": 0.7786751389503479,
      "perplexity": 2.178583860397339,
      "step": 4000
    },
    {
      "epoch": 63.496,
      "iterations": 4000,
      "loss": 1.2171629667282104,
      "perplexity": 3.377591609954834,
      "step": 4000
    },
    {
      "epoch": 63.496,
      "iterations": 4000,
      "loss": 0.5979626178741455,
      "perplexity": 1.8184102773666382,
      "step": 4000
    },
    {
      "epoch": 63.496,
      "iterations": 4000,
      "loss": 1.1947007179260254,
      "perplexity": 3.3025691509246826,
      "step": 4000
    },
    {
      "epoch": 63.496,
      "iterations": 4000,
      "loss": 0.9970216751098633,
      "perplexity": 2.710197925567627,
      "step": 4000
    },
    {
      "epoch": 64.288,
      "grad_norm": 1.9035325050354004,
      "learning_rate": 3.8816326530612244e-05,
      "loss": 5.8163,
      "step": 4050
    },
    {
      "epoch": 64.288,
      "iterations": 4050,
      "loss": 0.36355358362197876,
      "perplexity": 1.438431978225708,
      "step": 4050
    },
    {
      "epoch": 64.288,
      "iterations": 4050,
      "loss": 0.864241361618042,
      "perplexity": 2.3732049465179443,
      "step": 4050
    },
    {
      "epoch": 64.288,
      "iterations": 4050,
      "loss": 0.15960273146629333,
      "perplexity": 1.1730446815490723,
      "step": 4050
    },
    {
      "epoch": 64.288,
      "iterations": 4050,
      "loss": 0.2929256558418274,
      "perplexity": 1.3403431177139282,
      "step": 4050
    },
    {
      "epoch": 64.288,
      "iterations": 4050,
      "loss": 0.5925188064575195,
      "perplexity": 1.8085380792617798,
      "step": 4050
    },
    {
      "epoch": 64.288,
      "iterations": 4050,
      "loss": 0.5312949419021606,
      "perplexity": 1.7011337280273438,
      "step": 4050
    },
    {
      "epoch": 64.288,
      "iterations": 4050,
      "loss": 0.18343882262706757,
      "perplexity": 1.2013413906097412,
      "step": 4050
    },
    {
      "epoch": 64.288,
      "iterations": 4050,
      "loss": 0.9596134424209595,
      "perplexity": 2.610687255859375,
      "step": 4050
    },
    {
      "epoch": 65.08,
      "grad_norm": 2.147230863571167,
      "learning_rate": 3.677551020408164e-05,
      "loss": 5.6211,
      "step": 4100
    },
    {
      "epoch": 65.08,
      "iterations": 4100,
      "loss": 0.28047284483909607,
      "perplexity": 1.3237556219100952,
      "step": 4100
    },
    {
      "epoch": 65.08,
      "iterations": 4100,
      "loss": 0.16674141585826874,
      "perplexity": 1.1814486980438232,
      "step": 4100
    },
    {
      "epoch": 65.08,
      "iterations": 4100,
      "loss": 0.9030773639678955,
      "perplexity": 2.467183828353882,
      "step": 4100
    },
    {
      "epoch": 65.08,
      "iterations": 4100,
      "loss": 0.8443883657455444,
      "perplexity": 2.326554298400879,
      "step": 4100
    },
    {
      "epoch": 65.08,
      "iterations": 4100,
      "loss": 0.11462419480085373,
      "perplexity": 1.1214519739151,
      "step": 4100
    },
    {
      "epoch": 65.08,
      "iterations": 4100,
      "loss": 1.2697659730911255,
      "perplexity": 3.5600192546844482,
      "step": 4100
    },
    {
      "epoch": 65.08,
      "iterations": 4100,
      "loss": 1.2555336952209473,
      "perplexity": 3.5097107887268066,
      "step": 4100
    },
    {
      "epoch": 65.08,
      "iterations": 4100,
      "loss": 0.7290500998497009,
      "perplexity": 2.073110342025757,
      "step": 4100
    },
    {
      "epoch": 65.88,
      "grad_norm": 1.918330430984497,
      "learning_rate": 3.473469387755102e-05,
      "loss": 5.841,
      "step": 4150
    },
    {
      "epoch": 65.88,
      "iterations": 4150,
      "loss": 0.13741204142570496,
      "perplexity": 1.1473008394241333,
      "step": 4150
    },
    {
      "epoch": 65.88,
      "iterations": 4150,
      "loss": 0.9601403474807739,
      "perplexity": 2.612062931060791,
      "step": 4150
    },
    {
      "epoch": 65.88,
      "iterations": 4150,
      "loss": 0.11930060386657715,
      "perplexity": 1.1267086267471313,
      "step": 4150
    },
    {
      "epoch": 65.88,
      "iterations": 4150,
      "loss": 0.9705430269241333,
      "perplexity": 2.6393775939941406,
      "step": 4150
    },
    {
      "epoch": 65.88,
      "iterations": 4150,
      "loss": 0.36268213391304016,
      "perplexity": 1.4371790885925293,
      "step": 4150
    },
    {
      "epoch": 65.88,
      "iterations": 4150,
      "loss": 1.0708709955215454,
      "perplexity": 2.917919874191284,
      "step": 4150
    },
    {
      "epoch": 65.88,
      "iterations": 4150,
      "loss": 1.2221773862838745,
      "perplexity": 3.39457106590271,
      "step": 4150
    },
    {
      "epoch": 65.88,
      "iterations": 4150,
      "loss": 0.4202609956264496,
      "perplexity": 1.5223588943481445,
      "step": 4150
    },
    {
      "epoch": 66.672,
      "grad_norm": 2.5598888397216797,
      "learning_rate": 3.269387755102041e-05,
      "loss": 5.7698,
      "step": 4200
    },
    {
      "epoch": 66.672,
      "iterations": 4200,
      "loss": 0.11737557500600815,
      "perplexity": 1.1245417594909668,
      "step": 4200
    },
    {
      "epoch": 66.672,
      "iterations": 4200,
      "loss": 1.2625770568847656,
      "perplexity": 3.5345184803009033,
      "step": 4200
    },
    {
      "epoch": 66.672,
      "iterations": 4200,
      "loss": 0.8895174264907837,
      "perplexity": 2.4339547157287598,
      "step": 4200
    },
    {
      "epoch": 66.672,
      "iterations": 4200,
      "loss": 0.09897436946630478,
      "perplexity": 1.1040380001068115,
      "step": 4200
    },
    {
      "epoch": 66.672,
      "iterations": 4200,
      "loss": 0.1633753478527069,
      "perplexity": 1.177478551864624,
      "step": 4200
    },
    {
      "epoch": 66.672,
      "iterations": 4200,
      "loss": 0.16062261164188385,
      "perplexity": 1.1742416620254517,
      "step": 4200
    },
    {
      "epoch": 66.672,
      "iterations": 4200,
      "loss": 0.6010051369667053,
      "perplexity": 1.823951244354248,
      "step": 4200
    },
    {
      "epoch": 66.672,
      "iterations": 4200,
      "loss": 1.1944937705993652,
      "perplexity": 3.3018858432769775,
      "step": 4200
    },
    {
      "epoch": 67.464,
      "grad_norm": 1.7183393239974976,
      "learning_rate": 3.06530612244898e-05,
      "loss": 5.7591,
      "step": 4250
    },
    {
      "epoch": 67.464,
      "iterations": 4250,
      "loss": 1.406154751777649,
      "perplexity": 4.080235958099365,
      "step": 4250
    },
    {
      "epoch": 67.464,
      "iterations": 4250,
      "loss": 1.1565600633621216,
      "perplexity": 3.17897891998291,
      "step": 4250
    },
    {
      "epoch": 67.464,
      "iterations": 4250,
      "loss": 1.1924021244049072,
      "perplexity": 3.2949864864349365,
      "step": 4250
    },
    {
      "epoch": 67.464,
      "iterations": 4250,
      "loss": 0.2525545656681061,
      "perplexity": 1.2873097658157349,
      "step": 4250
    },
    {
      "epoch": 67.464,
      "iterations": 4250,
      "loss": 0.1469717174768448,
      "perplexity": 1.1583212614059448,
      "step": 4250
    },
    {
      "epoch": 67.464,
      "iterations": 4250,
      "loss": 0.9880040287971497,
      "perplexity": 2.685868263244629,
      "step": 4250
    },
    {
      "epoch": 67.464,
      "iterations": 4250,
      "loss": 0.35029831528663635,
      "perplexity": 1.419490933418274,
      "step": 4250
    },
    {
      "epoch": 67.464,
      "iterations": 4250,
      "loss": 0.9616502523422241,
      "perplexity": 2.6160099506378174,
      "step": 4250
    },
    {
      "epoch": 68.256,
      "grad_norm": 1.3039575815200806,
      "learning_rate": 2.8612244897959184e-05,
      "loss": 5.6655,
      "step": 4300
    },
    {
      "epoch": 68.256,
      "iterations": 4300,
      "loss": 1.2575123310089111,
      "perplexity": 3.516662120819092,
      "step": 4300
    },
    {
      "epoch": 68.256,
      "iterations": 4300,
      "loss": 0.34475117921829224,
      "perplexity": 1.411638617515564,
      "step": 4300
    },
    {
      "epoch": 68.256,
      "iterations": 4300,
      "loss": 1.3370507955551147,
      "perplexity": 3.8077969551086426,
      "step": 4300
    },
    {
      "epoch": 68.256,
      "iterations": 4300,
      "loss": 0.09847752749919891,
      "perplexity": 1.103489637374878,
      "step": 4300
    },
    {
      "epoch": 68.256,
      "iterations": 4300,
      "loss": 0.6400905847549438,
      "perplexity": 1.8966526985168457,
      "step": 4300
    },
    {
      "epoch": 68.256,
      "iterations": 4300,
      "loss": 0.1245991438627243,
      "perplexity": 1.1326943635940552,
      "step": 4300
    },
    {
      "epoch": 68.256,
      "iterations": 4300,
      "loss": 0.13978612422943115,
      "perplexity": 1.150027871131897,
      "step": 4300
    },
    {
      "epoch": 68.256,
      "iterations": 4300,
      "loss": 1.4122846126556396,
      "perplexity": 4.105323791503906,
      "step": 4300
    },
    {
      "epoch": 69.048,
      "grad_norm": 2.060377836227417,
      "learning_rate": 2.6571428571428576e-05,
      "loss": 5.7032,
      "step": 4350
    },
    {
      "epoch": 69.048,
      "iterations": 4350,
      "loss": 0.5954154133796692,
      "perplexity": 1.8137842416763306,
      "step": 4350
    },
    {
      "epoch": 69.048,
      "iterations": 4350,
      "loss": 0.9422469139099121,
      "perplexity": 2.565739870071411,
      "step": 4350
    },
    {
      "epoch": 69.048,
      "iterations": 4350,
      "loss": 1.2097008228302002,
      "perplexity": 3.3524816036224365,
      "step": 4350
    },
    {
      "epoch": 69.048,
      "iterations": 4350,
      "loss": 0.15334275364875793,
      "perplexity": 1.165724515914917,
      "step": 4350
    },
    {
      "epoch": 69.048,
      "iterations": 4350,
      "loss": 0.9760486483573914,
      "perplexity": 2.6539487838745117,
      "step": 4350
    },
    {
      "epoch": 69.048,
      "iterations": 4350,
      "loss": 1.046943187713623,
      "perplexity": 2.8489291667938232,
      "step": 4350
    },
    {
      "epoch": 69.048,
      "iterations": 4350,
      "loss": 0.9095613360404968,
      "perplexity": 2.4832332134246826,
      "step": 4350
    },
    {
      "epoch": 69.048,
      "iterations": 4350,
      "loss": 0.6004908084869385,
      "perplexity": 1.8230133056640625,
      "step": 4350
    },
    {
      "epoch": 69.848,
      "grad_norm": 2.4325273036956787,
      "learning_rate": 2.4530612244897962e-05,
      "loss": 5.6302,
      "step": 4400
    },
    {
      "epoch": 69.848,
      "iterations": 4400,
      "loss": 1.0811796188354492,
      "perplexity": 2.948155164718628,
      "step": 4400
    },
    {
      "epoch": 69.848,
      "iterations": 4400,
      "loss": 1.4063483476638794,
      "perplexity": 4.08102560043335,
      "step": 4400
    },
    {
      "epoch": 69.848,
      "iterations": 4400,
      "loss": 0.7416278719902039,
      "perplexity": 2.0993502140045166,
      "step": 4400
    },
    {
      "epoch": 69.848,
      "iterations": 4400,
      "loss": 1.1692103147506714,
      "perplexity": 3.219449281692505,
      "step": 4400
    },
    {
      "epoch": 69.848,
      "iterations": 4400,
      "loss": 0.9962765574455261,
      "perplexity": 2.708179235458374,
      "step": 4400
    },
    {
      "epoch": 69.848,
      "iterations": 4400,
      "loss": 0.15225964784622192,
      "perplexity": 1.1644625663757324,
      "step": 4400
    },
    {
      "epoch": 69.848,
      "iterations": 4400,
      "loss": 0.4034944772720337,
      "perplexity": 1.497046947479248,
      "step": 4400
    },
    {
      "epoch": 69.848,
      "iterations": 4400,
      "loss": 1.3372247219085693,
      "perplexity": 3.8084592819213867,
      "step": 4400
    },
    {
      "epoch": 70.64,
      "grad_norm": 1.2300087213516235,
      "learning_rate": 2.2489795918367347e-05,
      "loss": 5.9355,
      "step": 4450
    },
    {
      "epoch": 70.64,
      "iterations": 4450,
      "loss": 0.13578876852989197,
      "perplexity": 1.1454399824142456,
      "step": 4450
    },
    {
      "epoch": 70.64,
      "iterations": 4450,
      "loss": 1.1624536514282227,
      "perplexity": 3.1977698802948,
      "step": 4450
    },
    {
      "epoch": 70.64,
      "iterations": 4450,
      "loss": 1.1655844449996948,
      "perplexity": 3.207797050476074,
      "step": 4450
    },
    {
      "epoch": 70.64,
      "iterations": 4450,
      "loss": 0.497501015663147,
      "perplexity": 1.644606351852417,
      "step": 4450
    },
    {
      "epoch": 70.64,
      "iterations": 4450,
      "loss": 0.4645126163959503,
      "perplexity": 1.5912384986877441,
      "step": 4450
    },
    {
      "epoch": 70.64,
      "iterations": 4450,
      "loss": 0.425331711769104,
      "perplexity": 1.5300978422164917,
      "step": 4450
    },
    {
      "epoch": 70.64,
      "iterations": 4450,
      "loss": 0.8855159878730774,
      "perplexity": 2.4242348670959473,
      "step": 4450
    },
    {
      "epoch": 70.64,
      "iterations": 4450,
      "loss": 1.115976095199585,
      "perplexity": 3.052546262741089,
      "step": 4450
    },
    {
      "epoch": 71.432,
      "grad_norm": 1.4003725051879883,
      "learning_rate": 2.0448979591836736e-05,
      "loss": 5.3377,
      "step": 4500
    },
    {
      "epoch": 71.432,
      "iterations": 4500,
      "loss": 1.0482257604599,
      "perplexity": 2.852585554122925,
      "step": 4500
    },
    {
      "epoch": 71.432,
      "iterations": 4500,
      "loss": 0.10908687114715576,
      "perplexity": 1.1152591705322266,
      "step": 4500
    },
    {
      "epoch": 71.432,
      "iterations": 4500,
      "loss": 1.2381038665771484,
      "perplexity": 3.4490673542022705,
      "step": 4500
    },
    {
      "epoch": 71.432,
      "iterations": 4500,
      "loss": 0.13387465476989746,
      "perplexity": 1.14324951171875,
      "step": 4500
    },
    {
      "epoch": 71.432,
      "iterations": 4500,
      "loss": 0.12905961275100708,
      "perplexity": 1.1377578973770142,
      "step": 4500
    },
    {
      "epoch": 71.432,
      "iterations": 4500,
      "loss": 1.0823270082473755,
      "perplexity": 2.9515397548675537,
      "step": 4500
    },
    {
      "epoch": 71.432,
      "iterations": 4500,
      "loss": 1.2238621711730957,
      "perplexity": 3.400294780731201,
      "step": 4500
    },
    {
      "epoch": 71.432,
      "iterations": 4500,
      "loss": 0.4609525799751282,
      "perplexity": 1.5855835676193237,
      "step": 4500
    },
    {
      "epoch": 72.224,
      "grad_norm": 1.8956360816955566,
      "learning_rate": 1.840816326530612e-05,
      "loss": 5.9399,
      "step": 4550
    },
    {
      "epoch": 72.224,
      "iterations": 4550,
      "loss": 0.12971706688404083,
      "perplexity": 1.138506293296814,
      "step": 4550
    },
    {
      "epoch": 72.224,
      "iterations": 4550,
      "loss": 0.512643575668335,
      "perplexity": 1.6696993112564087,
      "step": 4550
    },
    {
      "epoch": 72.224,
      "iterations": 4550,
      "loss": 0.10312613844871521,
      "perplexity": 1.1086312532424927,
      "step": 4550
    },
    {
      "epoch": 72.224,
      "iterations": 4550,
      "loss": 1.2748768329620361,
      "perplexity": 3.578260660171509,
      "step": 4550
    },
    {
      "epoch": 72.224,
      "iterations": 4550,
      "loss": 0.12544789910316467,
      "perplexity": 1.1336561441421509,
      "step": 4550
    },
    {
      "epoch": 72.224,
      "iterations": 4550,
      "loss": 0.11103199422359467,
      "perplexity": 1.117430567741394,
      "step": 4550
    },
    {
      "epoch": 72.224,
      "iterations": 4550,
      "loss": 0.6364063024520874,
      "perplexity": 1.8896777629852295,
      "step": 4550
    },
    {
      "epoch": 72.224,
      "iterations": 4550,
      "loss": 0.4840526878833771,
      "perplexity": 1.6226372718811035,
      "step": 4550
    },
    {
      "epoch": 73.016,
      "grad_norm": 1.6968188285827637,
      "learning_rate": 1.636734693877551e-05,
      "loss": 5.5699,
      "step": 4600
    },
    {
      "epoch": 73.016,
      "iterations": 4600,
      "loss": 0.7132027745246887,
      "perplexity": 2.0405161380767822,
      "step": 4600
    },
    {
      "epoch": 73.016,
      "iterations": 4600,
      "loss": 0.12366320192813873,
      "perplexity": 1.1316347122192383,
      "step": 4600
    },
    {
      "epoch": 73.016,
      "iterations": 4600,
      "loss": 0.6420978307723999,
      "perplexity": 1.9004634618759155,
      "step": 4600
    },
    {
      "epoch": 73.016,
      "iterations": 4600,
      "loss": 0.6678670048713684,
      "perplexity": 1.9500733613967896,
      "step": 4600
    },
    {
      "epoch": 73.016,
      "iterations": 4600,
      "loss": 0.1473727524280548,
      "perplexity": 1.1587859392166138,
      "step": 4600
    },
    {
      "epoch": 73.016,
      "iterations": 4600,
      "loss": 0.10872507840394974,
      "perplexity": 1.1148557662963867,
      "step": 4600
    },
    {
      "epoch": 73.016,
      "iterations": 4600,
      "loss": 0.979705810546875,
      "perplexity": 2.66367244720459,
      "step": 4600
    },
    {
      "epoch": 73.016,
      "iterations": 4600,
      "loss": 0.8664887547492981,
      "perplexity": 2.378544569015503,
      "step": 4600
    },
    {
      "epoch": 73.816,
      "grad_norm": 1.0401540994644165,
      "learning_rate": 1.4326530612244899e-05,
      "loss": 5.8291,
      "step": 4650
    },
    {
      "epoch": 73.816,
      "iterations": 4650,
      "loss": 1.0060396194458008,
      "perplexity": 2.7347490787506104,
      "step": 4650
    },
    {
      "epoch": 73.816,
      "iterations": 4650,
      "loss": 0.4344525933265686,
      "perplexity": 1.5441175699234009,
      "step": 4650
    },
    {
      "epoch": 73.816,
      "iterations": 4650,
      "loss": 0.8833141922950745,
      "perplexity": 2.418903350830078,
      "step": 4650
    },
    {
      "epoch": 73.816,
      "iterations": 4650,
      "loss": 0.5461385250091553,
      "perplexity": 1.7265729904174805,
      "step": 4650
    },
    {
      "epoch": 73.816,
      "iterations": 4650,
      "loss": 1.3531845808029175,
      "perplexity": 3.8697292804718018,
      "step": 4650
    },
    {
      "epoch": 73.816,
      "iterations": 4650,
      "loss": 1.383141040802002,
      "perplexity": 3.9874064922332764,
      "step": 4650
    },
    {
      "epoch": 73.816,
      "iterations": 4650,
      "loss": 0.10146244615316391,
      "perplexity": 1.1067883968353271,
      "step": 4650
    },
    {
      "epoch": 73.816,
      "iterations": 4650,
      "loss": 0.6975482106208801,
      "perplexity": 2.008821487426758,
      "step": 4650
    },
    {
      "epoch": 74.608,
      "grad_norm": 1.0968893766403198,
      "learning_rate": 1.2285714285714286e-05,
      "loss": 5.4487,
      "step": 4700
    },
    {
      "epoch": 74.608,
      "iterations": 4700,
      "loss": 1.2078254222869873,
      "perplexity": 3.3461999893188477,
      "step": 4700
    },
    {
      "epoch": 74.608,
      "iterations": 4700,
      "loss": 1.405938982963562,
      "perplexity": 4.079355716705322,
      "step": 4700
    },
    {
      "epoch": 74.608,
      "iterations": 4700,
      "loss": 0.11632373183965683,
      "perplexity": 1.1233594417572021,
      "step": 4700
    },
    {
      "epoch": 74.608,
      "iterations": 4700,
      "loss": 0.11296747624874115,
      "perplexity": 1.1195955276489258,
      "step": 4700
    },
    {
      "epoch": 74.608,
      "iterations": 4700,
      "loss": 0.7328265905380249,
      "perplexity": 2.0809543132781982,
      "step": 4700
    },
    {
      "epoch": 74.608,
      "iterations": 4700,
      "loss": 0.8467910885810852,
      "perplexity": 2.332151174545288,
      "step": 4700
    },
    {
      "epoch": 74.608,
      "iterations": 4700,
      "loss": 0.11546146869659424,
      "perplexity": 1.1223913431167603,
      "step": 4700
    },
    {
      "epoch": 74.608,
      "iterations": 4700,
      "loss": 1.278566837310791,
      "perplexity": 3.591488838195801,
      "step": 4700
    },
    {
      "epoch": 75.4,
      "grad_norm": 1.646669864654541,
      "learning_rate": 1.0244897959183673e-05,
      "loss": 5.9572,
      "step": 4750
    },
    {
      "epoch": 75.4,
      "iterations": 4750,
      "loss": 0.9829138517379761,
      "perplexity": 2.672231435775757,
      "step": 4750
    },
    {
      "epoch": 75.4,
      "iterations": 4750,
      "loss": 0.4475571811199188,
      "perplexity": 1.564485788345337,
      "step": 4750
    },
    {
      "epoch": 75.4,
      "iterations": 4750,
      "loss": 0.7794442176818848,
      "perplexity": 2.180260181427002,
      "step": 4750
    },
    {
      "epoch": 75.4,
      "iterations": 4750,
      "loss": 0.08883166313171387,
      "perplexity": 1.092896580696106,
      "step": 4750
    },
    {
      "epoch": 75.4,
      "iterations": 4750,
      "loss": 0.9825602769851685,
      "perplexity": 2.6712868213653564,
      "step": 4750
    },
    {
      "epoch": 75.4,
      "iterations": 4750,
      "loss": 1.2587603330612183,
      "perplexity": 3.5210537910461426,
      "step": 4750
    },
    {
      "epoch": 75.4,
      "iterations": 4750,
      "loss": 0.11816376447677612,
      "perplexity": 1.125428318977356,
      "step": 4750
    },
    {
      "epoch": 75.4,
      "iterations": 4750,
      "loss": 0.8470024466514587,
      "perplexity": 2.332643985748291,
      "step": 4750
    },
    {
      "epoch": 76.192,
      "grad_norm": 1.1736412048339844,
      "learning_rate": 8.204081632653062e-06,
      "loss": 5.2027,
      "step": 4800
    },
    {
      "epoch": 76.192,
      "iterations": 4800,
      "loss": 0.9912774562835693,
      "perplexity": 2.6946747303009033,
      "step": 4800
    },
    {
      "epoch": 76.192,
      "iterations": 4800,
      "loss": 1.3090403079986572,
      "perplexity": 3.7026185989379883,
      "step": 4800
    },
    {
      "epoch": 76.192,
      "iterations": 4800,
      "loss": 1.2241705656051636,
      "perplexity": 3.401343584060669,
      "step": 4800
    },
    {
      "epoch": 76.192,
      "iterations": 4800,
      "loss": 0.9576701521873474,
      "perplexity": 2.605618715286255,
      "step": 4800
    },
    {
      "epoch": 76.192,
      "iterations": 4800,
      "loss": 0.11204151064157486,
      "perplexity": 1.1185593605041504,
      "step": 4800
    },
    {
      "epoch": 76.192,
      "iterations": 4800,
      "loss": 0.45816075801849365,
      "perplexity": 1.5811631679534912,
      "step": 4800
    },
    {
      "epoch": 76.192,
      "iterations": 4800,
      "loss": 0.6941545009613037,
      "perplexity": 2.0020155906677246,
      "step": 4800
    },
    {
      "epoch": 76.192,
      "iterations": 4800,
      "loss": 1.22405207157135,
      "perplexity": 3.4009406566619873,
      "step": 4800
    },
    {
      "epoch": 76.992,
      "grad_norm": 1.5364288091659546,
      "learning_rate": 6.163265306122449e-06,
      "loss": 5.8031,
      "step": 4850
    },
    {
      "epoch": 76.992,
      "iterations": 4850,
      "loss": 2.2349038124084473,
      "perplexity": 9.345582962036133,
      "step": 4850
    },
    {
      "epoch": 76.992,
      "iterations": 4850,
      "loss": 2.7432920932769775,
      "perplexity": 15.538053512573242,
      "step": 4850
    },
    {
      "epoch": 76.992,
      "iterations": 4850,
      "loss": 1.5072320699691772,
      "perplexity": 4.514218330383301,
      "step": 4850
    },
    {
      "epoch": 76.992,
      "iterations": 4850,
      "loss": 2.5999937057495117,
      "perplexity": 13.463653564453125,
      "step": 4850
    },
    {
      "epoch": 77.784,
      "grad_norm": 1.0886855125427246,
      "learning_rate": 4.122448979591837e-06,
      "loss": 5.747,
      "step": 4900
    },
    {
      "epoch": 77.784,
      "iterations": 4900,
      "loss": 0.11988294869661331,
      "perplexity": 1.1273648738861084,
      "step": 4900
    },
    {
      "epoch": 77.784,
      "iterations": 4900,
      "loss": 0.16825827956199646,
      "perplexity": 1.1832420825958252,
      "step": 4900
    },
    {
      "epoch": 77.784,
      "iterations": 4900,
      "loss": 1.2663575410842896,
      "perplexity": 3.547905921936035,
      "step": 4900
    },
    {
      "epoch": 77.784,
      "iterations": 4900,
      "loss": 1.0477434396743774,
      "perplexity": 2.851210117340088,
      "step": 4900
    },
    {
      "epoch": 77.784,
      "iterations": 4900,
      "loss": 1.0466102361679077,
      "perplexity": 2.8479807376861572,
      "step": 4900
    },
    {
      "epoch": 77.784,
      "iterations": 4900,
      "loss": 0.5557199120521545,
      "perplexity": 1.7431955337524414,
      "step": 4900
    },
    {
      "epoch": 77.784,
      "iterations": 4900,
      "loss": 0.11918488144874573,
      "perplexity": 1.1265782117843628,
      "step": 4900
    },
    {
      "epoch": 77.784,
      "iterations": 4900,
      "loss": 0.20341859757900238,
      "perplexity": 1.2255854606628418,
      "step": 4900
    },
    {
      "epoch": 78.576,
      "grad_norm": 1.0034043788909912,
      "learning_rate": 2.0816326530612247e-06,
      "loss": 5.528,
      "step": 4950
    },
    {
      "epoch": 78.576,
      "iterations": 4950,
      "loss": 0.8837618231773376,
      "perplexity": 2.4199862480163574,
      "step": 4950
    },
    {
      "epoch": 78.576,
      "iterations": 4950,
      "loss": 0.2886950969696045,
      "perplexity": 1.3346847295761108,
      "step": 4950
    },
    {
      "epoch": 78.576,
      "iterations": 4950,
      "loss": 1.2122282981872559,
      "perplexity": 3.3609654903411865,
      "step": 4950
    },
    {
      "epoch": 78.576,
      "iterations": 4950,
      "loss": 0.8139578700065613,
      "perplexity": 2.2568225860595703,
      "step": 4950
    },
    {
      "epoch": 78.576,
      "iterations": 4950,
      "loss": 0.3778842091560364,
      "perplexity": 1.4591939449310303,
      "step": 4950
    },
    {
      "epoch": 78.576,
      "iterations": 4950,
      "loss": 0.3456547260284424,
      "perplexity": 1.4129146337509155,
      "step": 4950
    },
    {
      "epoch": 78.576,
      "iterations": 4950,
      "loss": 0.8416080474853516,
      "perplexity": 2.3200948238372803,
      "step": 4950
    },
    {
      "epoch": 78.576,
      "iterations": 4950,
      "loss": 0.11180219799280167,
      "perplexity": 1.1182916164398193,
      "step": 4950
    },
    {
      "epoch": 79.368,
      "grad_norm": 1.2092978954315186,
      "learning_rate": 4.081632653061225e-08,
      "loss": 5.5955,
      "step": 5000
    }
  ],
  "logging_steps": 50,
  "max_steps": 5000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 80,
  "save_steps": 5000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8.36444403366101e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
