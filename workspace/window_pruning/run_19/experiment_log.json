[
  {
    "step": 0,
    "action": "baseline",
    "layers_remaining": 32,
    "perplexity": 10.907449722290039,
    "model_path": "src/checkpoints/llama3.1-8b",
    "timestamp": "2025-08-05T17:17:57.460901"
  },
  {
    "step": 1,
    "action": "prune",
    "layers_removed": [
      0,
      1,
      2
    ],
    "layers_remaining": 29,
    "perplexity": 35869.1796875,
    "timestamp": "2025-08-05T17:18:01.581031"
  },
  {
    "step": 2,
    "action": "train",
    "perplexity": 17465.412109375,
    "training_steps": 5000,
    "total_steps_used": 5000,
    "timestamp": "2025-08-05T19:50:49.470201"
  }
]