{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4da6254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "torch.cuda.is_available()\n",
    "from copy import deepcopy\n",
    "from transformers import pipeline\n",
    "import torch.nn as nn\n",
    "from transformers import LlamaForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "\n",
    "from perplexity_eval import evaluate_model_perplexity  # убедись, что он доступен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9122710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_layers(model, layers_to_remove):\n",
    "    \"\"\"\n",
    "    Удаляет указанные слои из модели LlamaForCausalLM.\n",
    "\n",
    "    Args:\n",
    "        model (LlamaForCausalLM): Исходная модель.\n",
    "        layers_to_remove (list): Индексы слоёв, которые нужно удалить.\n",
    "\n",
    "    Returns:\n",
    "        LlamaForCausalLM: Новая модель с удалёнными слоями.\n",
    "    \"\"\"\n",
    "    new_model = LlamaForCausalLM(config=deepcopy(model.config))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Копируем эмбеддинги\n",
    "        new_model.model.embed_tokens.weight.data = model.model.embed_tokens.weight.data.clone()\n",
    "        \n",
    "        # Копируем оставшиеся слои энкодера\n",
    "        new_layers = [\n",
    "            layer for idx, layer in enumerate(model.model.layers) if idx not in layers_to_remove\n",
    "        ]\n",
    "        new_model.model.layers = torch.nn.ModuleList(new_layers)\n",
    "        \n",
    "        # Копируем параметры нормализации\n",
    "        new_model.model.norm.weight.data = model.model.norm.weight.data.clone()\n",
    "        \n",
    "        # === ОБНОВИ config перед сохранением ===\n",
    "        new_model.config.num_hidden_layers = len(new_model.model.layers)\n",
    "\n",
    "        # Копируем lm_head\n",
    "        new_model.lm_head.weight.data = model.lm_head.weight.data.clone()\n",
    "\n",
    "    return new_model.to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ddb58d",
   "metadata": {},
   "source": [
    "## BASE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5596456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5031cc80316e49c3bdfbd3236304f62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "local_model_path = \"/home/user31/polina/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# === Загрузка модели и токенизатора ===\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3fe98a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaDecoderLayer(\n",
       "  (self_attn): LlamaAttention(\n",
       "    (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "    (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "    (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "  )\n",
       "  (mlp): LlamaMLP(\n",
       "    (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "    (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "    (act_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d24a1115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model_perplexity(model,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9128312d",
   "metadata": {},
   "source": [
    "## PRUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25f42c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model = prune_layers(model , layers_to_remove=[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83245e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-26): 27 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27b73220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a7513e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "def apply_lora_to_layers(\n",
    "    model,\n",
    "    layer_indices,\n",
    "    r=8,\n",
    "    alpha=32,\n",
    "    dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    "):\n",
    "    \"\"\"\n",
    "    Навешивает LoRA на указанные индексы слоёв модели LLaMA.\n",
    "    \n",
    "    :param model: Модель типа LlamaForCausalLM\n",
    "    :param layer_indices: Список индексов слоёв, на которые нужно навесить LoRA\n",
    "    :param r: ранк LoRA\n",
    "    :param alpha: параметр alpha\n",
    "    :param dropout: dropout для LoRA\n",
    "    :param bias: тип bias в LoRA ('none', 'all', 'lora_only')\n",
    "    :param task_type: тип задачи для PEFT (обычно TaskType.CAUSAL_LM)\n",
    "    :return: модель с LoRA адаптерами\n",
    "    \"\"\"\n",
    "    target_modules = []\n",
    "    for layer_idx in layer_indices:\n",
    "        prefix = f\"model.layers.{layer_idx}\"\n",
    "        target_modules.extend([\n",
    "            f\"{prefix}.self_attn.q_proj\",\n",
    "            f\"{prefix}.self_attn.k_proj\",\n",
    "            f\"{prefix}.self_attn.v_proj\",\n",
    "            f\"{prefix}.self_attn.o_proj\",\n",
    "            f\"{prefix}.mlp.gate_proj\",\n",
    "            f\"{prefix}.mlp.up_proj\",\n",
    "            f\"{prefix}.mlp.down_proj\",\n",
    "        ])\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=dropout,\n",
    "        bias=bias,\n",
    "        task_type=task_type\n",
    "    )\n",
    "    \n",
    "    return get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb35996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lora_pruned_model = apply_lora_to_layers(\n",
    "    pruned_model,\n",
    "    layer_indices = ,\n",
    "    r=8,\n",
    "    alpha=32,\n",
    "    dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555926c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eacf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Perplexity Evaluation on device: cuda ---\n",
      "Loading dataset: 'wikitext' (config: 'wikitext-103-raw-v1', split: 'test')...\n",
      "Using 50 samples from the dataset.\n",
      "Tokenizing the text...\n",
      "Calculating perplexity with max_length=1024 and stride=512...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 100%|█████████████████████████████████████████| 5/5 [00:02<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "✅ Final Perplexity: 14.1014\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.101369857788086"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model_perplexity(pruned_model,tokenizer, num_samples = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e70ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45056d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96aeb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyllama-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
