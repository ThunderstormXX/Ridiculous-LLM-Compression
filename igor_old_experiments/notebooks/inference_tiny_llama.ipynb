{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a93f36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user31/.conda/envs/tinyllama-env/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518ea24e2f564a68bd64180b078b76fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "local_model_path = \"/home/user31/polina/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    device_map=\"cuda:0\",\n",
    "    # load_in_4bit=True,            # активируем 4-bit квантование\n",
    "    torch_dtype=\"auto\"            # автоматически выбираем тип данных\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb0de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import transformers\n",
    "import os \n",
    "# 1. Подавление всех Python-предупреждений\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39d004df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "torch.cuda.is_available()\n",
    "from copy import deepcopy\n",
    "\n",
    "DEVICE = model.device\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# from torchinfo import summary\n",
    "\n",
    "# summary(model, input_size=(1, 64), dtypes=[torch.long])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68b63916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbac4cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subtasks = ['abstract_algebra', 'all', 'anatomy', 'astronomy', 'auxiliary_train', 'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry', 'college_computer_science', 'college_mathematics', 'college_medicine', 'college_physics', 'computer_security', 'conceptual_physics', 'econometrics', 'electrical_engineering', 'elementary_mathematics', 'formal_logic', 'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_computer_science', 'high_school_european_history', 'high_school_geography', 'high_school_government_and_politics', 'high_school_macroeconomics', 'high_school_mathematics', 'high_school_microeconomics', 'high_school_physics', 'high_school_psychology', 'high_school_statistics', 'high_school_us_history', 'high_school_world_history', 'human_aging', 'human_sexuality', 'international_law', 'jurisprudence', 'logical_fallacies', 'machine_learning', 'management', 'marketing', 'medical_genetics', 'miscellaneous', 'moral_disputes', 'moral_scenarios', 'nutrition', 'philosophy', 'prehistory', 'professional_accounting', 'professional_law', 'professional_medicine', 'professional_psychology', 'public_relations', 'security_studies', 'sociology', 'us_foreign_policy', 'virology', 'world_religions']\n",
    "len(all_subtasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f909b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Format prompt with one-shot example\n",
    "def format_mmlu_prompt(example, preprompt = None ):\n",
    "\n",
    "    prompt = f\"Question: {example['question']}\\n\"\n",
    "    prompt += \"Choices:\\n\"\n",
    "    for i, choice in enumerate(example['choices']):\n",
    "        prompt += f\"{chr(ord('A') + i)}. {choice}\\n\"\n",
    "    prompt += \"Give your answer. Just give the choice.\\n\"\n",
    "    \n",
    "    return preprompt + prompt\n",
    "\n",
    "# def format_mmlu_prompt(ex, num_shots=5, shots= None):\n",
    "#     # Примеры для few-shot\n",
    "    \n",
    "    \n",
    "#     prompt = \"\"\n",
    "#     for shot in shots:\n",
    "#         prompt += f\"Q: {shot['question']}\\n\"\n",
    "#         for i, choice in enumerate(shot['choices']):\n",
    "#             prompt += f\"{i+1}. {choice}\\n\"\n",
    "#         prompt += f\"A: {shot['choices'][shot['answer']]}\\n\\n\"\n",
    "\n",
    "#     # Текущий вопрос\n",
    "#     prompt += f\"Q: {ex['question']}\\n\"\n",
    "#     for i, choice in enumerate(ex['choices']):\n",
    "#         prompt += f\"{i+1}. {choice}\\n\"\n",
    "#     prompt += f\"A:\"\n",
    "    \n",
    "#     return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fce98cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(output, choices):\n",
    "    answer_part = output.split(\"A:\")[-1].strip().lower()\n",
    "    \n",
    "    # Проверяем буквы A/B/C/D\n",
    "    for letter in ['a', 'b', 'c', 'd']:\n",
    "        if letter in answer_part.split()[0]:\n",
    "            return ord(letter) - ord('a')  # A -> 0\n",
    "\n",
    "    # Проверяем цифры\n",
    "    for token in answer_part.split():\n",
    "        if token.isdigit() and 1 <= int(token) <= len(choices):\n",
    "            return int(token) - 1\n",
    "    \n",
    "    # По умолчанию: попробуем найти совпадение с вариантом\n",
    "    for i, choice in enumerate(choices):\n",
    "        if choice.lower() in answer_part:\n",
    "            return i\n",
    "\n",
    "    return None\n",
    "\n",
    "# def extract_answer(output, choices):\n",
    "#     answer_part = output.split(\"A:\")[-1].strip().lower()\n",
    "    \n",
    "#     # # Проверяем буквы\n",
    "#     # for letter in ['a', 'b', 'c', 'd']:\n",
    "#     #     if letter in answer_part.split()[0]:\n",
    "#     #         return ord(letter) - ord('a')  # A -> 0, B -> 1 и т.д.\n",
    "    \n",
    "#     # Проверяем цифры\n",
    "#     for token in answer_part.split():\n",
    "#         if token.isdigit() and 1 <= int(token) <= len(choices):\n",
    "#             return int(token) - 1\n",
    "    \n",
    "#     # По умолчанию: попробуем найти совпадение с вариантом\n",
    "#     for i, choice in enumerate(choices):\n",
    "#         if choice.lower() in answer_part:\n",
    "#             return i\n",
    "\n",
    "#     return None  # Не удалось определить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8179490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset, num_shots = 5, verbose = False ):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=model.device )\n",
    "\n",
    "    shots = dataset.select(range(num_shots))    \n",
    "    pre_prompt = \"Some examples: \"\n",
    "    for shot in shots:\n",
    "        pre_prompt += f\"Question: {shot['question']}\\n\"\n",
    "        for i, choice in enumerate(shot['choices']):\n",
    "            pre_prompt += f\"{i+1}. {choice}\\n\"\n",
    "        pre_prompt += f\"Answer: {shot['choices'][shot['answer']]}\\n\\n and next question:\"\n",
    "\n",
    "    progress_bar = tqdm(enumerate(dataset), total=len(dataset), desc=\"Evaluating\")\n",
    "\n",
    "    for i, example in progress_bar:\n",
    "        prompt = format_mmlu_prompt(example, preprompt=pre_prompt )\n",
    "        output = pipe(prompt, max_new_tokens=50, do_sample=False)[0][\"generated_text\"]\n",
    "        # raise Exception()\n",
    "        # print(output)\n",
    "        predicted_answer = extract_answer(output, example[\"choices\"])\n",
    "        # print(predicted_answer)\n",
    "        # answer_map = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
    "        # ground_truth_answer = answer_map.get(example[\"answer\"], \"\")\n",
    "        ground_truth_answer = example[\"answer\"]\n",
    "        # print(ground_truth_answer)\n",
    "        is_correct = predicted_answer == ground_truth_answer\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Question: {example['question']}\")\n",
    "            print(f\"Choices: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}\")\n",
    "            print(f\"Predicted Answer: {predicted_answer}, Ground Truth: {ground_truth_answer}, Correct: {is_correct}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            accuracy = correct / total\n",
    "            print(f\"Processed {i+1}/{len(dataset)}. Current Accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "            progress_bar.set_postfix(accuracy=f\"{accuracy:.2f}\")\n",
    "    \n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337894cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_correct = 0\n",
    "total_examples = 0\n",
    "\n",
    "for subtask in all_subtasks[:1]:\n",
    "    dataset = load_dataset(\"cais/mmlu\", subtask, split=\"test\")\n",
    "    if dataset.shape[0] < 200:\n",
    "        correct, count = evaluate_model(model, tokenizer, dataset, verbose=False)\n",
    "        total_correct += correct\n",
    "        total_examples += count\n",
    "\n",
    "overall_accuracy = total_correct / total_examples if total_examples > 0 else 0\n",
    "print(f\"\\n🎯 Общая точность на всём MMLU: {overall_accuracy:.2f} ({total_correct}/{total_examples})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a8ea88",
   "metadata": {},
   "source": [
    "## Distilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b2d5b4",
   "metadata": {},
   "source": [
    "## Layer pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0741ba23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7314a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import LlamaForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "\n",
    "def prune_layers(model, layers_to_remove):\n",
    "    \"\"\"\n",
    "    Удаляет указанные слои из модели LlamaForCausalLM.\n",
    "\n",
    "    Args:\n",
    "        model (LlamaForCausalLM): Исходная модель.\n",
    "        layers_to_remove (list): Индексы слоёв, которые нужно удалить.\n",
    "\n",
    "    Returns:\n",
    "        LlamaForCausalLM: Новая модель с удалёнными слоями.\n",
    "    \"\"\"\n",
    "    new_model = LlamaForCausalLM(config=model.config)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Копируем эмбеддинги\n",
    "        new_model.model.embed_tokens.weight.data = model.model.embed_tokens.weight.data.clone()\n",
    "        \n",
    "        # Копируем оставшиеся слои энкодера\n",
    "        new_layers = [\n",
    "            layer for idx, layer in enumerate(model.model.layers) if idx not in layers_to_remove\n",
    "        ]\n",
    "        new_model.model.layers = torch.nn.ModuleList(new_layers)\n",
    "        \n",
    "        # Копируем параметры нормализации\n",
    "        new_model.model.norm.weight.data = model.model.norm.weight.data.clone()\n",
    "        \n",
    "        # Копируем lm_head\n",
    "        new_model.lm_head.weight.data = model.lm_head.weight.data.clone()\n",
    "\n",
    "    return new_model\n",
    "\n",
    "def reuse_previous_layers(model, layers_to_replace):\n",
    "    \"\"\"\n",
    "    Заменяет указанные слои ссылками на предыдущие слои (повторное использование) в модели LlamaForCausalLM.\n",
    "\n",
    "    Args:\n",
    "        model (LlamaForCausalLM): Исходная модель.\n",
    "        layers_to_replace (list): Индексы слоёв, которые нужно заменить ссылкой на предыдущий слой.\n",
    "\n",
    "    Returns:\n",
    "        LlamaForCausalLM: Новая модель с переиспользуемыми слоями.\n",
    "    \"\"\"\n",
    "    new_model = LlamaForCausalLM(config=model.config)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Копируем эмбеддинги\n",
    "        new_model.model.embed_tokens.weight = model.model.embed_tokens.weight\n",
    "\n",
    "        # Подготавливаем новые слои\n",
    "        new_layers = []\n",
    "        for idx, layer in enumerate(model.model.layers):\n",
    "            if idx in layers_to_replace:\n",
    "                if idx == 0:\n",
    "                    raise ValueError(\"Нельзя заменить нулевой слой на предыдущий — его не существует.\")\n",
    "                # Используем ссылку на предыдущий слой\n",
    "                new_layers.append(model.model.layers[idx - 1])\n",
    "            else:\n",
    "                new_layers.append(layer)\n",
    "\n",
    "        new_model.model.layers = torch.nn.ModuleList(new_layers)\n",
    "\n",
    "        # Копируем параметры нормализации\n",
    "        new_model.model.norm.weight = model.model.norm.weight\n",
    "\n",
    "        # Копируем lm_head\n",
    "        new_model.lm_head.weight = model.lm_head.weight\n",
    "\n",
    "    return new_model\n",
    "\n",
    "\n",
    "\n",
    "def print_param_count_table(module, simple_module_types=(nn.Linear, nn.Embedding)):\n",
    "    \"\"\"\n",
    "    Подсчитывает и выводит в табличной форме количество подмодулей \n",
    "    с определённым числом параметров.\n",
    "    \n",
    "    Args:\n",
    "        module (nn.Module): модель для анализа\n",
    "        simple_module_types (tuple): типы подмодулей для подсчёта параметров\n",
    "    \"\"\"\n",
    "    param_counts = []\n",
    "    for submodule in module.modules():\n",
    "        if type(submodule) in simple_module_types:\n",
    "            num_params = sum(p.numel() for p in submodule.parameters() if p.requires_grad)\n",
    "            param_counts.append(num_params)\n",
    "\n",
    "    total_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    \n",
    "    counter = Counter(param_counts)\n",
    "    \n",
    "    print(f\"Total trainable parameters in model: {total_params:,}\\n\")\n",
    "    print(f\"{'Parameters per submodule':>24} | {'Number of such submodules':>25}\")\n",
    "    print(\"-\" * 54)\n",
    "    for params_num, count in sorted(counter.items()):\n",
    "        print(f\"{params_num:24,} | {count:25,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99b9b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def prune_llama_mlp(mlp: nn.Module, prune_ratio: float):\n",
    "    # 1. Определяем размеры\n",
    "    hidden_size = mlp.gate_proj.in_features  # 3072\n",
    "    intermediate_size = mlp.gate_proj.out_features  # 8192\n",
    "\n",
    "    new_intermediate_size = int(intermediate_size * (1 - prune_ratio))\n",
    "    assert new_intermediate_size > 0\n",
    "\n",
    "    # 2. Оцениваем важность нейронов\n",
    "    neuron_importance = torch.norm(mlp.gate_proj.weight.data, dim=1)\n",
    "    topk_idx = torch.topk(neuron_importance, k=new_intermediate_size, largest=True).indices.sort()[0]\n",
    "\n",
    "    # 3. Создаём новые линейные слои вручную\n",
    "    new_gate_proj = nn.Linear(hidden_size, new_intermediate_size, bias=False)\n",
    "    new_up_proj   = nn.Linear(hidden_size, new_intermediate_size, bias=False)\n",
    "    new_down_proj = nn.Linear(new_intermediate_size, hidden_size, bias=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        new_gate_proj.weight.copy_(mlp.gate_proj.weight.data[topk_idx])\n",
    "        new_up_proj.weight.copy_(mlp.up_proj.weight.data[topk_idx])\n",
    "        new_down_proj.weight.copy_(mlp.down_proj.weight.data[:, topk_idx])\n",
    "\n",
    "    # 4. Собираем новый mlp, используя config\n",
    "    # Важно: используйте config того слоя!\n",
    "    mlp_config = mlp.config\n",
    "    mlp_config.intermediate_size = new_intermediate_size\n",
    "\n",
    "    # В некоторых реализациях LlamaMLP принимает только config!\n",
    "    pruned_mlp = type(mlp)(mlp_config)\n",
    "    # Подменяем веса в уже созданных слоях\n",
    "    with torch.no_grad():\n",
    "        pruned_mlp.gate_proj.weight.copy_(new_gate_proj.weight)\n",
    "        pruned_mlp.up_proj.weight.copy_(new_up_proj.weight)\n",
    "        pruned_mlp.down_proj.weight.copy_(new_down_proj.weight)\n",
    "\n",
    "    return pruned_mlp\n",
    "\n",
    "def prune_llama_decoder_layer(layer: nn.Module, prune_ratio: float):\n",
    "    pruned_mlp = prune_llama_mlp(layer.mlp, prune_ratio)\n",
    "    # Attention можно тоже проредить, если нужно — аналогично, но аккуратнее с форматом.\n",
    "    # Пока не трогаем attention (или реализуйте по аналогии)\n",
    "    pruned_layer = type(layer)(\n",
    "        self_attn=layer.self_attn,  # Оставляем без изменений\n",
    "        mlp=pruned_mlp,\n",
    "        input_layernorm=layer.input_layernorm,\n",
    "        post_attention_layernorm=layer.post_attention_layernorm,\n",
    "    )\n",
    "    return pruned_layer\n",
    "\n",
    "def prune_llama_model(model, prune_ratio: float, layers_to_prune=None):\n",
    "    \"\"\"\n",
    "    Возвращает новую модель LlamaForCausalLM, где MLP каждого слоя прорежен на prune_ratio.\n",
    "    Можно задать список индексов слоев для прореживания, если нужно не все.\n",
    "    \"\"\"\n",
    "    new_model = LlamaForCausalLM(config=model.config)\n",
    "    with torch.no_grad():\n",
    "        new_model.model.embed_tokens.weight.data = model.model.embed_tokens.weight.data.clone()\n",
    "        new_layers = []\n",
    "        for idx, layer in enumerate(model.model.layers):\n",
    "            if layers_to_prune is None or idx in layers_to_prune:\n",
    "                pruned_layer = prune_llama_decoder_layer(layer, prune_ratio)\n",
    "            else:\n",
    "                pruned_layer = layer\n",
    "            new_layers.append(pruned_layer)\n",
    "        new_model.model.layers = nn.ModuleList(new_layers)\n",
    "        new_model.model.norm.weight.data = model.model.norm.weight.data.clone()\n",
    "        new_model.lm_head.weight.data = model.lm_head.weight.data.clone()\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "293c7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_duplicate = reuse_previous_layers(model, layers_to_replace=[12,14,16,18])\n",
    "model_slimmed = prune_layers(model, layers_to_remove=[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e587a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_pruned = prune_llama_model(model, prune_ratio = 0.8 , layers_to_prune=[17,18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "432382a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class LlamaDecoderLayer(GradientCheckpointingLayer):\n",
      "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
      "        super().__init__()\n",
      "        self.hidden_size = config.hidden_size\n",
      "\n",
      "        self.self_attn = LlamaAttention(config=config, layer_idx=layer_idx)\n",
      "\n",
      "        self.mlp = LlamaMLP(config)\n",
      "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
      "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Cache] = None,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n",
      "        **kwargs: Unpack[FlashAttentionKwargs],\n",
      "    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.input_layernorm(hidden_states)\n",
      "\n",
      "        # Self Attention\n",
      "        hidden_states, self_attn_weights = self.self_attn(\n",
      "            hidden_states=hidden_states,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_value=past_key_value,\n",
      "            output_attentions=output_attentions,\n",
      "            use_cache=use_cache,\n",
      "            cache_position=cache_position,\n",
      "            position_embeddings=position_embeddings,\n",
      "            **kwargs,\n",
      "        )\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        # Fully Connected\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "        hidden_states = self.mlp(hidden_states)\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        outputs = (hidden_states,)\n",
      "        if output_attentions:\n",
      "            outputs += (self_attn_weights,)\n",
      "\n",
      "        return outputs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getsource(type(model.model.layers[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f0b8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.model.layers[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5211ec8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class LlamaMLP(nn.Module):\n",
      "    def __init__(self, config):\n",
      "        super().__init__()\n",
      "        self.config = config\n",
      "        self.hidden_size = config.hidden_size\n",
      "        self.intermediate_size = config.intermediate_size\n",
      "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
      "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
      "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n",
      "        self.act_fn = ACT2FN[config.hidden_act]\n",
      "\n",
      "    def forward(self, x):\n",
      "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "        return down_proj\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(type(layer.mlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4db4250",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_layer = layer.mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00f7399f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=8192, out_features=3072, bias=False),\n",
       " SiLU(),\n",
       " Linear(in_features=3072, out_features=8192, bias=False),\n",
       " Linear(in_features=3072, out_features=8192, bias=False))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_layer.down_proj, mlp_layer.act_fn, mlp_layer.gate_proj, mlp_layer.up_proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df682f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class LlamaAttention(nn.Module):\n",
      "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
      "\n",
      "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
      "        super().__init__()\n",
      "        self.config = config\n",
      "        self.layer_idx = layer_idx\n",
      "        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
      "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
      "        self.scaling = self.head_dim**-0.5\n",
      "        self.attention_dropout = config.attention_dropout\n",
      "        self.is_causal = True\n",
      "\n",
      "        self.q_proj = nn.Linear(\n",
      "            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n",
      "        )\n",
      "        self.k_proj = nn.Linear(\n",
      "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
      "        )\n",
      "        self.v_proj = nn.Linear(\n",
      "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
      "        )\n",
      "        self.o_proj = nn.Linear(\n",
      "            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n",
      "        )\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
      "        attention_mask: Optional[torch.Tensor],\n",
      "        past_key_value: Optional[Cache] = None,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "        **kwargs: Unpack[FlashAttentionKwargs],\n",
      "    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n",
      "        input_shape = hidden_states.shape[:-1]\n",
      "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
      "\n",
      "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "\n",
      "        cos, sin = position_embeddings\n",
      "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
      "\n",
      "        if past_key_value is not None:\n",
      "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
      "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
      "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
      "\n",
      "        attention_interface: Callable = eager_attention_forward\n",
      "        if self.config._attn_implementation != \"eager\":\n",
      "            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
      "\n",
      "        attn_output, attn_weights = attention_interface(\n",
      "            self,\n",
      "            query_states,\n",
      "            key_states,\n",
      "            value_states,\n",
      "            attention_mask,\n",
      "            dropout=0.0 if not self.training else self.attention_dropout,\n",
      "            scaling=self.scaling,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
      "        attn_output = self.o_proj(attn_output)\n",
      "        return attn_output, attn_weights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(type(layer.self_attn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b69537d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaDecoderLayer(\n",
       "  (self_attn): LlamaAttention(\n",
       "    (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "    (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "    (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "  )\n",
       "  (mlp): LlamaMLP(\n",
       "    (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "    (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "    (act_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "91ef725b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.self_attn.config.num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e905e099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8dce8927",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m (cos, sin)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Прямой проход\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Передаем позиционные эмбеддинги\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# [batch_size, seq_len, hidden_dim]\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:290\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:247\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    245\u001b[0m     attention_interface \u001b[38;5;241m=\u001b[39m ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[0;32m--> 247\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    259\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py:41\u001b[0m, in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sdpa` attention does not support `output_attentions=True` or `head_mask`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please set your attention to `eager` if you want any of these features.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m     )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_key_value_groups\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43mrepeat_kv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_key_value_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     value \u001b[38;5;241m=\u001b[39m repeat_kv(value, module\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py:16\u001b[0m, in \u001b[0;36mrepeat_kv\u001b[0;34m(hidden_states, n_rep)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrepeat_kv\u001b[39m(hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, n_rep: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     batch, num_key_value_heads, slen, head_dim \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_rep \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# Параметры\n",
    "batch_size = 1\n",
    "seq_len = 1\n",
    "hidden_dim = 3072\n",
    "num_heads = layer.self_attn.config.num_attention_heads\n",
    "head_dim = hidden_dim // num_heads  # Размерность одной головы внимания\n",
    "\n",
    "\n",
    "# Входные данные\n",
    "# hidden_states = torch.randn(batch_size, seq_len, hidden_dim, device=\"cuda:0\", dtype=torch.bfloat16)\n",
    "hidden_states = torch.randn(1,1,1, hidden_dim, device=\"cuda:0\", dtype=torch.bfloat16)\n",
    "attention_mask = torch.ones(batch_size, seq_len, device=\"cuda:0\")\n",
    "\n",
    "# Генерация ротационных позиционных эмбеддингов (RoPE)\n",
    "# Теперь размерность соответствует [seq_len, head_dim]\n",
    "cos = torch.randn(seq_len, head_dim, device=\"cuda:0\", dtype=torch.bfloat16)\n",
    "sin = torch.randn(seq_len, head_dim, device=\"cuda:0\", dtype=torch.bfloat16)\n",
    "position_embeddings = (cos, sin)\n",
    "\n",
    "# Прямой проход\n",
    "output = layer(\n",
    "    hidden_states=hidden_states,\n",
    "    attention_mask=attention_mask,\n",
    "    position_embeddings=position_embeddings  # Передаем позиционные эмбеддинги\n",
    ")\n",
    "\n",
    "print(output[0].shape)  # [batch_size, seq_len, hidden_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026cafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0e797d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = layer.self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c020ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.config.attention_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f2e678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaAttention(\n",
       "  (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "  (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "  (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "  (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03b6ff69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
      ")\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_slimmed)\n",
    "print(model_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3f7a126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters in model: 3,212,749,824\n",
      "\n",
      "Parameters per submodule | Number of such submodules\n",
      "------------------------------------------------------\n",
      "               3,145,728 |                        56\n",
      "               9,437,184 |                        56\n",
      "              25,165,824 |                        84\n",
      "             394,002,432 |                         2\n",
      "Total trainable parameters in model: 2,810,072,064\n",
      "\n",
      "Parameters per submodule | Number of such submodules\n",
      "------------------------------------------------------\n",
      "               3,145,728 |                        48\n",
      "               9,437,184 |                        48\n",
      "              25,165,824 |                        72\n",
      "             394,002,432 |                         2\n",
      "Total trainable parameters in model: 2,810,072,064\n",
      "\n",
      "Parameters per submodule | Number of such submodules\n",
      "------------------------------------------------------\n",
      "               3,145,728 |                        48\n",
      "               9,437,184 |                        48\n",
      "              25,165,824 |                        72\n",
      "             394,002,432 |                         2\n"
     ]
    }
   ],
   "source": [
    "print_param_count_table(model)\n",
    "print_param_count_table(model_duplicate)\n",
    "print_param_count_table(model_slimmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ed7b2d",
   "metadata": {},
   "source": [
    "# fast test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4706c076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, tell me about the situation in Russia this year.\n",
      "The situation in Russia has been marked by significant challenges and tensions, particularly in the following areas:\n",
      "\n",
      "1.  **Economic instability**: Russia has faced economic difficulties, including a decline in oil prices, which has impacted the country's economy. The government has implemented various measures to stabilize the economy, including a freeze on foreign currency exchange rates and a reduction in interest rates.\n",
      "2.  **Ukraine conflict**: The conflict in Ukraine continues to be a major concern, with ongoing fighting between Ukrainian government forces and Russian-backed separatists. The situation remains volatile, and there have been reports of human rights abuses and civilian casualties.\n",
      "3.  **Human rights concerns**: Russia has faced criticism for its human rights record, particularly in the areas of freedom of speech, assembly, and the press. The government has been accused of suppressing dissent and cracking down on opposition groups.\n",
      "4.  **Corruption**: Corruption remains a significant issue in Russia, with many high-ranking officials and business leaders accused of embe\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=model.device )\n",
    "prompt = 'Hello, tell me about the situation in Russia this year'\n",
    "answer = pipe(prompt, max_new_tokens=200, do_sample=False)[0][\"generated_text\"]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d497839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, tell me about the situation in Russia this year 2023, and the impact of the situation on the world.\n",
      "The situation you talking talking about mean the situation of the situation of the situation.  situation. ( there. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the. the.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model_duplicate, tokenizer=tokenizer, device_map=model.device )\n",
    "prompt = 'Hello, tell me about the situation in Russia this year'\n",
    "answer = pipe(prompt, max_new_tokens=200, do_sample=False)[0][\"generated_text\"]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bff46a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------0 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year........................................................................................................................................................................................................\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------1 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.                                                                                                                                                                                                       \n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------2 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "The situation in Russia is complex and multifaceted, with various factors contributing to the country's current state. Here are some key aspects:\n",
      "\n",
      "1.  **Economic Challenges**: Russia has faced significant economic challenges, including a decline in oil prices, which has reduced government revenue and led to a decline in the ruble's value against the US dollar. The country has also struggled with inflation, which has been exacerbated by the ongoing conflict in Ukraine and the subsequent sanctions imposed by Western countries.\n",
      "2.  **Ukraine Conflict**: The ongoing conflict in Ukraine has had a significant impact on Russia. The conflict has led to a decline in oil prices, which has affected the Russian economy. The sanctions imposed by Western countries have also had a negative impact on the Russian economy.\n",
      "3.  **Corruption and Governance**: Corruption and governance issues have been a persistent problem in Russia. The country has struggled with high levels of corruption, which has led to a decline in trust in institutions and a lack of\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------3 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "The situation in Russia in 2023 is complex and multifaceted, with various factors contributing to the country's dynamics. Here are some key aspects:\n",
      "\n",
      "1.  **Economic Challenges**: Russia is facing economic difficulties, including a decline in oil prices, which has impacted the country's revenue. The government has implemented measures to diversify the economy, but the process is slow.\n",
      "2.  **Ukraine Conflict**: The ongoing conflict in Ukraine has had a significant impact on Russia. The country has been involved in the war, and the situation remains volatile. The conflict has also affected the global economy and led to increased tensions between Russia and Western countries.\n",
      "3.  **Human Rights Concerns**: Russia has faced criticism for its human rights record, particularly regarding the treatment of LGBTQ+ individuals and the suppression of dissenting voices. The government has also been accused of suppressing free speech and independent media.\n",
      "4.  **Corruption and Governance**: Corruption remains a significant issue in Russia, with\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------4 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "The situation in Russia is complex and multifaceted. Here are some key developments and challenges:\n",
      "\n",
      "1. **Economic challenges**: Russia has faced economic difficulties, including a decline in oil prices, which has impacted the country's budget and revenue. The government has implemented various measures to address these challenges, including reducing public spending and increasing taxes.\n",
      "2. **Ukraine conflict**: The ongoing conflict in Ukraine has had a significant impact on Russia, with many Russians expressing support for the Russian military intervention. The conflict has also led to increased tensions with the West, particularly the United States and European Union.\n",
      "3. **Corruption and crime**: Corruption remains a significant issue in Russia, with many high-profile cases of embezzlement and money laundering. The government has taken steps to address corruption, including increasing penalties for corrupt officials.\n",
      "4. **Human rights**: Russia has faced criticism for its human rights record, particularly in regards to the treatment of LGBTQ+ individuals and the use of torture in prisons.\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------5 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "The situation in Russia is complex and multifaceted, with various factors contributing to the current state of affairs. Here are some key points to consider:\n",
      "\n",
      "1. **Economic challenges**: Russia has faced significant economic difficulties in recent years, including a decline in oil prices, which has reduced the country's revenue and led to a decline in the value of the ruble. This has resulted in high inflation, unemployment, and a decline in living standards.\n",
      "2. **Ukraine crisis**: The conflict in Ukraine, which began in 2014, remains unresolved. Russia's annexation of Crimea and support for separatist groups in eastern Ukraine have led to ongoing fighting, displacement of people, and humanitarian crises.\n",
      "3. **Human rights concerns**: Russia has faced criticism for its human rights record, particularly regarding the treatment of LGBTQ+ individuals, journalists, and opposition figures. The government has also been accused of suppressing dissent and limiting freedom of speech.\n",
      "4. **International relations**: Russia's relations with the\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------6 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "The situation in Russia has been marked by significant challenges and tensions, particularly in the following areas:\n",
      "1. **Ukraine Conflict**: The ongoing conflict between Russia and Ukraine has continued to escalate, with both countries accusing each other of aggression. The conflict began in 2014, and while there have been periods of relative calm, tensions remain high.\n",
      "2. **Economic Sanctions**: The United States and the European Union have imposed economic sanctions on Russia in response to its actions in Ukraine and other issues. These sanctions have had a significant impact on Russia's economy, particularly in the energy sector.\n",
      "3. **Human Rights Concerns**: Russia has faced criticism for its human rights record, particularly in the treatment of LGBTQ+ individuals and the suppression of opposition voices. The government has been accused of restricting civil liberties and freedom of speech.\n",
      "4. **Corruption and Electoral Interference**: Russia has been accused of interfering in the electoral processes of other countries, including the 2016 US presidential election.\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------7 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "The situation in Russia has been marked by a mix of challenges and tensions. Here are some key developments that have shaped the country's situation this year:\n",
      "\n",
      "1.  **Economic struggles**: Russia has faced economic difficulties, including a decline in oil prices, which has impacted the country's revenue and led to a decrease in government spending. This has resulted in a reduction in social services and infrastructure development.\n",
      "2.  **Ukraine conflict**: The ongoing conflict in Ukraine has continued to affect Russia, with the country imposing sanctions on Ukraine and its allies. This has led to a deterioration in relations with Western countries and has resulted in a significant impact on Russia's economy and international reputation.\n",
      "3.  **Corruption**: Corruption has remained a significant issue in Russia, with high-profile cases of embezzlement and bribery continuing to plague the country. This has led to widespread public discontent and has undermined trust in the government.\n",
      "4.  **Human rights**: Russia has faced criticism for its human rights\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------8 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "The situation in Russia has been marked by a series of challenges and controversies. Here are some key developments that have shaped the country's landscape in 2023:\n",
      "\n",
      "1.  **Economic struggles**: Russia has faced economic difficulties, including a decline in oil prices, which has impacted the country's revenue and led to a decline in the value of the ruble.\n",
      "2.  **Ukraine conflict**: The ongoing conflict in Ukraine has continued to be a major concern for Russia, with the country providing military support to the Ukrainian government. The conflict has resulted in significant human suffering and displacement.\n",
      "3.  **Human rights concerns**: Russia has faced criticism for its human rights record, particularly in the context of the conflict in Ukraine. The country has been accused of violating international law and human rights standards.\n",
      "4.  **Corruption and governance**: There have been concerns about corruption and governance in Russia, with the country's leadership facing criticism for its handling of the economy and its response to the conflict\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------9 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "The situation in Russia has been quite volatile this year, with ongoing conflicts in Ukraine, tensions with the West, and a significant economic downturn. The Russian government has been cracking down on dissent and opposition, with several high-profile arrests and imprisonment of opposition leaders and activists.\n",
      "The situation in Ukraine has been particularly tense, with ongoing fighting between Ukrainian government forces and Russian-backed separatists in the Donbas region. The conflict has resulted in significant human rights abuses, including the use of torture, arbitrary detention, and forced disappearances.\n",
      "The economic situation in Russia has been severely impacted by international sanctions, which have led to a significant decline in the country's economy. The sanctions have resulted in a severe shortage of food, medicine, and other essential goods, leading to widespread poverty and hardship.\n",
      "The situation in Russia is complex and multifaceted, with various factors contributing to the country's instability. The ongoing conflicts in Ukraine and the economic downturn have created a sense of uncertainty and fear among the Russian people.\n",
      "It\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------10 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "The situation in Russia is complex and multifaceted. Here are some key developments that have shaped the country's politics and society in 2023:\n",
      "\n",
      "1.  **Economic challenges**: Russia has faced significant economic challenges, including a decline in oil prices, which has impacted the country's economy and led to a decline in the value of the ruble.\n",
      "2.  **Ukraine conflict**: The ongoing conflict in Ukraine has had a significant impact on Russia's relations with the West. The country has imposed sanctions on several Western countries in response to their support for Ukraine.\n",
      "3.  **Human rights concerns**: Russia has been criticized for its human rights record, including the treatment of opposition figures and the suppression of dissent.\n",
      "4.  **Corruption**: Corruption is a significant problem in Russia, with many high-ranking officials accused of embezzlement and cronyism.\n",
      "5.  **Media freedom**: Russia has been criticized for its restrictions on media freedom, with many independent outlets facing\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------11 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "The situation in Russia is complex and multifaceted. Here's a brief overview:\n",
      "\n",
      "**Economic situation:**\n",
      "\n",
      "Russia's economy has been facing significant challenges in recent years, including:\n",
      "\n",
      "1. **Sanctions:** The ongoing sanctions imposed by the United States, the European Union, and other countries have had a significant impact on Russia's economy, particularly in the energy sector.\n",
      "2. **Inflation:** Russia has been experiencing high inflation rates, which have been exacerbated by the sanctions and the decline in oil prices.\n",
      "3. **Currency fluctuations:** The Russian ruble has been volatile, and the country has been struggling to maintain a stable exchange rate.\n",
      "\n",
      "**Politics:**\n",
      "\n",
      "1. **Vladimir Putin's presidency:** Putin has been in power since 2000, and his presidency has been marked by authoritarian tendencies and human rights abuses.\n",
      "2. **Ukraine crisis:** The ongoing conflict in Ukraine has been a major challenge for Russia, with the country facing international sanctions and isolation.\n",
      "3.\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------12 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "The situation in Russia has been marked by a significant increase in tensions between Russia and the West, particularly the United States and European Union. The main issue is the ongoing conflict in Ukraine, which has been ongoing since 2014. The conflict has led to a significant escalation of tensions between Russia and the West, with Russia accusing Ukraine of supporting separatist groups in the eastern regions of Donbass and Luhansk.\n",
      "\n",
      "In 2022, Russia launched a large-scale invasion of Ukraine, which has resulted in significant human suffering and loss of life. The invasion has also led to a significant increase in the number of refugees fleeing Ukraine, with many seeking asylum in other countries.\n",
      "\n",
      "The situation in Russia has also been marked by a significant increase in authoritarianism, with President Vladimir Putin's government cracking down on opposition and dissent. The government has also been accused of human rights abuses, including the persecution of LGBTQ+ individuals and the suppression of free speech.\n",
      "\n",
      "In addition, there have been reports of widespread corruption\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------13 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. The situation in Russia is quite complex and multifaceted, but I'll try to provide a brief overview of the key developments and trends.\n",
      "\n",
      "In 2022, Russia has faced significant challenges, including:\n",
      "\n",
      "1.  **Economic sanctions**: The international community has imposed economic sanctions on Russia in response to its actions in Ukraine, including the annexation of Crimea in 2014. These sanctions have had a significant impact on the Russian economy, leading to inflation, currency fluctuations, and a decline in economic growth.\n",
      "2.  **Energy dependence**: Russia is heavily dependent on energy exports, which has made it vulnerable to fluctuations in global energy prices. The ongoing conflict in Ukraine has further exacerbated this vulnerability, leading to a decline in energy exports and a rise in energy prices.\n",
      "3.  **Inflation**: Russia has experienced high inflation rates in recent years, driven by a combination of factors including economic sanctions, currency fluctuations, and a decline in economic growth.\n",
      "4.  **Human rights concerns\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------14 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year?\n",
      "As of my knowledge cutoff in March 2022, the situation in Russia had escalated significantly due to the ongoing conflict with Ukraine. The Russian government, led by President Vladimir Putin, had launched a large-scale invasion of Ukraine, which began in late 2021 but intensified in February 2022. The conflict has resulted in significant human rights abuses, including the targeting of civilians, the destruction of infrastructure, and the displacement of people.\n",
      "\n",
      "The international community has condemned the invasion, and many countries have imposed economic sanctions on Russia. The United States, European Union, and other major economies have also provided significant financial and military support to Ukraine.\n",
      "\n",
      "However, the situation in Russia itself has also been affected by the conflict. Protests and demonstrations have taken place in major cities, with many Russians expressing discontent with the government's handling of the war. Some have also called for greater transparency and accountability within the government.\n",
      "\n",
      "Despite the challenges, the Russian government has maintained control over the country, and the situation remains complex\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------15 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "The situation in Russia has been marked by several developments in 2023, which have significant implications for the country and the world. Here are some key points to consider:\n",
      "\n",
      "1.  **Economic challenges**: Russia's economy has been facing significant challenges in recent years, including a decline in oil production and a decline in global demand for Russian energy exports. The country has also been affected by Western sanctions, which have limited its access to international trade and investment.\n",
      "2.  **Military mobilization**: In September 2022, Russia launched a large-scale military mobilization, which has resulted in significant human casualties and displacement. The mobilization has also led to a significant increase in the country's military spending, which has put a strain on the economy.\n",
      "3.  **Nuclear energy**: Russia has been a major player in the global nuclear energy market, and its nuclear power plants have been a key source of electricity for the country. However, the country's nuclear industry has faced significant challenges\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------16 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "The situation in Russia has been marked by significant developments in various areas, including politics, economy, and international relations. Here are some key points to consider:\n",
      "\n",
      "1.  **Economic Challenges**: Russia has faced economic challenges, including a decline in oil prices, which has led to a decrease in the country's GDP. The Russian government has implemented various measures to address the issue, such as increasing energy production and investing in new technologies.\n",
      "2.  **Politics**: The Russian government has been criticized for its human rights record, particularly in regards to the treatment of opposition leaders and activists. The government has also been accused of suppressing dissent and freedom of speech.\n",
      "3.  **International Relations**: Russia has been involved in several international conflicts, including the conflict in Ukraine and the annexation of Crimea. The country has also been accused of interfering in the elections of other countries, including the United States.\n",
      "4.  **Corruption**: Corruption has been a significant issue in Russia, with many high-ranking\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------17 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "The situation in Russia in 2023 is complex and multifaceted, with various factors contributing to the country's internal and external challenges. Here are some key developments:\n",
      "\n",
      "1. **Economic challenges**: Russia is facing significant economic difficulties, including a decline in oil prices, which has reduced the country's revenue and led to a sharp decline in the ruble's value. The government has responded by implementing austerity measures, including reducing subsidies and increasing prices for certain goods.\n",
      "2. **Ukraine conflict**: The ongoing conflict between Russia and Ukraine has continued to escalate, with both sides accusing each other of aggression. The conflict has led to significant human suffering, displacement, and economic losses for Ukraine.\n",
      "3. **Energy sanctions**: The international community has imposed energy sanctions on Russia in response to its actions in Ukraine and other areas. These sanctions have had a significant impact on Russia's economy, particularly in the energy sector.\n",
      "4. **Human rights concerns**: Russia has faced criticism over human rights issues,\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------18 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. 2023 is a critical year for Russia, with a significant amount of international pressure on the country, particularly with regards to its invasion of Ukraine. The situation is complex and multifaceted, and it's difficult to provide a comprehensive overview without getting into the weeds. However, I'll try to break it down into some key points.\n",
      "\n",
      "**Economic Sanctions and International Isolation**\n",
      "\n",
      "The international community has imposed severe economic sanctions on Russia in response to its invasion of Ukraine. These sanctions have had a significant impact on Russia's economy, particularly in the energy sector. The sanctions have limited Russia's access to international financial markets, and the country's currency, the ruble, has lost significant value against the US dollar.\n",
      "\n",
      "**Energy Sector**\n",
      "\n",
      "The energy sector has been a major point of contention between Russia and the West. Russia has been a major player in the global energy market, and the sanctions have limited its ability to export energy to Europe. The Nord Stream 2 pipeline, which was\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------19 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "The situation in Russia is complex and multifaceted, with various factors contributing to its current state. Here are some key developments that have shaped the country's situation this year:\n",
      "\n",
      "1.  **Ongoing Conflict in Ukraine**: The conflict in Ukraine, which began in 2014, continues to be a major issue. Russia's annexation of Crimea in 2014 and its support for separatist forces in eastern Ukraine have led to ongoing tensions with Ukraine and the international community.\n",
      "2.  **Economic Sanctions**: The international community has imposed economic sanctions on Russia in response to its actions in Ukraine and other issues, such as its treatment of LGBTQ+ individuals and its interference in the 2016 US presidential election. These sanctions have had a significant impact on Russia's economy.\n",
      "3.  **Human Rights**: Russia has been criticized for its human rights record, particularly with regards to its treatment of LGBTQ+ individuals, journalists, and opposition politicians. The government has also been accused of suppressing\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------20 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year 2023\n",
      "The situation in Russia in 2023 is complex and multifaceted. The country is facing a range of challenges and challenges that are affecting its economy, politics, and society. Here are some key developments and trends that are shaping the situation in Russia in 2023:\n",
      "\n",
      "1. Economic challenges: Russia is facing significant economic challenges, including a decline in oil prices, a weak currency, and a shrinking domestic market. The country's economy has been hit hard by the COVID-19 pandemic, and the ongoing conflict in Ukraine has further exacerbated the situation.\n",
      "2. Ongoing conflict in Ukraine: The conflict in Ukraine has been ongoing for several years, and it has had a significant impact on Russia's economy and international relations. The conflict has led to a decline in Russia's relations with Western countries, including the United States, the United Kingdom, and other NATO countries.\n",
      "3. Government crackdown on opposition: The Russian government has been cracking down on opposition groups and activists, including those\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------21 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "2022 has been a tumultuous year for Russia, marked by significant geopolitical tensions, economic sanctions, and military actions. Here are some key events that have shaped the situation in Russia this year:\n",
      "\n",
      "1. **Ukraine Crisis**: The ongoing conflict in Ukraine has been a major source of tension between Russia and the international community. Russia's annexation of Crimea in 2014 led to international condemnation, and the situation has escalated further in 2022, with Russia's ongoing support for separatist groups in eastern Ukraine.\n",
      "\n",
      "2. **Nuclear Threats**: Russia has made several nuclear threats, including a warning to the United States and Europe to stay away from Ukraine, and a warning to the United States to stop interfering in Russian affairs. These threats have raised concerns about a potential nuclear conflict.\n",
      "\n",
      "3. **Economic Sanctions**: The United States, the European Union, and other countries have imposed significant economic sanctions on Russia in response to its actions in Ukraine and other human rights abuses. These\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------22 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. 2022 has been a year of great turmoil and change in Russia, marked by a significant shift in the country's global standing and a major crisis in its economy. Here are some key developments that have shaped the situation in Russia this year:\n",
      "\n",
      "1.  **Ukraine Conflict**: The ongoing conflict in Ukraine has been a major factor in the turmoil in Russia. The conflict began in 2014, but it has escalated significantly in 2022, with Russia's invasion of Ukraine in February. The conflict has resulted in significant human rights abuses, displacement of people, and a major crisis in the global economy.\n",
      "\n",
      "2.  **Economic Crisis**: The economic crisis in Russia has been caused by a combination of factors, including the conflict in Ukraine, a decline in oil prices, and a significant devaluation of the Russian ruble. The crisis has resulted in high inflation, unemployment, and a significant decline in the country's GDP.\n",
      "\n",
      "3.  **Global Response**: The global response to\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------23 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "2023 is off to a tumultuous start for Russia, with the country facing a number of challenges and uncertainties. Here are some key developments that have shaped the situation in Russia this year:\n",
      "\n",
      "1. Economic sanctions: The international community has imposed economic sanctions on Russia in response to its actions in Ukraine, including the annexation of Crimea and ongoing conflict in eastern Ukraine. These sanctions have had a significant impact on Russia's economy, with many Russian companies and individuals facing restrictions on access to international markets and financial resources.\n",
      "2. Sanctions on Russian oligarchs: The international community has also targeted Russian oligarchs, who are wealthy individuals who have accumulated wealth and power through their business dealings. These sanctions have restricted their access to international financial markets and assets, making it difficult for them to maintain their wealth and influence.\n",
      "3. Sanctions on Russian banks: The international community has also imposed sanctions on Russian banks, which has restricted their access to international financial markets and made it difficult for them to conduct business with\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------24 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "2023 has been a significant year for Russia, marked by several key events and developments. Here are a few notable ones:\n",
      "\n",
      "1.  **Ukraine Conflict and Sanctions**: The ongoing conflict in Ukraine, which began in 2014, has continued to escalate. Russia's actions in Ukraine have led to international sanctions, including those imposed by the United States and European Union. These sanctions have had a significant impact on Russia's economy and have led to a decline in its economic and economic competitiveness.\n",
      "\n",
      "2.  **Military Confrontations**: Russia has been involved in military confrontations with Ukraine and has also been accused of supporting and arming other countries that are involved in conflicts in the region. These military actions have led to significant human and economic costs.\n",
      "\n",
      "3.  2023 has also seen a significant increase in tensions between Russia and the United States and its allies. This has led to a number of diplomatic and diplomatic efforts to de-escalate tensions and find a peaceful resolution to\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------25 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year.\n",
      "**Warning: Some of the information I will provide is sensitive and may be disturbing to some readers.**\n",
      "\n",
      "As of my knowledge cutoff in 2023, Russia is experiencing a complex and multifaceted situation. Here are some key developments:\n",
      "\n",
      "1. **Invasion of Ukraine**: Russia's ongoing invasion of Ukraine, which began in February 2022, continues to be a major source of tension in the region. The conflict has resulted in significant human suffering, displacement, and economic disruption.\n",
      "2. **Economic sanctions**: The international community has imposed various economic sanctions on Russia in response to its actions in Ukraine. These sanctions have had a significant impact on Russia's economy, particularly in the energy and finance sectors.\n",
      "3. **Human rights concerns**: The Russian government has been accused of numerous human rights abuses, including the suppression of dissent, persecution of minorities, and restrictions on freedom of speech and assembly.\n",
      "4. **Media censorship**: The Russian government has been criticized for its efforts to censor and\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------26 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. 2023 has been a year of significant challenges for Russia, with the ongoing conflict in Ukraine and the ongoing economic sanctions imposed by Western countries. The conflict has resulted in significant human suffering and loss of life, with estimates suggesting that over 13,000 people have been killed in the conflict since its start in 2022. The economic sanctions have had a significant impact on Russia's economy, with many businesses and individuals facing significant financial hardship. The sanctions have also led to a significant decline in Russia's oil and natural gas exports, which has further exacerbated the economic hardship. Despite these challenges, Russia has continued to maintain its military presence in Ukraine and has continued to escalate the conflict. The situation remains highly volatile and uncertain, with many experts predicting that the conflict will continue for years to come.\n",
      "\n",
      "In addition to the conflict in Ukraine, Russia has faced significant challenges in other areas, including the ongoing COVID-19 pandemic, the ongoing economic downturn in the United States, and the growing concerns about climate\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------27 th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year, 2023, regarding the war in Ukraine.\n",
      "As of my last update, the situation in Russia in 2023 regarding the war in Ukraine is complex and dynamic. Here's a brief overview of the current situation:\n",
      "\n",
      "1.  ** continued support for the war effort**: The Russian government, led by President Vladimir Putin, has continued to support the war effort in Ukraine, despite international pressure and economic sanctions. The Russian military has maintained a significant presence in eastern Ukraine, particularly in the Donbas region, where pro-Russian separatists have been waging a conflict with the Ukrainian government since 2014.\n",
      "2.  ** escalation of hostilities**: In recent months, there have been reports of increased hostilities between Russian and Ukrainian forces, including reports of Russian forces launching new attacks on Ukrainian positions in eastern Ukraine. These attacks have resulted in significant losses for both sides, with reports of civilian casualties and destruction of infrastructure.\n",
      "3.  ** international response**: The international community has continued to respond to\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(28):\n",
    "    print(f'-----------------{i} th layer dropped------------------------------------------------')\n",
    "    model_slimmed = prune_layers(model, layers_to_remove=[i])\n",
    "    pipe = pipeline(\"text-generation\", model=model_slimmed, tokenizer=tokenizer, device_map=model.device )\n",
    "    prompt = 'Hello, tell me about the situation in Russia this year'\n",
    "    answer = pipe(prompt, max_new_tokens=200, do_sample=False)[0][\"generated_text\"]\n",
    "    print(answer)\n",
    "    print('-------------------------------------------------------------------------\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20b653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------[2, 3, 4] th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. The Russian government has been facing a number of challenges in recent years, but the country has also been working to address these issues and has been making progress in various areas. The Russian government has been actively engaged in a number of initiatives and has been making progress in these areas. The Russian government has been working to address the challenges in the country and has been making progress in these areas. The Russian government has been actively engaged in a number of initiatives and has been making progress in these areas. The Russian government has been working to address the challenges in the country and has been making progress in these areas. The Russian government has been actively engaged in a number of initiatives and has been making progress in these areas. The Russian government has been working to address the challenges in the country and has been making progress in these areas. The Russian government has been actively engaged in a number of initiatives and has been making progress in these areas. The Russian government has been working to address the challenges in the country and has\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------[2, 3, 5] th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. I am not sure if I am correct in my assessment of the situation in Russia.\n",
      "I am not sure if I am correct in my assessment of the situation in Russia.\n",
      "I am not sure if I am correct in my assessment of the situation in Russia.\n",
      "I am not certain about the situation in Russia.\n",
      "I am not sure if I am correct in my assessment of the situation in Russia.\n",
      "I am not certain about the situation in Russia.\n",
      "I am not sure if I am correct in my assessment of the situation in Russia.\n",
      "\n",
      "## Step 1: Understand the context\n",
      "The situation in Russia is complex and multifaceted.\n",
      "\n",
      "## Step 2: Gather information\n",
      "I need to gather information about the current situation in Russia.\n",
      "\n",
      "## Step 3: Analyze the data\n",
      "I must analyze the data to identify the key issues in Russia.\n",
      "\n",
      "## Step 4: Draw conclusions\n",
      "Based on the analysis, I can draw a conclusion about the situation in Russia.\n",
      "\n",
      "## Step 5: Evaluate the\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------[2, 3, 6] th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. What do you know about the situation in Russia this year?\n",
      "The situation in Russia this year is a complex one, with many challenges and uncertainties. The country is facing a number of challenges, including:\n",
      "1. Economic difficulties: Russia is facing a severe economic downturn, with a decline in GDP and a significant increase in unemployment.\n",
      "2. International sanctions: Russia is under international sanctions, which have led to a decline in trade and a significant increase in imports.\n",
      "3. Energy crisis: Russia is facing an energy crisis, with a significant increase in prices and a decline in production.\n",
      "4. Social unrest: Russia is experiencing social unrest, with a rise in protests and a decline in public demonstrations.\n",
      "5. Political instability: Russia is facing political instability, with a rise in protests and a decline in public demonstrations.\n",
      "6. Health care challenges: Russia is facing health care challenges, with a significant increase in costs and a decline in access to healthcare.\n",
      "7. Environmental challenges: Russia is facing environmental challenges,\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------[2, 3, 7] th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. The Russian government has been working to strengthen the country's economy and has been making efforts to reduce poverty and inequality. However, the situation in Russia is more complex than in many other countries. The Russian government has been struggling to reduce poverty and inequality, and the situation is becoming increasingly dire. The situation is not just a problem in Russia, but a global issue. The international community is taking notice of the situation, and the international community is calling for action. The international community is urging the Russian government to take more decisive action to address the situation. The international community is urging the Russian government to take more decisive action to address the situation. The international community is calling for the Russian government to take more decisive action. The international community is urging the Russian government to take more decisive action. The international community is calling for the Russian government to take more decisive action. The international community is urging the Russian government to take more decisive action. The international community is calling for the Russian government to take more decisive\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------[2, 3, 8] th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. The Russian government has been working to reduce the number of children who are not vaccinated against COVID-19, and the number of children who are not vaccinated has been increasing. The government has also been working to increase the number of children who are vaccinated against COVID-19. The government has been working to increase the number of children who are not vaccinated against COVID-19. The government has been working to reduce the number of children who are not vaccinated against COVID-19. The government has been working to increase the number of children who are not vaccinated against COVID-19. The government has been working to increase the number of children who are not vaccinated against COVID-19. The government has been working to reduce the number of children who are not vaccinated against COVID-19. The government has been working to increase the number of children who are not vaccinated against COVID-19. The government has been working to reduce the number of children who are not vaccinated against COVID-19. The government has been working\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------[2, 3, 9] th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. The situation in Russia is that the country is facing a severe economic crisis, with a high inflation rate and a struggling economy. The situation in Russia is that the country is facing a severe economic crisis, with a high inflation rate and a struggling economy. The situation in Russia is that the country is facing a severe economic crisis, with a high inflation rate and a struggling economy. The situation in Russia is that the country is facing a severe economic crisis, with a high inflation rate and a struggling economy. The situation in Russia is that the country is facing a severe economic crisis, with a high inflation rate and a struggling economy. The situation in Russia is that the country is facing a severe economic crisis, with a high inflation rate and a struggling economy. The situation in Russia is that the country is facing a severe economic crisis, with a high inflation rate and a struggling economy. The situation in Russia is that the country is facing a severe economic crisis, with a high inflation rate and a struggling economy\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------[2, 3, 10] th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. 2023.\n",
      "I'm referring to the invasion of Ukraine, which began in February 2022, and the subsequent Russian military intervention in Ukraine, also known as the \"Donbas\" or \"Donbas\" (a reference to the Russian word for Moscow, which is the capital city of Russia, not Ukraine). I'm not sure what you're referring to, but I'm happy to help you with your question about the situation in Russia this year.\n",
      "\n",
      "I'm happy to help, but I want to clarify that the situation you're referring to is not the invasion of Ukraine, which began in February 2022, and the subsequent Russian military intervention in Ukraine, also known as the \"Donbas\" or \"Donbas\" (a reference to the Russian city of Moscow, which is the capital city of Russia, not Ukraine). It seems like you're asking about a situation that doesn't exist, as the Russian city of Moscow is the capital city of Russia, not Ukraine, and\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------[2, 3, 11] th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. (I assume you're referring to the 2022 Winter Olympics in Beijing, China, but I think you meant the 2022 Winter Olympics in Beijing, China. I assume that's the case, as I'm not sure if you're referring to the 2022 Winter Olympics in Beijing, China. (I assume that's the case, as I'm not sure if you're referring to the 2022 Winter Olympics in Beijing, China.)\n",
      "\n",
      "I think there may be some confusion here. The 2022 Winter Olympics were held in Beijing, China, but I think you meant the 2022 Winter Olympics in Beijing, China. (I assume that's the case, as I'm not sure if you're referring to the 2022 Winter Olympics in Beijing, China.)\n",
      "\n",
      "I think there may be some confusion here. The 2022 Winter Olympics were held in Beijing, China, but I think you meant the 2022 Winter Olympics in Beijing, China.\n",
      "\n",
      "I think there\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------[2, 3, 12] th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. The situation is that the Russian government has been facing a severe shortage of medical supplies, including antibiotics, and the government has been struggling to find a solution to the shortage.\n",
      "The situation in Russia is not unique to Russia, as many countries face similar shortages of medical supplies. However, the Russian government has been facing a unique challenge in addressing the shortage. The government has been trying to find a solution to the shortage, but the situation is becoming increasingly dire.\n",
      "\n",
      "The shortage of medical supplies is affecting the Russian healthcare system, and the government is struggling to find a solution. The situation is becoming increasingly dire, and the government is under pressure to find a solution.\n",
      "\n",
      "The Russian government has been trying to find a solution to the shortage, but so far, no solution has been found. The situation is becoming increasingly dire, and the government is under pressure to find a solution.\n",
      "\n",
      "The shortage of medical supplies is a symptom of a larger problem, the lack of investment in healthcare. The Russian government is aware of\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------[2, 3, 13] th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. The Russian government has been working on a new law to banish the ban on the use of fossil fuels in the country. The law aims to reduce carbon emissions by 50% by 2030. The law is expected to be implemented by 2025.\n",
      "The law aims to reduce carbon emissions by 50% by 2030. The law is expected to be implemented by 2025. The law is expected to be implemented by 2025.\n",
      "The law aims to reduce carbon emissions by 50% by 2030. The law is expected to be implemented by 2025.\n",
      "The law aims to reduce carbon emissions by 50% by 2030. The law is expected to be implemented by 2025.\n",
      "The law aims to reduce carbon emissions by 50% by 2030. The law is expected to be implemented by 2025.\n",
      "The law aims to reduce carbon emissions by 50% by 2023. The law is expected to be\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------[2, 3, 14] th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. The country has been facing a number of challenges, including the COVID-19 pandemic, economic instability, and a growing sense of discontent with the government. Here are some of the key issues affecting Russia:\n",
      "1. Economic instability: The COVID-19 pandemic has exacerbated existing economic challenges, including high unemployment, low wages, and a lack of investment opportunities. Many Russians are struggling to make ends meet, and the government's response to the crisis has been criticized for being inadequate.\n",
      "2. Unrested discontent: There is a growing sense of discontent among Russians with the government, particularly with regards to the economic policies and the lack of investment opportunities. The protests and demonstrations have been taking place in major cities, with the opposition of the government being criticized for its handling of the crisis.\n",
      "3. The COVID-19 pandemic has also had a significant impact on the country's healthcare system. The pandemic has led to a shortage of medical staff, a lack of investment in healthcare, and a growing sense of discontent\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------[2, 3, 15] th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. The situation is that the Russian government has been accused of human rights violations in the United Nations, and the US government has been accused of human rights violations in the US. Both governments are accused of violating human rights in their respective countries. This is a classic example of a \"bad\" system, where the government is the aggressor and the people are the victims.\n",
      "The situation in Russia is complex and multifaceted. The Russian government has been accused of human rights violations, including the suppression of free speech and the restriction of civil liberties. The Russian government has also been accused of violating human rights, including the suppression of free speech and the restriction of access to information. The Russian government has also been accused of violating human rights, including the suppression of free movement and the restriction of access to information.\n",
      "The situation in the US is also complex and multifaceted. The US government has been accused of human rights violations, including the suppression of free speech and the restriction of civil liberties. The US\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------[2, 3, 16] th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. The situation in Russia is quite different from the situation in the United States. The situation in Russia is quite different from the situation in the United States. The situation in Russia is quite different from the situation in the United States. (The situation in Russia is quite different from the situation in the United States.)\n",
      "This is a classic example of the \"analogical fallacy\" or \"analogical fallacy\" in language. The speaker is trying to make a point by using a similar structure and word choice in the other two sentences, but the speaker is not using the same word or phrase in the other two sentences. This is a common pitfall in language when trying to make a point or make a point, but using similar language or structure in other sentences.\n",
      "The situation in Russia is quite different from the saying in the United States. The situation in Russia is quite different from the situation in the States. The situation in Russia is quite different from the situation in the States.\n",
      "This is\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------[2, 3, 17] th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. The country is facing a number of challenges, including a severe economic crisis, a decline in the birth rate of children, and a rise in the number of children who are being raised by their parents as a result of the economic crisis. This is a situation that is often referred to as a \"demographic crisis\" or a \"demographic crisis.\" The demographic crisis is a situation that is often seen in many countries, including the United States, where the population is aging and the birth rate is declining. The demographic crisis is a situation that is often seen in many countries, including the United States, where the population is aging and the birth rate is declining.\n",
      "\n",
      "## Step 1: Identify the key aspects of the demographic crisis in Russia.\n",
      "The key aspects of the demographic crisis in Russia include a severe economic crisis, a decline in the birth rate of children, and an increase in the number of children being raised by their parents as a result of the economic crisis.\n",
      "\n",
      "## Step 2: Explain\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------[2, 3, 18] th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. The situation in Russia is a complex one, with many different factors at play. Here are some key points to consider:\n",
      "\n",
      "1.  **Economic challenges**: Russia is facing significant economic challenges, including a decline in oil production, a large budget deficit, and a decline in economic growth. The country is also struggling to pay its debt, which has led to a decline in the value of the ruble.\n",
      "2.  **International relations**: Russia's international relations are also complex. The country has been involved in a number of conflicts, including the Ukraine conflict, and has also been involved in the Syrian Civil War. Russia has also had a tense relationship with NATO, particularly with the US, which has led to a number of sanctions and countermeasures.\n",
      "3.  **Sanctions and countermeasures**: Russia has faced significant sanctions and countermeasures, including the imposition of sanctions on its diplomats in the US and other countries. The country has also been the target of several counterme\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------[2, 3, 19] th layer dropped------------------------------------------------\n",
      "Hello, tell me about the situation in Russia this year. The Russian economy is in a state of flux, with a mix of growth and decline. The country is experiencing a severe economic crisis, with a significant decline in GDP growth, a decline in industrial production, and a significant decrease in investment income.\n",
      "The Russian economy is heavily dependent on oil exports, with a significant portion of the country's GDP being generated from the sale of oil and gas. The decline in oil prices has had a significant impact on the Russian economy, with a decline in oil production and a decrease in government revenue.\n",
      "The Russian economy is also heavily dependent on the state of the oil market, with a significant portion of the country's GDP being generated from the sale of oil and gas. The decline in oil prices has had a significant impact on the Russian economy, with a decline in oil production and a decrease in government revenue.\n",
      "The Russian economy is heavily dependent on the sale of oil and gas, with a significant portion of GDP being generated from this activity. The decline in oil prices\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------[2, 3, 20] th layer dropped------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "good_layers = range(2,26)\n",
    "for i in good_layers:\n",
    "    for j in good_layers:\n",
    "        for k in good_layers:\n",
    "            if i < j and j < k:\n",
    "                print(f'-----------------{[i,j,k]} th layer dropped------------------------------------------------')\n",
    "                model_slimmed = prune_layers(model, layers_to_remove=[i,j,k])\n",
    "                pipe = pipeline(\"text-generation\", model=model_slimmed, tokenizer=tokenizer, device_map=model.device )\n",
    "                prompt = 'Hello, tell me about the situation in Russia this year'\n",
    "                answer = pipe(prompt, max_new_tokens=200, do_sample=False)[0][\"generated_text\"]\n",
    "                print(answer)\n",
    "                print('-------------------------------------------------------------------------\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e39134",
   "metadata": {},
   "source": [
    "## Count parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1059d4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "layers_count = 28\n",
    "\n",
    "# Данные по количеству параметров для каждого типа слоя в одном блоке\n",
    "params_per_type = {\n",
    "    \"q_proj\": 3072*3072,\n",
    "    \"k_proj\": 3072*1024,\n",
    "    \"v_proj\": 3072*1024,\n",
    "    \"o_proj\": 3072*3072,\n",
    "    \"gate_proj\": 3072*8192,\n",
    "    \"up_proj\": 3072*8192,\n",
    "    \"down_proj\": 8192*3072,\n",
    "}\n",
    "\n",
    "# Суммируем параметры по всем 28 слоям\n",
    "params_sum = {k: v * layers_count for k, v in params_per_type.items()}\n",
    "\n",
    "# Добавляем lm_head отдельно\n",
    "params_sum[\"lm_head\"] = 3072*128256\n",
    "\n",
    "# Визуализация\n",
    "names = list(params_sum.keys())\n",
    "values = list(params_sum.values())\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(names, values)\n",
    "plt.ylabel(\"Number of Parameters\")\n",
    "plt.title(\"Total Number of Parameters per Layer Type in Llama Model\")\n",
    "plt.yscale(\"log\")  # Логарифмическая шкала для удобства восприятия больших чисел\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d726dbca",
   "metadata": {},
   "source": [
    "## SVD decomposition of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2890aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_singular_values_energy(module, top_k=100, energy_threshold=0.99):\n",
    "    \"\"\"\n",
    "    Рисует топ-k сингулярных чисел всех линейных слоев в модуле.\n",
    "    Цвет линии меняется после индекса накопления заданной энергии (energy_threshold).\n",
    "    Подкрашивает точки на пороге энергии и в конце линии.\n",
    "    Использует логарифмическую шкалу по оси Y.\n",
    "    \n",
    "    Параметры:\n",
    "    - module: torch.nn.Module с линейными слоями\n",
    "    - top_k: сколько верхних сингулярных чисел отображать (если больше ранга - максимум)\n",
    "    - energy_threshold: доля накопленной энергии (0..1), напр. 0.99\n",
    "    \n",
    "    Отображает график.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_singular_values(weight, name):\n",
    "        print(f\"Singular values computing for {name} ...\")\n",
    "        W = weight.detach().cpu().float()\n",
    "        U, S, Vh = torch.linalg.svd(W, full_matrices=False)\n",
    "        print(f\"Done computing singular values for {name}.\")\n",
    "        return S.numpy()\n",
    "\n",
    "    def find_linear_layers(mod):\n",
    "        layers = []\n",
    "        for name, child in mod.named_children():\n",
    "            if isinstance(child, torch.nn.Linear):\n",
    "                layers.append((name, child))\n",
    "            else:\n",
    "                layers.extend([(f\"{name}.{n}\", c) for n, c in find_linear_layers(child)])\n",
    "        return layers\n",
    "\n",
    "    linear_layers = find_linear_layers(module)\n",
    "    if not linear_layers:\n",
    "        print(\"Linear layers not found in the given module.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    colors = plt.cm.get_cmap('tab20', len(linear_layers))\n",
    "\n",
    "    for idx, (name, layer) in enumerate(linear_layers):\n",
    "        try:\n",
    "            S_full = get_singular_values(layer.weight, name)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Skipping layer {name} due to error in SVD: {e}\")\n",
    "            continue\n",
    "\n",
    "        S_full_norm = S_full / S_full.sum()\n",
    "        cumulative_full = np.cumsum(S_full_norm)\n",
    "        k_energy = np.searchsorted(cumulative_full, energy_threshold) + 1\n",
    "\n",
    "        max_rank = len(S_full)\n",
    "        visual_top_k = min(top_k, max_rank)\n",
    "\n",
    "        S = S_full[:visual_top_k]\n",
    "        S_norm = S / S.sum()\n",
    "\n",
    "        x = np.arange(1, visual_top_k + 1)\n",
    "        color_main = colors(idx % 20)\n",
    "        color_faded = (0.8, 0.8, 0.8, 0.5)  # светло-серый с прозрачностью\n",
    "\n",
    "        if k_energy < visual_top_k:\n",
    "            plt.plot(x[:k_energy], S_norm[:k_energy], color=color_main, linewidth=2)\n",
    "            plt.plot(x[k_energy-1:], S_norm[k_energy-1:], color=color_faded, linewidth=2)\n",
    "            plt.scatter(k_energy, S_norm[k_energy-1], color=color_main, s=60, zorder=5, edgecolors='black', linewidth=0.8)\n",
    "        elif k_energy == visual_top_k:\n",
    "            plt.plot(x, S_norm, color=color_main, linewidth=2)\n",
    "            plt.scatter(visual_top_k, S_norm[-1], color=color_main, s=60, zorder=6, marker='o', edgecolors='black', linewidth=0.8)\n",
    "        else:\n",
    "            plt.plot(x, S_norm, color=color_main, linewidth=2)\n",
    "            plt.scatter(visual_top_k, S_norm[-1], color=color_main, s=60, zorder=6, marker='o', edgecolors='black', linewidth=0.8)\n",
    "\n",
    "        # Подписи рядом с точками\n",
    "        if k_energy <= visual_top_k:\n",
    "            plt.text(k_energy + 1, S_norm[k_energy-1], f\"{name}\\nk={k_energy}\", fontsize=8, color=color_main)\n",
    "        plt.text(visual_top_k + 1, S_norm[-1], f\"end {name}\", fontsize=7, color=color_main)\n",
    "\n",
    "    plt.xlabel(\"Index of singular value\")\n",
    "    plt.ylabel(\"Normalized singular value magnitude\")\n",
    "    plt.yscale('log')\n",
    "    plt.title(f\"Top singular values of linear layers\\nColor changes at {int(energy_threshold*100)}% energy accumulation index (full SVD)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f981b9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaDecoderLayer(\n",
       "  (self_attn): LlamaAttention(\n",
       "    (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "    (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "    (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "  )\n",
       "  (mlp): LlamaMLP(\n",
       "    (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "    (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "    (act_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc6fb4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular values computing for self_attn.q_proj ...\n",
      "Done computing singular values for self_attn.q_proj.\n",
      "Singular values computing for self_attn.k_proj ...\n",
      "Done computing singular values for self_attn.k_proj.\n",
      "Singular values computing for self_attn.v_proj ...\n",
      "Done computing singular values for self_attn.v_proj.\n",
      "Singular values computing for self_attn.o_proj ...\n",
      "Done computing singular values for self_attn.o_proj.\n",
      "Singular values computing for mlp.gate_proj ...\n",
      "Done computing singular values for mlp.gate_proj.\n",
      "Singular values computing for mlp.up_proj ...\n",
      "Done computing singular values for mlp.up_proj.\n",
      "Singular values computing for mlp.down_proj ...\n",
      "Done computing singular values for mlp.down_proj.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABWYAAAMWCAYAAAByQl98AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3Xd4FNX+BvB3trf0npAOAUJvUhRpCoKi2CsCoqLgVRTrTwWxoehVVKLoVUARBSygggJSRWkB6QFCQkKA9F43287vj002WdIhJBDez/Psk90zZ2bO7Myu976c/Y4khBAgIiIiIiIiIiIiohYja+0BEBEREREREREREV1pGMwSERERERERERERtTAGs0REREREREREREQtjMEsERERERERERERUQtjMEtERERERERERETUwhjMEhEREREREREREbUwBrNERERERERERERELYzBLBEREREREREREVELYzBLRERERERERERE1MIYzBIREdFlQ5KkRj22bNnS2kPFli1bLpmxVJIkCa+99lprD+O8DB06FEOHDm3tYVywffv2YciQIXBzc4MkSZg3b16dfc89X5fiNdUUixcvhiRJSE5Obu2hEBEREV0SFK09ACIiIqLG2rFjh9PrN954A5s3b8amTZuc2qOjo1tyWLXq3bs3duzYcUmMhS4dDz30EEpKSrBs2TJ4eHggLCys0evymiIiIiJqWxjMEhER0WVjwIABTq99fHwgk8lqtF8KXF1dL8lxNRer1QqLxQK1Wt3aQ7msHD58GI888ghGjx7d5HUvh2uqtLQUOp2utYdxXsrKyqDRaCBJUmsPhYiIiK4QLGVAREREbUpubi6mTp2KoKAgqFQqRERE4OWXX0Z5eblTP0mS8MQTT+Dzzz9HVFQU1Go1oqOjsWzZskbt57PPPkOPHj1gMBjg4uKCTp064f/+7/8cy2v72fnEiRNhMBiQkJCAMWPGwGAwIDg4GDNmzKgxvjNnzuCOO+6Ai4sL3N3dcf/99yM2NhaSJGHx4sWOfnX9xH/ixIkNzsbMysrC1KlTER0dDYPBAF9fXwwfPhzbtm1z6pecnAxJkjB37ly8+eabCA8Ph1qtxubNm2vdbq9evTB48OAa7VarFUFBQbjtttscbbNnz0b//v3h6ekJV1dX9O7dG1999RWEEPWOva6f9VeOtfp7BAB79uzBzTffDE9PT2g0GvTq1QsrVqxw6lNaWopnn30W4eHh0Gg08PT0RN++ffH999/XOxbAHrjecsst8PDwgEajQc+ePfH11187llf+jN9iseCzzz5zlN1oigu9pkwmE95880106tQJarUaPj4+mDRpErKyspz6LV++HCNHjkRAQAC0Wi06d+6MF198ESUlJU79Kvd96NAhjBw5Ei4uLhgxYkSTjunPP//ELbfcgnbt2kGj0aB9+/aYMmUKsrOzHX22bdsGSZJqPQ/ffPMNJElCbGyso60x57ryfKxfvx4PPfQQfHx8oNPpUF5ejqysLDz66KMIDg52vE9XX301NmzY0KRjIyIiImoIZ8wSERFRm2E0GjFs2DAkJiZi9uzZ6N69O7Zt24Y5c+Zg//79WLNmjVP/X3/9FZs3b8brr78OvV6PTz/9FPfeey8UCgXuuOOOOvezbNkyTJ06Ff/5z3/w/vvvQyaTISEhAXFxcQ2O0Ww24+abb8bkyZMxY8YM/PXXX3jjjTfg5uaGmTNnAgBKSkowbNgw5Obm4t1330X79u2xdu1a3H333Rf2Bp0jNzcXADBr1iz4+/ujuLgYK1euxNChQ7Fx48Yage/HH3+MqKgovP/++3B1dUWHDh1q3e6kSZPw1FNP4cSJE0591q9fj9TUVEyaNMnRlpycjClTpiAkJAQAsHPnTvznP//B2bNnHe/Hhdq8eTNuuOEG9O/fHwsWLICbmxuWLVuGu+++G6WlpZg4cSIA4JlnnsGSJUvw5ptvolevXigpKcHhw4eRk5NT7/aPHz+OQYMGwdfXFx9//DG8vLzw7bffYuLEicjIyMDzzz+PG2+8ETt27MDAgQNxxx13YMaMGc1ybEDjrimbzYZbbrkF27Ztw/PPP49Bgwbh1KlTmDVrFoYOHYo9e/ZAq9UCAE6cOIExY8Zg+vTp0Ov1OHbsGN59913s3r27RtkQk8mEm2++GVOmTMGLL74Ii8XSpLEnJiZi4MCBePjhh+Hm5obk5GR88MEHuOaaa3Do0CEolUoMHjwYvXr1QkxMDO69916n9efPn49+/fqhX79+ABp/ris99NBDuPHGG7FkyRKUlJRAqVRi/Pjx+Pfff/HWW28hKioK+fn5+Pfffxu8DoiIiIiaTBARERFdpiZMmCD0er3j9YIFCwQAsWLFCqd+7777rgAg1q9f72gDILRarUhPT3e0WSwW0alTJ9G+fft69/vEE08Id3f3evts3rxZABCbN292Gm9t4xszZozo2LGj43VMTIwAIP744w+nflOmTBEAxKJFixxtQ4YMEUOGDKmx/wkTJojQ0FCnNgBi1qxZdY7ZYrEIs9ksRowYIW699VZHe1JSkgAgIiMjhclkqvugK2RnZwuVSiX+7//+z6n9rrvuEn5+fsJsNte6ntVqFWazWbz++uvCy8tL2Gy2Oo+ztve3+lirv0edOnUSvXr1qrHfm266SQQEBAir1SqEEKJr165i3LhxDR7fue655x6hVqtFSkqKU/vo0aOFTqcT+fn5jjYAYtq0aY3a7rnn60Kuqe+//14AED/99JNTv9jYWAFAfPrpp7WOwWazCbPZLLZu3SoAiAMHDtTY98KFCxt1PIsWLRIARFJSUr37OnXqlAAgfvnllxrr7tu3z9G2e/duAUB8/fXXjrbGnuvK7T344IM1xmEwGMT06dMbdUxEREREF4KlDIiIiKjN2LRpE/R6fY3ZrpWz5DZu3OjUPmLECPj5+Tley+Vy3H333UhISMCZM2fq3M9VV12F/Px83Hvvvfjll1+cfnbdEEmSMHbsWKe27t2749SpU47XW7duhYuLC2644QanfufOFmwOCxYsQO/evaHRaKBQKKBUKrFx40YcPXq0Rt+bb74ZSqWywW16eXlh7Nix+Prrr2Gz2QAAeXl5+OWXX/Dggw9Coaj60damTZtw3XXXwc3NDXK5HEqlEjNnzkROTg4yMzMv+PgSEhJw7Ngx3H///QAAi8XieIwZMwZpaWk4fvw4APt5/eOPP/Diiy9iy5YtKCsra9Q+Nm3ahBEjRiA4ONipfeLEiSgtLa1x07rm1phravXq1XB3d8fYsWOd3oOePXvC39/fqTzCyZMncd9998Hf399xToYMGQIAtV4Xt99++3mPPTMzE4899hiCg4Md119oaGiNfd17773w9fVFTEyMo+2TTz6Bj4+PYyZ5U851fWO/6qqrsHjxYrz55pvYuXMnzGbzeR8fERERUX0YzBIREVGbkZOTA39//xq1O319faFQKGr8FNnf37/GNirb6vvZ8vjx47Fw4UKcOnUKt99+O3x9fdG/f3/8+eefDY5Rp9NBo9E4tanVahiNRqfjqB4YV6qt7UJ88MEHePzxx9G/f3/89NNP2LlzJ2JjY3HDDTfUGkoGBAQ0etsPPfQQzp4963hPvv/+e5SXlzv9lHz37t0YOXIkAOB///sf/vnnH8TGxuLll18GgEYHo/XJyMgAADz77LNQKpVOj6lTpwKAI1j/+OOP8cILL2DVqlUYNmwYPD09MW7cOJw4caLefeTk5NT63gQGBjqWX0yNuaYyMjKQn58PlUpV431IT093vAfFxcUYPHgwdu3ahTfffBNbtmxBbGwsfv75ZwA1z4lOp4Orq+t5jdtms2HkyJH4+eef8fzzz2Pjxo3YvXs3du7cWWNfarUaU6ZMwXfffYf8/HxkZWVhxYoVePjhhx03oGvKua5U23lbvnw5JkyYgC+//BIDBw6Ep6cnHnzwQaSnp5/XcRIRERHVhTVmiYiIqM3w8vLCrl27IIRwCmczMzNhsVjg7e3t1L+2oKWyzcvLq959TZo0CZMmTUJJSQn++usvzJo1CzfddBPi4+MdM/4u5Dh2795d59iq02g0KCgoqNHemFm83377LYYOHYrPPvvMqb2oqKjW/k25WdWoUaMQGBiIRYsWYdSoUVi0aBH69++P6OhoR59ly5ZBqVRi9erVTsHiqlWrGtx+Zf9zb3B17nFXnvOXXnrJ6aZj1XXs2BEAoNfrMXv2bMyePRsZGRmO2bNjx47FsWPH6hyLl5cX0tLSarSnpqY6jaE1eXt7w8vLC2vXrq11uYuLCwD77N/U1FRs2bLFMUsWAPLz82tdr6k3MKvu8OHDOHDgABYvXowJEyY42hMSEmrt//jjj+Odd97BwoULYTQaYbFY8NhjjzmWN+Vc1zd+b29vzJs3D/PmzUNKSgp+/fVXvPjii8jMzKzz/SMiIiI6HwxmiYiIqM0YMWIEVqxYgVWrVuHWW291tH/zzTeO5dVt3LgRGRkZjpmoVqsVy5cvR2RkJNq1a9eofer1eowePRomkwnjxo3DkSNHLjiYHTJkCFasWIE//vgDo0ePdrQvW7asRt+wsDD88MMPKC8vd8wczMnJwfbt2xucyShJkmOdSgcPHsSOHTtq/Cy/qeRyOcaPH4958+Zh27Zt2LNnDz7//PMa+1coFJDL5Y62srIyLFmypMHth4WFOcY7atQoR/uvv/7q1K9jx47o0KEDDhw4gLfffrvR4/fz88PEiRNx4MABzJs3D6WlpdDpdLX2HTFiBFauXInU1FTHLFnAft3pdDoMGDCg0fu9WG666SYsW7YMVqsV/fv3r7NfZVB57nVx7rlrDk3dV0BAAO688058+umnMJlMGDt2rOOmccD5n+v6hISE4IknnsDGjRvxzz//NMs2iYiIiCoxmCUiIqI248EHH0RMTAwmTJiA5ORkdOvWDX///TfefvttjBkzBtddd51Tf29vbwwfPhyvvvoq9Ho9Pv30Uxw7dqzWALS6Rx55BFqtFldffTUCAgKQnp6OOXPmwM3NzXF3+AsxYcIEfPjhh3jggQfw5ptvon379vjjjz+wbt06AIBMVlWNavz48fj888/xwAMP4JFHHkFOTg7mzp3bqJ+X33TTTXjjjTcwa9YsDBkyBMePH8frr7+O8PBwWCyWCz6Ohx56CO+++y7uu+8+aLVaRy3QSjfeeCM++OAD3HfffXj00UeRk5OD999/v0ZQVxt/f39cd911mDNnDjw8PBAaGoqNGzc6fnJf3eeff47Ro0dj1KhRmDhxIoKCgpCbm4ujR4/i33//xQ8//AAA6N+/P2666SZ0794dHh4eOHr0KJYsWYKBAwfWGcoCwKxZs7B69WoMGzYMM2fOhKenJ5YuXYo1a9Zg7ty5cHNza+I71/zuueceLF26FGPGjMFTTz2Fq666CkqlEmfOnMHmzZtxyy234NZbb8WgQYPg4eGBxx57DLNmzYJSqcTSpUtx4MCBZh9Tp06dEBkZiRdffBFCCHh6euK3336rtyTIU0895QiWFy1aVGN5Y891XQoKCjBs2DDcd9996NSpE1xcXBAbG4u1a9fWOQuXiIiI6HwxmCUiIqI2Q6PRYPPmzXj55Zfx3nvvISsrC0FBQXj22Wcxa9asGv1vvvlmdOnSBa+88gpSUlIQGRmJpUuX1ggQzzV48GAsXrwYK1asQF5eHry9vXHNNdfgm2++gY+PzwUfh16vx6ZNmzB9+nQ8//zzkCQJI0eOxKeffooxY8bA3d3d0ffqq6/G119/jXfeeQe33HILIiIiMGvWLPz+++9ON3Sqzcsvv4zS0lJ89dVXmDt3LqKjo7FgwQKsXLmywXUbIyoqCoMGDcL27dtx//331wgohw8fjoULF+Ldd9/F2LFjERQUhEceeQS+vr6YPHlyg9tfsmQJ/vOf/+CFF16A1WrF2LFj8f3336Nv375O/YYNG4bdu3fjrbfewvTp05GXlwcvLy9ER0fjrrvuchrPr7/+ig8//BClpaUICgrCgw8+6Kh5W5eOHTti+/bt+L//+z9MmzYNZWVl6Ny5MxYtWuRUU7c1yeVy/Prrr/joo4+wZMkSzJkzBwqFAu3atcOQIUPQrVs3APayDGvWrMGMGTPwwAMPQK/X45ZbbsHy5cvRu3fvZh2TUqnEb7/9hqeeegpTpkyBQqHAddddhw0bNjjNhK3uqquuQlhYGLRabY0Z8EDjz3VdNBoN+vfvjyVLliA5ORlmsxkhISF44YUX8Pzzz1/wMRMRERFVJwkhRGsPgoiIiKilSZKEadOmYf78+a09lEZ7++23HSFyY0stELUlBw8eRI8ePRATE+O4oRcRERHR5YozZomIiIguQZWBcadOnWA2m7Fp0yZ8/PHHeOCBBxjK0hUnMTERp06dwv/93/8hICDgkpmJTERERHQhGMwSERERXYJ0Oh0+/PBDJCcno7y83PFz6ldeeaW1h0bU4t544w0sWbIEnTt3xg8//FBvzV8iIiKiywVLGRARERERERERERG1MFnDXYiIiIiIiIiIiIioOTGYJSIiqsXBgwcxadIkhIeHQ6PRwGAwoHfv3pg7dy5yc3ObvL2JEyciLCys+Qd6niZOnAiDwdDaw7hsxMXF4bXXXkNycnKj19m3bx/GjRuHwMBA6HQ6dOrUCa+//jpKS0tr9P33339x3XXXwWAwwN3dHbfddhtOnjzp1MdoNGLatGnw8fFBu3bt8Prrr+PcHz6dOnUKBoMBGzduPK/jpCtPcnIyJEnC4sWLm7xuamoqXnvtNezfv7/Gstdeew2SJF34AM9DWFhYs9egHTp0KIYOHdqs22ysxMREqNVq7Nixw6n9lVdeQUhICBQKBdzd3Zu0zS1btkCSJGzZssXR1pRztm7dOowcORKBgYFQq9UIDAzE0KFD8c477wAADhw4AEmS8OKLL9a5jRMnTkCSJDz55JNO+6986HQ6tGvXDqNGjcInn3yCoqKiGtsYP348xo0b1/gDJyIiusQwmCUiIjrH//73P/Tp0wexsbF47rnnsHbtWqxcuRJ33nknFixYgMmTJ7f2EKmFxcXFYfbs2Y0OZuPi4jBo0CAkJydj3rx5WL16Ne655x68/vrruPfee536Hjt2DEOHDoXJZMKKFSuwcOFCxMfHY/DgwcjKynL0mzt3Ln7++Wd89tlneP311/Huu+9i6dKlTtt6/PHHcfvtt2PEiBEXfMxEDUlNTcXs2bNrDWYffvjhGkFiS1m5ciVeffXVVtn3xfDss8/i+uuvx8CBAx1tv/zyC9566y08+OCD2Lp1KzZs2NBi41mwYAFuuOEGuLq6Yv78+Vi3bh3effdddO7cGT/++CMAoEePHujTpw+++eYbWK3WWrezaNEiAKjx39S1a9dix44dWLt2Ld5//32EhITg+eefR5cuXXDgwAGnvq+99hrWrFmDTZs2XYQjJSIiuvh48y8iIqJqduzYgccffxzXX389Vq1aBbVa7Vh2/fXXY8aMGVi7dm0rjtCurKwMWq22tYdBdfjuu+9gNBrx008/ITIyEgAwfPhwpKWl4YsvvkBeXh48PDwAADNnzoRarcbq1avh6uoKAOjTpw86dOiA999/H++++y4AYM2aNXjyySdxxx13AAB27tyJ1atX44EHHgAALFu2DLt378axY8da+nCbjdlshiRJUCj4P1Evd+3atUO7du1aZd+9evVqlf1eDEePHsWqVatq/Hfn8OHDAIAnn3wSvr6+LTqmOXPm4Nprr3WEsJXGjx8Pm83meD158mRMnToVf/zxB2666SanvlarFd988w369OmDHj16OC3r06cPvL29Ha/vuecePPHEExgyZAhuvvlmxMfHO/7bHBkZiRtuuAHvvPMOhg8f3tyHSkREdNFxxiwREVE1b7/9NiRJwhdffOEUylZSqVS4+eabHa9tNhvmzp2LTp06Qa1Ww9fXFw8++CDOnDnT4L6MRiNeeuklhIeHQ6VSISgoCNOmTUN+fr5Tv7CwMNx00034+eef0atXL2g0GsyePbveba9duxYjRoyAm5sbdDodOnfujDlz5tTol5CQgDFjxsBgMCA4OBgzZsxAeXm5U5/Zs2ejf//+8PT0hKurK3r37o2vvvqqxs/oK8e5du1a9O7dG1qtFp06dcLChQtr7Pfvv//GwIEDodFoEBQUhFdffRVffvklJEmqMSt1+fLlGDhwIPR6PQwGA0aNGoV9+/Y59Tl58iTuuecex89q/fz8MGLEiFpn8lW3Z88e3HPPPQgLC4NWq0VYWBjuvfdenDp1ytFn8eLFuPPOOwEAw4YNc/zMtr6ffiuVSgCAm5ubU7u7uztkMhlUKhUAwGKxYPXq1bj99tsdoSwAhIaGYtiwYVi5cqWjzWg0Qq/XO14bDAYYjUYAQH5+PqZPn44PPvjAKdBojMLCQjz77LNO1+H06dNRUlLi1E+SJDzxxBNYsmQJOnfuDJ1Ohx49emD16tU1tnnixAncd9998PX1hVqtRufOnRETE+PUp/Kn1EuWLMGMGTMQFBQEtVqNhIQEAPaZ61FRUVCr1YiOjsZ3333nVBJECIEOHTpg1KhRNfZfXFwMNzc3TJs2rd5jj4mJwbXXXgtfX1/o9Xp069YNc+fOhdlsrtG3MZ+pXbt2YezYsfDy8oJGo0FkZCSmT5/uWF5XSZPafkJe+X4vWrQIHTt2hFarRd++fbFz504IIfDee+8hPDwcBoMBw4cPd7xvler6OX9jfpKfkJCASZMmoUOHDtDpdAgKCsLYsWNx6NAhR58tW7agX79+AIBJkyY5PhevvfZancfU2O/LoUOHomvXroiNjcXgwYOh0+kQERGBd955xyn4q8u5x155rX3//fd4+eWXERgYCFdXV1x33XU4fvy407pCCMydOxehoaHQaDTo3bs3/vjjj1r305jPzmOPPQaNRoO9e/c6vQ8jRoyAn58f0tLS6j2Wzz77DP7+/rj++uudju+VV14BAPj5+Tm979Wf1/eeXIicnBwEBATUukwmq/q/l/fddx+0Wq1jZmx169evx9mzZ/HQQw81ap89evTAyy+/jJSUFCxfvtxp2fjx47FhwwYkJiY24SiIiIguEYKIiIiEEEJYLBah0+lE//79G73Oo48+KgCIJ554Qqxdu1YsWLBA+Pj4iODgYJGVleXoN2HCBBEaGup4bbPZxKhRo4RCoRCvvvqqWL9+vXj//feFXq8XvXr1Ekaj0dE3NDRUBAQEiIiICLFw4UKxefNmsXv37jrH9OWXXwpJksTQoUPFd999JzZs2CA+/fRTMXXqVKfxqFQq0blzZ/H++++LDRs2iJkzZwpJksTs2bOdtjdx4kTx1VdfiT///FP8+eef4o033hBarbZGv9DQUNGuXTsRHR0tvvnmG7Fu3Tpx5513CgBi69atjn4HDhwQGo1GdO/eXSxbtkz8+uuvYsyYMSIsLEwAEElJSY6+b731lpAkSTz00ENi9erV4ueffxYDBw4Uer1eHDlyxNGvY8eOon379mLJkiVi69at4qeffhIzZswQmzdvrvf8/fDDD2LmzJli5cqVYuvWrWLZsmViyJAhwsfHx3H+MjMzxdtvvy0AiJiYGLFjxw6xY8cOkZmZWed2k5KShLu7u7jjjjtEYmKiKCwsFL/99ptwc3MT//nPfxz9jh075tjuuZ599lkhSZIoKysTQgjx2GOPiR49eojk5GRx+PBhERgYKN59910hhBCPPPKIGD58eL3HWpuSkhLRs2dP4e3tLT744AOxYcMG8dFHHwk3NzcxfPhwYbPZHH0BiLCwMHHVVVeJFStWiN9//10MHTpUKBQKkZiY6Oh35MgR4ebmJrp16ya++eYbsX79ejFjxgwhk8nEa6+95ui3efNmAUAEBQWJO+64Q/z6669i9erVIicnR3z++ecCgLj99tvF6tWrxdKlS0VUVJQIDQ11+hx99NFHQpIkER8f73RcMTExAoDTNVKbp59+Wnz22Wdi7dq1YtOmTeLDDz8U3t7eYtKkSU79GvOZWrt2rVAqlaJ79+5i8eLFYtOmTWLhwoXinnvucfQ593ug0qxZs8S5/7McgAgNDRWDBg0SP//8s1i5cqWIiooSnp6e4umnnxa33HKL473x8/MT3bt3dzpfoaGhYsKECTX2NWTIEDFkyBDH66SkJAFALFq0yNG2detWMWPGDPHjjz+KrVu3ipUrV4px48YJrVYrjh07JoQQoqCgQCxatEgAEK+88orjc3H69Ok6j6mx35dDhgwRXl5eokOHDmLBggXizz//FFOnThUAxNdff13jmM517rFXXmthYWHi/vvvF2vWrBHff/+9CAkJER06dBAWi6XGuZg8ebL4448/xBdffCGCgoKEv7+/0/vW2M9OWVmZ6Nmzp4iIiBB5eXlCCCFmzpwpZDKZWL9+fYPHEhERIe666y6ntn///VdMnjxZABBr1651et8BiFmzZjX6Pan+HVnbOavNddddJxQKhZg1a5bYv3+/0/t3rgceeEAolcoa35d33nmn0Gg0jvek+v6rXwvVVX5fTp482ak9IyNDABAff/xxg2MnIiK61DCYJSIiqpCeni4AOAUp9Tl69KgA4BTOCCHErl27BADxf//3f462cwOZtWvXCgBi7ty5TusuX75cABBffPGFoy00NFTI5XJx/PjxBsdUVFQkXF1dxTXXXOMU0pxrwoQJAoBYsWKFU/uYMWNEx44d61zParUKs9ksXn/9deHl5VUjCNJoNOLUqVOOtrKyMuHp6SmmTJniaLvzzjuFXq93+j/fVqtVREdHOwWzKSkpQqFQOAWZlcfo7+/vCCuys7MFADFv3rx63pnGsVgsori4WOj1evHRRx852n/44YcaIUZDjh49Kjp16iQAOB5PPvmk03v2zz//CADi+++/r7F+ZRicmpoqhLBfn/369XNsa8yYMaK0tFT89ddfQqvV1ggnG2POnDlCJpOJ2NhYp/Yff/xRABC///67ow2A8PPzE4WFhY629PR0IZPJxJw5cxxto0aNEu3atRMFBQVO23ziiSeERqMRubm5QoiqYOjaa6916me1WoW/v3+NfyA5deqUUCqVTp+jwsJC4eLiIp566imnvtHR0WLYsGGNfyNE1bX9zTffCLlc7hhnYz9TkZGRIjIy0hGk16apway/v78oLi52tK1atUoAED179nQay7x58wQAcfDgQUfbhQSz57JYLMJkMokOHTqIp59+2tEeGxtb57rnHlNTvi+HDBkiAIhdu3Y59Y2OjhajRo2qc5yV6gohx4wZ49RvxYoVAoDYsWOHEEKIvLw8odFoxK233urUr/JzWv19a8pn58SJE8LV1VWMGzdObNiwQchkMvHKK680eByVgeM777xTY1ldIWZLBLMJCQmia9euju8irVYrRowYIebPny9MJpNT38r9fPDBB462nJwcoVarxf3339+oY6pUVlYmAIjRo0fXWBYUFCTuvvvuBsdORER0qWEpAyIiovO0efNmAKjx89CrrroKnTt3xsaNG+tct/JGJeeue+edd0Kv19dYt3v37oiKimpwTNu3b0dhYSGmTp3a4N21JUnC2LFja+yn+s/4K8d63XXXwc3NDXK5HEqlEjNnzkROTg4yMzOd+vbs2RMhISGO1xqNBlFRUU7b3Lp1K4YPH+70k3uZTIa77rrLaVvr1q2DxWLBgw8+CIvF4nhoNBoMGTLEcTdxT09PREZG4r333sMHH3yAffv2NernzoD9J+8vvPAC2rdvD4VCAYVCAYPBgJKSEhw9erRR26hNcnKy4yftP/74I7Zu3Yq5c+di8eLFePjhh2v0r+9cVS7z8/PDrl27kJSUhLNnz2LNmjWQy+WYMmUKXnnlFXTo0AE//fQTunTpAk9PT9x00004ffp0veNcvXo1unbtip49ezq9x6NGjapxx3bAXsrBxcXF8drPzw++vr6O82s0GrFx40bceuut0Ol0TtscM2YMjEYjdu7c6bTN22+/3en18ePHkZ6eXuN6CAkJwdVXX+3U5uLigkmTJmHx4sWOn49v2rQJcXFxeOKJJ+o9dgDYt28fbr75Znh5eTmu7QcffBBWqxXx8fEAGveZio+PR2JiIiZPngyNRtPgfhtr2LBhTuUrOnfuDAAYPXq001gq28/97J4vi8WCt99+G9HR0VCpVFAoFFCpVDhx4sR5fy6a+n3p7++Pq666yqmttu+npqhehqZye0DV+7Zjxw4YjUbcf//9Tv0GDRqE0NBQp7amfHbat2+P//3vf1i1ahVuuukmDB48uNZyA+dKTU0FgBavIduQyMhIHDhwAFu3bsXs2bNx3XXXITY2Fk888QQGDhzoKLECAEOGDEFkZKRTOYOlS5eivLy80WUMKolzyudU5+vri7Nnzzb9YIiIiFoZg1kiIqIK3t7e0Ol0SEpKalT/nJwcAKi11l5gYKBjeV3rKhQK+Pj4OLVLkgR/f/8a69ZVz+9cWVlZANCom+7odLoaIZJarXb6P9W7d+/GyJEjAdhrfv7zzz+IjY3Fyy+/DMB+E7LqvLy8auxHrVY79cvJyYGfn1+Nfue2ZWRkAAD69esHpVLp9Fi+fDmys7MB2N+zjRs3YtSoUZg7dy569+4NHx8fPPnkkygqKqr3Pbjvvvswf/58PPzww1i3bh12796N2NhY+Pj41Di2pnjxxRdRWFiIdevW4fbbb8e1116L5557DvPmzcPChQuxdetWAFXvV23XSm5uLiRJgru7u6NNkiSEhYUhMDAQAPDOO+9AJpPhueeew7Fjx3D//ffjv//9L86cOQNvb2/HjcHqkpGRgYMHD9Z4f11cXCCEcLzHlRo6vzk5ObBYLPjkk09qbHPMmDEAUGOb517ble9FY64RAPjPf/6DoqIiLF26FAAwf/58tGvXDrfccku9x56SkoLBgwfj7Nmz+Oijj7Bt2zbExsY6auFWHlNjPlNN+dw1haenp9PrytrEdbVX/+xeiGeeeQavvvoqxo0bh99++w27du1CbGwsevTocd6fi6Z+Xzbmu6Spzt1mZR3x6tcvYA+Fz3VuW1M/OzfeeCP8/PxgNBrxzDPPQC6XNzjeynE1Z9jfXGQyGa699lrMnDkTv/76K1JTU3H33Xdj7969TnXFJUnCQw89hEOHDmHPnj0AgEWLFiE8PBzDhg1r0j4rA/TK77/qNBrNBV0bRERErYW3vCUiIqogl8sxYsQI/PHHHzhz5kyDIUvl/8lPS0ur0Tc1NbXemzB5eXnBYrEgKyvLKZwVQiA9Pd1xU51KDc1+rVS5rcbcfKwxli1bBqVSidWrVzuFA6tWrTrvbXp5eTlC1+rS09OdXle+fz/++GON2WrnCg0NxVdffQXAPntxxYoVeO2112AymbBgwYJa1ykoKMDq1asxa9YsvPjii4728vJy5ObmNumYzrV//35ER0c7zXYE4Divhw8fdswk02q1TjdVqnTo0CG0b9++zlDm+PHjeOedd7BhwwYolUps2LABXbp0wQ033ADAHq716NEDxcXFMBgMtW7D29sbWq221hu0VS5vCg8PD8jlcowfP77OG2+Fh4c7vT732q78XDXmGgHssxFHjx6NmJgYjB49Gr/++itmz57dYPC1atUqlJSU4Oeff3a6vs69YVxjPlON/dxpNJoaN9cDaobVzaG+fTV0Xr/99ls8+OCDePvtt2usW/0fCpriQr4vW0rlGGu7ztLT051u3NbUz85jjz2GoqIidOnSBU8++SQGDx4MDw+PesdTuY2mfB+p1epaz3t9/1DYHPR6PV566SUsX74chw8fdlo2ceJEzJw5EwsXLoRSqcS+ffvwxhtvNPq/a5V+/fVXAKj15nW5ubm13liPiIjoUscZs0RERNW89NJLEELgkUcegclkqrHcbDbjt99+AwAMHz4cgD3EqC42NhZHjx7FiBEj6txP5bJz1/3pp59QUlJS77r1GTRoENzc3LBgwYJ6f/bZWJIkQaFQOIVcZWVlWLJkyXlvc8iQIdi0aZNTGGWz2fDDDz849Rs1ahQUCgUSExPRt2/fWh+1iYqKwiuvvIJu3brh33//rffYhBCOWXOVvvzyS1itVqe2c2fWNSQwMBBHjhxBcXGxU/uOHTsAVM2sVCgUGDt2LH7++Wen2b0pKSnYvHkzbrvttjr3MWXKFEycOBGDBg0CYA/1q98NvnLf9V0HN910ExITE+Hl5VXr+9vUoEOn02HYsGHYt28funfvXus2a5sJWV3Hjh3h7++PFStWOLWnpKRg+/btta7z1FNP4eDBg5gwYQLkcjkeeeSRBsdaGQpVP/9CCPzvf/9z6teYz1RUVBQiIyOxcOHCWkOxSmFhYcjMzHQKnU0mE9atW9fgeJsqLCwMBw8edGqLj4/H8ePHG1xXkqQan4s1a9bU+Kl4Uz4XF/J92VIGDBgAjUbjmH1dafv27TVKKDTls/Pll1/i22+/xfz58/Hrr78iPz8fkyZNanA8oaGh0Gq1SExMbPQx1HbeN23aVOO76EKkpaXV2l5Z5uLcGa2BgYG44YYb8P333yMmJgYymQwTJkxo0j4PHDiAt99+G2FhYTXKnFgsFpw+fRrR0dFN2iYREdGlgDNmiYiIqhk4cCA+++wzTJ06FX369MHjjz+OLl26wGw2Y9++ffjiiy/QtWtXjB07Fh07dsSjjz6KTz75BDKZDKNHj0ZycjJeffVVBAcH4+mnn65zP9dffz1GjRqFF154AYWFhbj66qtx8OBBzJo1C7169cL48ePPa/wGgwH//e9/8fDDD+O6667DI488Aj8/PyQkJODAgQOYP39+k7Z344034oMPPsB9992HRx99FDk5OXj//fdrhDZN8fLLL+O3337DiBEj8PLLL0Or1WLBggWOUFEms/+7cVhYGF5//XW8/PLLOHnyJG644QZ4eHggIyMDu3fvhl6vx+zZs3Hw4EE88cQTuPPOO9GhQweoVCps2rQJBw8edJoJey5XV1dce+21eO+99+Dt7Y2wsDBs3boVX331VY1ZgV27dgUAfPHFF3BxcYFGo0F4eHidIeP06dMxbtw4XH/99Xj66afh7e2NnTt3Ys6cOYiOjsbo0aMdfWfPno1+/frhpptuwosvvgij0YiZM2fC29sbM2bMqHX7CxcuRHx8PH755RdH24gRI/D0009j5syZGDx4MGbNmoWrr77aqSZsbeP86aefcO211+Lpp59G9+7dYbPZkJKSgvXr12PGjBno379/nevX5qOPPsI111yDwYMH4/HHH0dYWBiKioqQkJCA3377zVFfuS4ymQyzZ8/GlClTcMcdd+Chhx5Cfn4+Zs+ejYCAAMf1Ud3111+P6OhobN68GQ888ECjanJef/31UKlUuPfee/H888/DaDTis88+Q15enlO/xn6mYmJiMHbsWAwYMABPP/00QkJCkJKSgnXr1jmCvrvvvhszZ87EPffcg+eeew5GoxEff/xxjX8IaA7jx4/HAw88gKlTp+L222/HqVOnMHfu3BrlU2pz0003YfHixejUqRO6d++OvXv34r333qsx07VyxvfSpUvRuXNnGAwGBAYG1vpT8wv5vmwpHh4eePbZZ/Hmm2/i4Ycfxp133onTp0/jtddeq1HKoLGfnUOHDuHJJ5/EhAkTHGHsV199hTvuuAPz5s3D9OnT6xyPSqXCwIEDa9Rlrs/48ePx6quvYubMmRgyZAji4uIwf/58uLm5ndd7UpsuXbpgxIgRGD16NCIjI2E0GrFr1y7897//hZ+fHyZPnlxjncmTJ2PNmjX48ssvMWrUKAQHB9e5/b1798LNzQ1msxmpqanYuHEjlixZAl9fX/z222+O0h2VDh48iNLS0iaXRiAiIroktNJNx4iIiC5p+/fvFxMmTBAhISFCpVIJvV4vevXqJWbOnCkyMzMd/axWq3j33XdFVFSUUCqVwtvbWzzwwAPi9OnTTtur7W7sZWVl4oUXXhChoaFCqVSKgIAA8fjjj4u8vDynfqGhoeLGG29s0vh///13MWTIEKHX64VOpxPR0dHi3XffdRqPXq+vsV5td+VeuHCh6Nixo1Cr1SIiIkLMmTNHfPXVVwKASEpKanCc594FXgghtm3bJvr37y/UarXw9/cXzz33nHj33XcFAJGfn+/Ud9WqVWLYsGHC1dVVqNVqERoaKu644w6xYcMGIYT9zuUTJ04UnTp1Enq9XhgMBtG9e3fx4YcfCovFUu/7dObMGXH77bcLDw8P4eLiIm644QZx+PDhWu9oP2/ePBEeHi7kcnmDd7EXQohNmzaJkSNHCn9/f6HVakVUVJSYMWOGyM7OrtF3z549YsSIEUKn0znu3p6QkFDrdjMzM4Wnp6f44YcfaixbunSp6NChgzAYDOL6668XJ0+erHeMQghRXFwsXnnlFdGxY0ehUqmEm5ub6Natm3j66adFenq6ox8AMW3atBrr1/ZeJSUliYceekgEBQUJpVIpfHx8xKBBg8Sbb77p6FN5t/bajkMIIb744gvRvn17oVKpRFRUlFi4cKG45ZZbRK9evWrt/9prrwkAYufOnQ0ec6XffvtN9OjRQ2g0GhEUFCSee+458ccff9S4W70QDX+mhBBix44dYvTo0cLNzU2o1WoRGRkpnn766Rrb6dmzp9BqtSIiIkLMnz+/1s9dbe93UlKSACDee+89p/ba3kubzSbmzp0rIiIihEajEX379hWbNm2q8Xms3Gb16zkvL09MnjxZ+Pr6Cp1OJ6655hqxbdu2Wj/L33//vejUqZNQKpUCgJg1a5YQovbvksZ+Xw4ZMkR06dJFnKu279HanHtN1nWt1XbsNptNzJkzRwQHBwuVSiW6d+8ufvvtt1qPvaHPTnFxsejUqZOIjo4WJSUlTutOmzZNKJVKsWvXrnqP5auvvhJyuVykpqY6tVe+v1lZWU7t5eXl4vnnnxfBwcFCq9WKIUOGiP3799f5nlS/zms7Z7X5/PPPxW233SYiIiKETqcTKpVKREZGiscee6zGuaxkMpmEn5+fACBWrFhRa5/K/Vc+1Gq1CAgIECNHjhQfffSRKCwsrHW9V199VXh7ewuj0djg2ImIiC41khDN8DtHIiIiogs0cuRIJCcnIz4+vrWHQpeg/Px8REVFYdy4cfjiiy9qLO/bty8kSUJsbGwrjI7o4jAajQgJCcGMGTPwwgsvtPZwLjlWqxXt27fHfffdh7feequ1h0NERNRkLGVARERELe6ZZ55Br169EBwcjNzcXCxduhR//vmn4wZedGVLT0/HW2+9hWHDhsHLywunTp3Chx9+iKKiIjz11FOOfoWFhTh8+DBWr16NvXv3YuXKla04aqLmp9FoMHv2bLz22mt44oknatxQ8Er37bffori4GM8991xrD4WIiOi8MJglIiKiFme1WjFz5kykp6dDkiRER0djyZIleOCBB1p7aHQJUKvVSE5OxtSpU5GbmwudTocBAwZgwYIF6NKli6Pfv//+6whvZ82ahXHjxrXeoIkukkcffRT5+fk4efIkunXr1trDuaTYbDYsXbq0Rl1wIiKiywVLGRARERERERERERG1sJq3tSUiIiIiIiIiIiKii4rBLBEREREREREREVELYzBLRERERERERERE1MIYzBIRERERERERERG1MEVrD+BSZ7PZkJqaChcXF0iS1NrDISIiIiIiIiIiuqIJIVBUVITAwEDIZJfvvFMGsw1ITU1FcHBwaw+DiIiIiIiIiIiIqjl9+jTatWvX2sM4bwxmG+Di4gLAfqJdXV1beTRERERERERERERXtsLCQgQHBztyu8sVg9k6xMTEICYmBlarFQDg6urKYJaIiIiIiIiIiOgScbmXHZWEEKK1B3EpKywshJubGwoKChjMEhERERERERERtbK2ktddvtVxiYiIiIiIiIiIiC5TDGaJiIiIiIiIiIiIWhiDWSIiIiIiIiIiIqIWxmCWiIiIiIiIiIiIqIUxmCUiIiIiIiIiIiJqYQxmiYiIiIiIiIiIiFoYg1kiIiIiIiIiIiKiFsZgloiIiIiIiIiIiKiFMZglIiIiIiIiIiIiamEMZusQExOD6Oho9OvXr7WHQkRERERERERERG2MJIQQrT2IS1lhYSHc3NxQUFAAV1fX1h4OERERERERERHRFa2t5HWcMUtERERERERERETUwhjMEhEREREREREREbUwBrNERERERERERERELYzBLBEREREREREREVELYzBLRERERERERERE1MIYzBIRERERERERERG1MAazRERERERERERERC2MwSwRERERERERERFRC2MwS0RERERERERERNTCGMwSERERERERERERtTAGs0REREREREREREQtjMEsERERERERERERUQtjMEtERERERERERETUwhjMEhEREREREREREbUwRWsP4FIVExODmJgYWK3W1h7KxWUsAApSAGEFXAIBvW9rj4iIiIiIiIiIiKjNk4QQorUHcSkrLCyEm5sbCgoK4Orq2trDaX4Zh4FDS+3P248Gwq5t3fEQERERERERERHVo63kdSxlcKWTql0CwtZ64yAiIiIiIiIiIrqCMJi90smqB7NtvGwDERERERERERHRJYLB7JWOM2aJiIiIiIiIiIhaHIPZK50kr3rOYJaIiIiIiIiIiKhFMJi9wu0/U+h4Hnc2rxVHQkREREREREREdOVgMHuFK7NUPTdbWGOWiIiIiIiIiIioJTCYvcLJZFWlDCQwmCUiIiIiIiIiImoJDGavcDJZ1SUgscYsERERERERERFRi2Awe4WTqs+YZTBLRERERERERETUIhjMXuFk8qpgFgxmiYiIiIiIiIiIWgSD2Suc04xZMJglIiIiIiIiIiJqCQxmr3ByljIgIiIiIiIiIiJqcQxmr3CSnDNmiYiIiIiIiIiIWhqD2SucrNqMWRlnzBIREREREREREbUIBrNXOKebf3HGLBERERERERERUYtgMHuFk8sUjucyBrNEREREREREREQtgsHsFU4ur7oEWMqAiIiIiIiIiIioZTCYvcLJ5JwxS0RERERERERE1NLafDB7+vRpDB06FNHR0ejevTt++OGH1h7SJUUhq7oEJIhWHAkREREREREREdGVQ9Fwl8ubQqHAvHnz0LNnT2RmZqJ3794YM2YM9Hp9aw/tkiCTy2CySVDJBGfMEhERERERERERtZA2H8wGBAQgICAAAODr6wtPT0/k5uYymK2gkEmwCgmAgIwzZomIiIiIiIiIiFpEq5cy+OuvvzB27FgEBgZCkiSsWrWqRp9PP/0U4eHh0Gg06NOnD7Zt23Ze+9qzZw9sNhuCg4MvcNRth1wmwWyTAAAKWFt5NERERERERERERFeGVg9mS0pK0KNHD8yfP7/W5cuXL8f06dPx8ssvY9++fRg8eDBGjx6NlJQUR58+ffqga9euNR6pqamOPjk5OXjwwQfxxRdfXPRjupzIJQklVjkAQC1ZWnk0REREREREREREVwZJCHHJ/H5dkiSsXLkS48aNc7T1798fvXv3xmeffeZo69y5M8aNG4c5c+Y0arvl5eW4/vrr8cgjj2D8+PEN9i0vL3e8LiwsRHBwMAoKCuDq6tq0A7oMFBrNyFg7Bx0MZSizKaAd+UZrD4mIiIiIiIiIiKhOhYWFcHNzu+zzulafMVsfk8mEvXv3YuTIkU7tI0eOxPbt2xu1DSEEJk6ciOHDhzcYygLAnDlz4Obm5ni09bIHClnVjFmtzAII3gCMiIiIiIiIiIjoYrukg9ns7GxYrVb4+fk5tfv5+SE9Pb1R2/jnn3+wfPlyrFq1Cj179kTPnj1x6NChOvu/9NJLKCgocDxOnz59QcdwqZNJEoot8qoGq7n1BkNERERERERERHSFULT2ABpDkiSn10KIGm11ueaaa2CzNX4WqFqthlqtbtL4LmcKmYRia7XLwGIEFFfO8RMREREREREREbWGS3rGrLe3N+RyeY3ZsZmZmTVm0dL5kcsklDjNmC2vuzMRERERERERERE1i0s6mFWpVOjTpw/+/PNPp/Y///wTgwYNaqVRtS2SJKHYWi2YtRhbbzBERERERERERERXiFYvZVBcXIyEhATH66SkJOzfvx+enp4ICQnBM888g/Hjx6Nv374YOHAgvvjiC6SkpOCxxx67qOOKiYlBTEwMrFbrRd3PpaDAoqp6YS5tvYEQERERERERERFdIVo9mN2zZw+GDRvmeP3MM88AACZMmIDFixfj7rvvRk5ODl5//XWkpaWha9eu+P333xEaGnpRxzVt2jRMmzYNhYWFcHNzu6j7am15FmXVC1Nx6w2EiIiIiIiIiIjoCtHqwezQoUMhhKi3z9SpUzF16tQWGtGVx2nGrKmk9QZCRERERERERER0hbika8xSyyi2qatemBnMEhERERERERERXWwMZusQExOD6Oho9OvXr7WHctGVCE3VC5YyICIiIiIiIiIiuugYzNZh2rRpiIuLQ2xsbGsP5aIzStqqFyxlQEREREREREREdNExmCWYZWpYK8v8spQBERERERERERHRRcdglqBWKpBnVgIARDmDWSIiIiIiIiIioouNwSxBo5Qjq9wezKK8gHVmiYiIiIiIiIiILjIGs3W4km7+pVHKsTXHAwAgQQC5Ca08IiIiIiIiIiIioraNwWwdrqSbf2mUcuwrcKlqKM1pvcEQERERERERERFdARjMErRKGZJLtVUNDGaJiIiIiIiIiIguKgazBK1SjtNGTVVDeUHrDYaIiIiIiIiIiOgKwGCW4KZTodQqR7FFbm8oL2rdAREREREREREREbVxDGYJnjolACDLZP8LE4NZIiIiIiIiIiKii4nBbB1iYmIQHR2Nfv36tfZQLjoPvQoAkFlu/wuLEbCaWnFEREREREREREREbRuD2TpMmzYNcXFxiI2Nbe2hXHSeFcFsSlm1OrNFqa00GiIiIiIiIiIioraPwSzBS68GAOzJd61qzD/VSqMhIiIiIiIiIiJq+xjMEkK9dACA2OrBbF5SK42GiIiIiIiIiIio7WMwS9CrFfB31eBkqRZZpoo6s7kJgLmsdQdGRERERERERETURjGYJQBAhI8egITV6V72BmEFMg+36piIiIiIiIiIiIjaKgazBKAymAVWpftWNZ7dDQhbK42IiIiIiIiIiIio7WIwW4eYmBhER0ejX79+rT2UFhHubQAAHCg0oFDmbm8sPANkHW29QREREREREREREbVRDGbrMG3aNMTFxSE2Nra1h9IiKmfMAhI2mntWLTj1FyBEawyJiIiIiIiIiIiozWIwSwCAyIoZswCwLtMT0LjbXxSkABkHW2dQREREREREREREbRSDWQIABLprIJdJAIDTeUYgfHjVwtPbW2lUREREREREREREbRODWQIAKOQyBLhpAABn88uAwL6A3s++sCAFKE5vxdERERERERERERG1LQxmySHIXQsAyC81I6OoHAi6qmrhkR8Am6WVRkZERERERERERNS2MJglh35hno7naw+nAwG9ALnK3lCUChz6njcCIyIiIiIiIiIiagYMZslhRGdfx/OjaYWAUgt0uauqQ1YcsG8hYC5thdERERERERERERG1HQxmyaGDn4vjeXxGkf2JbxfncDY3Adj1CVBwuoVHR0RERERERERE1HYwmK1DTEwMoqOj0a9fv9YeSosxqBWOOrMnMoohKssWBPQCutwJyNX218Z8IPZT4PhvgNXcOoMlIiIiIiIiIiK6jDGYrcO0adMQFxeH2NjY1h5Ki4ryMwAAisotSCswVi0I6A0MeAowBFS1nd4OHPgaKMls4VESERERERERERFd3hjMkpOo2soZVNJ6AH0fBYL6V7XlJgI7PgQOfgeUn9OfiIiIiIiIiIiIasVglpzUG8wCgEIDdB4HdLsPkKuq2jMPAXs+t5c5ICIiIiIiIiIionoxmCUnHf2rgtlDZwvr7ujXDbj6OaDDGECps7eV5dhnzyb+ydmzRERERERERERE9WAwS046+rtAq5QDAGKTcmGzibo7qwxA6GDgqicAjbu9zWoCkjYB294G9n8DlGZf/EETERERERERERFdZhjMkhOlXIYBEZ4AgPRCI7YlNCJY1XoA/f/jXHsWALKPArs+AVL+BizlF2G0RERERERERERElycGs1TD3f1CHM+X7U5p3EpKnb32bP8nKwJayd5uNQHxa4B/3gVObgTMZc0+XiIiIiIiIiIiossNg9kr3Ml9sVg8Yyq+mDoJhzavBwBc19kX3gb7jb3Wx2XgWHo9tWbP5RJgD2ivfRnwbF/Vbi4DTm4A/n4HOPEHZ9ASEREREREREdEVjcHsFc5qMiPnTAqKcrJQVmgPYBVyGe6pmDVrtQm8vy6+6RtW6YFeDwFXTQP8ewGSrHKHwKm/gO3vA8lb7a+JiIiIiIiIiIiuMAxmr3BKrdbx3FRWVWbgieHt4e+qAQBsOJqBxKzipm9ckgDXdkDXu4BBM+wlDiR5xc6KgYS1wNY37QGtzXJBx0FERERERERERHQ5YTBbh5iYGERHR6Nfv36tPZSLSqWpFswaSx3PNUo5Jl8T7nj99fbkC9uR1rOiBu0TFSUOKmrQ2sz2gPaf94GU7YDVfGH7ISIiIiIiIiIiugwwmK3DtGnTEBcXh9jY2NYeykWl1ukcz02lzjfmuqVnINQK+yWyZOcp7EnOvfAdGvyB3pOBAU8CAb3gCGjLC4D434C/3wVS/uYMWiIiIiIiIiIiatMYzF7hlNVmzJYXFjgt83XV4NmRHQEAQgDP/3gQRcZmmtFq8Ae63AX0exzw7lzVbi4B4tcA2+YAGYeaZ19ERERERERERESXGAazV7iSVascz8vS02ssf+iacPQIdgcAnMwuwRPf7UO5xdp8A3ALBno+CPT/D+DVoardXAoc+g6I/RTIS2q+/REREREREREREV0CGMxe4XQRkY7n586YBQC5TMLH9/SEq0YBANgan4X/+/lw8w/EJRDo9RDQbyrgElTVXnAa2PsF8O+XQGlO8++XiIiIiIiIiIioFTCYvcLpu3WDzGYDAJhKS2rtE+qlx/8e7AuN0n65/PTvGfx3/XEYzc04c7aSWzBw1VQg+nZA51PVnpsI7PgAOLoSKG2GWrdEREREREREREStiMHsFU7h4wN5xQ24LOXlEBUh7bn6R3jhpdFVtWA/2ZSAiYt2o6T8ItykS5IBgX2B/k8AEdcBcrW9XdiAs7uB7e8Bh5YBpdnNv28iIiIiIiIiIqIWwGCWIJfbyxTYIGA6darOfuMHhOLBgaGQ2XNc7DyZi7s+34HMIuNFGpgKiBgBDH4RCB9eFdACQMYBYPt/gb3/s5c7ICIiIiIiIiIiuowwmCXIVSoAgJAklO7aVWc/mUzC67d0xbJHBzrKGhxJLcT4L3cjNb/s4g1QoQEirweufs4e0Cq1VcvyTtpvEHZ4OVCWd/HGQERERERERERE1IwYzBIUej0AwCZJyPnyK5jT0+vtf1W4J1ZMGQg/V/sM1uMZRRj90TasPVz/ehdMpbcHtANnAGHDAI171bL0/cCOD4HEPwHzRQyJiYiIiIiIiIiImgGDWYJCpwNgD2bNZ84g6bbbUfrvv/Wu072dO5Y/OhBB7vbZqwVlZkxduhdvro7D6dzSiztglR5oPxIY9CzQcSygtI8fNjOQtAn4+10gYS1gKr644yAiIiIiIiIiIjpPDGYJcoUSAGCrKB5rzc1FykOTUXbkSL3rhXnr8fuTg3F9tJ99fQF8+XcSRn+0DZuPZ17cQQOATA4ED7KXOAgeZL9pGABYy4HkrcDfc4HjvwHlhRd/LERERERERERERE3AYJYgV9hv/iUkCaroTvbnRiNO3f8Asj//AtbikjrXddMp8dn9vXFf/xAoKoLd4nILJi2Kxf1f7kRcaguEogqNfebsoBlAUH9AktvbbWbg9Hbgn/eBQ8uA/FOAEBd/PERERERERERERA2QhGBSVZ/CwkK4ubmhoKAArq6urT2ci2LZrBdw9ph9duwT8xch5Y47YcnIcCxXhoTA+/HH4XrjGMgqbhRWm+zicrz08yH8GZfh1H5332BMHhyOKD+Xi3MA5zIWAKe2AWd328PZ6tzD7DcQ84ysmmFLRERERERERESXjbaS1zGYbUBbOdH1+eHNV5ByaD8A4D9f/wBZUTHS33wLRevXO80wVfj6wu2WW6C/5hpoe3SHTKOpsS2rTWBZbAreW3cc+aXOoejgDt54ZHAEro3yuajH42AqBpI2A6l77eUNqnMPAzrdAhj8W2YsRERERERERETULNpKXsdgtg4xMTGIiYmB1WpFfHz8ZX+i6/PzO68had8eAMDUr76H1mCf2Vp+Mglpr7yCslpuBCap1dD16QP91YOgHzQI6g4dIFWURACAglIzXl8dh9UHU1FusTmt2yfUA+/c1g0dWmoGrcUIpP4LnP4HKMt1XuYTDXQYDei8W2YsRERERERERER0QRjMXiHayomuzy/vv4mE2J0AgMc+XwK9u4djmRACxZu3IH/5chT//Tdgtda6DUmrhaZzZ2i6doG2a1dounaFKiwMxSYrfthzBou3JyMlt9RpnTv6tMMTw9ojzFt/8Q6uOpsVOLMTOPEHIKofh2SfQevbFWjX335TMSIiIiIiIiIiuiS1lbyOwWwD2sqJrs9v895F/I5tAIBH5i+Eq49vrf3MmZko3rwFZYcOouSf7bCkpdW7XZleD02XLtB07Qp1ly5Yb/XAnH2FyCuzOPXrGeyO6dd1wJAoH0iS1DwHVZ+yPOD0DuDMDsDmPBbofICONwFeURd/HERERERERERE1GRtJa9jMNuAtnKi6/P7/P/i6LbNAICH5n0Oj4CgBtcRQsCUlIyS7dtRumcPjIcPw3zmTIPryVxcke0diH3CFUfd2iHZxR9pei/kalwRHeiGBweG4rpoP3gb1Bd8XA0yFQNndgPp+4DSbOdlbiFAYD8gsA/QEmExERERERERERE1SlvJ6xQNd6G2Tq5QOp5bLZZ6elaRJAnqiHCoI8Lh+cD9AABLXh6MR+JgPHwYxiOHUXb4SI1ZtbaiQngWFWIEgBHY7WgvUWhw2sUHKWv8MNfVDxG9u+K22wbDp2PExZtFqzIAEcPtj4IUIP53oOCUfVlBiv1x6i97OOvfE9C4XZxxEBERERERERHRFYczZhvQVhL4ulhzipHx1x6U5Rfg4NHtGPT4o/ALj2y27VuyslB25AiMh4/YA9u4OFgyMxu9fpHeHYbwELiFhUAVGgpVRDjUkZFQhYVBptE02zgBAMJWUeJgF1Ca5bxMkgEBvYGQawCDX/Pul4iIiIiIiIiIGq2t5HUMZhvQVk50XaxpBTDvTwEA/BP7OzrcNhohXbtf3H0Wl8B0MhFl+/bBdOYsTKeSYUo8CfPZs43fiCRBGRQEVWQE1O3bQ92hA9SR7aEKD4fc0Aw3E8s8AiRvAQrPLc8gAb5dgHYDAI8IljkgIiIiIiIiImphbSWvYymDK51C5niqUqpRkpdz0XcpN+ih7d4d2u7OAbCttBTlSUk4e+Ao9v29D7bDhxCSnwp3U0nNjQgB85kzMJ85g5KtfzktUvj4QBURAVV4GFQhofYANywMqpBgyLTaxg3St4v9UZIJnPobSPsXEFYAAsg8bH/ofIDgAUDQVYCMHyUiIiIiIiIiImo8zphtQFtJ4OtiyyuBaedJAMCBuH8gtffGVbfc0cqjsisymrFs92ls3H8KZ48noV1RFoKLMxBclIngokyElmRBYzY2foOSBGW7dlBHRkLdPhKKgACoQkKgCg+HMiAAklxe97qWciB5K3B2F2AudV6mcQeCBwGBfQFlI4NfIiIiIiIiIiI6L20lr2Mw24C2cqLrYisywvT3CQBAXHwssrUlGPnof1p5VDWl5pdh5b6z+HbnKaQVVISxQsDbWIAOxRm43cuE7qIAqtTTMCUlwZqb26TtS0ollMHB9jq2ISFQhYVCGRICdVgYFAEBkGQVM4ttFiArzl6HNu/kORuRAX49gIjrAJ1nMxw1ERERERERERGdq63kdQxmG9BWTnRdRJkJ5VuOAwASkg/hn4PrMOWzr1t5VHWzWG3460QW5m9KwL8p+TWW9wn1wLheQRgVrIUh8yzMZ1NhSjkFU3IyTEnJKE9MhCgtrbnhekgaDZSBgVAGBkIdFVVRzzYCKi815Jk7gJzjNVfyjASCrwa8O9oDWyIiIiIiIiIiahZtJa9jMNuAtnKi6yLMVpRviAMAnDobj9V/LsITi5ZDrWuGG2hdZGfySrHon2Qs3p4Mq835MpbLJAzr6IuRXfwwplsADGp7DVhhs8GSloby5GRY0tJgSk5GeXIyzKdSYEpJgSgvb9IY5O7uUAUHQumphtrDCm07A5Reeig9tJAUMkDtai9zENAbULs027ETEREREREREV2p2kpex2C2AW3lRNdFCIHytYcBAGmZp/Dz7wvwwJx58Ito38oja7z0AiN+2X8WP/17BvEZxTWW61Ry3NU3GE+N6AAPvarO7QibDZbMTJiST8F06pR9pu3JJJiSk2FOS4MwNqWeLaBw10LlrYfSWweVlwHqjh2h7j8ays4DICmV53Ool77UvUD2MaD7/fbXiX8CGYcAhRq4alrTt5e4AQgfWnVztcwj9rDbLbjp2zryA+AaZA/KW9KZXYDVDIRe07L7JSIiIiIiImqj2kpex1vJX+EkSQLkEmAVUCs1AIB9a3/DqMen25ddBvzdNJgyJBKPXhuBfxJysOFoBtYcSkNWkX32a6nJisXbk/HdrhQMjPTC8E6+uKNPO+jVzpe/JJNB6e8Ppb8/9AP6Oy0TQsCSmQXjkcMwnUqB6eTJivA2BZb09JqDEoAlrwyWvDLgREXb6jgAKwGZBKW/H1Sh4VCGhEATHQ11RDiU7dpB4etb/03ILjen/gKueQFQGc5v/aSNQOjgqmA2K84erp5PMHux2KyArJ5z1q5/3cuIiIiIiIiI6IrFGbMNaCsJfH2MW44BZWaUGUuwcNmbAIBxz89EZJ+rWnlk589qEzhwJh8/7DmNFXvO1Frq4MUbOuH2Pu3gWc8s2sawlZXBlHIaxsOHUZ6YCPPZszCfOQPzmTOwFhQ0bWNKJVSBgfbZtR06QBkYCFV4GNSRkZC7uV3QOM+L1QzE/QAUZ9hr5aoMQO/JQNq/wOkdgLABcjXQ6WbA4O88Yzb2M6Agxd7uEQ50vLn2fZzaBmQcAGw2e8DZcSzgFgIcXQmc3W1fHxIQcg0QvxqQKwGlHggeCGi97G3uIUD+Kft4utwJuLaruZ/qM2bzkoBjq+ruW9lfkgNluUB5oT0M7nyrPSQ+8oN9FnBpNmAsAAY+DSRvtb8vkAAXf6DTOEChsc/6tZqAqDHNc06IiIiIiIiIrnBtJa/jjNkrXEZZOQ5662Cx2tC+QAWZTA6bzYqj2zZf1sGsXCahd4gHeod44JHBEVj4TxI2Hs1EWoG9HIHVJvDW70cx54+j6BHsjgERXri9dzu09236zE6ZVgtNxyhoOkbVWGYtKoL5zBmYTp6Ecc9WmI4fhCkzH+asEtjKLTU3ZjbbZ+KeOoWi9eudj8nHG+qISKgjI6FqH1nxPAJyb++LN7s5Jx4wl9mDRwAwlwL5yUD6QaDvFHtImZcEHF4ODHjKed1+jwMbXgL6PmYPMesS0Ms+KxawB7lxPwMDp9tD0LO7ndfPS3QuR5B7EijJAKJvswehZ3YBCeuB3g/Vvb/0/UDyX0DPiYDWo/7jLzxt379cCRz4Fkj5BwgbYl+Wfwro86h9bNnH7aF0v8cBpRY4+jOQsA7odEv92yciIiIiIiKiKxaD2StcqcWCMxU/6fctLYNe54qi4jwc37ENUQOuRtSAy78uZoSPAW+O64Y3bhHYeyoPC7aexIajGQAAmwD2peRjX0o+PtuSiMEdvHF3v2BcH+0HteLCSwrIXVwg79wZms6d4XrjjfafvWfFQaRshy01HuVphTCeKYApuwTmnFKY88wwZRdDGGvehMyalY3SrGyU7trl1C5zdYUqPAyqkFCoQisfIVCFhl74LFtDAFCSZZ9d6h4BeHcEso4CxWnA7k+r+plKAFstQXNjFKUCSVvsoa8kA0oy7duSNfLrSedTNevVLcRePqEuqXvts2D7PGIPUBvi170qFA7sA5zZWRXM+nWrWpabAPj3rNpm0ADg8PeNGz8RERERERERXZEYzF7higv2AOgAACiQJ6LX0NH4a/V3AIDfPnwH3a+7AcMnTYFccfnfrEqSJPQN88SXYZ44nl6En/89g43HMpGQWXXDsG0nsrHtRDbcdUoM7+iL+weEoHeIR/PNSJXJAb9ukPy6QV6aDV3mEejS9tlnfVYQNgFTthEmqz/MZTqUp+XBlJCI8sREWHNza2zSVlgI44GDMB44WGOZ3N0dyoqQ1v4IgzoiHKrwcMi0jQgmdZ722bJ5ifbwMeEPwLMDENgXiLz+gt4K++AtwMGl9pmnrkGAxQhsmV1Rt7WRX0/V+0mSvZxBXQwBQH6S/f12DzuPAVe7DuTVS2AI+77r6ktEREREREREdA4Gs1c4RbVQyyrJ0KP3tUjPTkH8zr8BAAc3rEX26RRc9/BU+ISEtdIom19Hfxe8NKYzXhrTGZmFRny7KwU/7T2Ds/llAID8UjN+3ncWP+87i+gAV4zo7Itro3zQM9gdSrmseQah87bPvgy9Fsg6Ahz/DSgvhCSToPbVQo0CAAVArxAgeBzg2xWWgkKYEhNRXhHUmhITUZ6cBEtqWq27sObnw5qfXzO0lSQog4KgjoyEMjQEmo6doAoPhzqqA+SGauUcjAX2WaA+0YBXFJAZZy89cGQFENQP0Ljbg9Ci1LprtdbHZrGvr6mY2Xt6h/Nyudoe1lbOTFWoAUvN2cSN5hpoL5twYIm9zIBXh/r7ZxwCQq62h7+pewHP9rX38+wAnPjDXmJBobaXYPCMPP9xEhEREREREVGbx2D2CqeUV/1c3ypJsKUV4qannsfOkFBsX7EUAJB6PA7fvTIDA269Gz1H3Qi1Tt9aw70ofF01eOb6KEwf0QE7T+ZgWexpbDiagVKTFQAQl1aIuLRCfLIpAQa1AiO7+OGGLv4Y0tGnWcodQJIA36728DP3JJB5EEj9FxD2/aMgxf5QuUAR2BuKqG7Q9e3rtAmb0Qjz6dOO+rSmUymO55b09Jr7FMJxg7JzKXx8oImOhiIwAIaugdB75kBSqSDJZPZQ1iMcaD/KHm4Km/3h3fH8glmFBoi4DtgdYw95fTo7Lw+9Bvj3S0CmtNeN9e8FxP1oD0wrb/5Vn50fA70mAupqhcANfkCvScD+r+3H4dul7vU9wuzHaSyw3/wreGDt/bw7AsXp9hueAVU3/6rUApNn839eieItW9Du448AAFkff4LCtWshMxgQvmJ5k7eX9cl8eE95FJLKPjO4aMMGKHx9oe3evVnHfTEVbdqE0j174ff8c609FCIiIiIiIqIaJCGEaLjblaut3OWtLslZe7E5PwAA4Ft0CCMyAqG+PhqSQo4zx47g90/eR1F2lqO/m68fRk55CiFdL59w5nwUGc34fncKfj2QisNnC2vt46JRYFQXf4ztEYhBkV7NN5MWsM8SzTwMnPrbqcyBnWQPSNsNsIehDZRZsBmNMKVUBLXJyTAlnkT5yZMwJSTAVlra6CEpAgOg7tABquAQqEKCoe7UCZrOnSF3cTmPA7wMHPnB+UZj5+vYKnvoHDa0GQZVt3OD2WM9eqL95k1QeHqe1/aOduqMjnv3QKa3/0NM6osvQdO1KzwfuL/ZxnyhhM1etkKSNeNnj4iIiIiIiC55bSWva/PBbFFREYYPHw6z2Qyr1Yonn3wSjzzySKPXbysnui5ncw5gfa4PAMCrOA4j0/2gujYKMr39p+MWkwnrP/8YR//e4rRe52uGYsj4ydC7N3BX+zYgo9CIv+Kz8HdCNjbEZaCkYiZtdTqVHL1C3HFNex8M7uCN6ABXyGTNME1SCCAvCTi9HciKA3DOx1Xnbb/pVGAfe/jXlE3bbLBkZsKUlATjseMwnUpG+YkEGI8ehWhCYKts1w6qsDCoIsKhjmwPdWQEVBERkHs0Y23e1tAcweyRH+wzaXuMb/L5sRmNSH3pJZTHn4CkUEDh5YWQhV+h4JdfkLv0OwiLGXKdHn4zX4UmKsopmE2+516U7d8PdVQUdFddBf9XXq51HzmLFqNwzRoIqxWSQgH/V16GtkcPpM16DfnLl0MdFQXIZPCcOAEZc96BTKOB3MMDHvffB1VIKDLmzIG2V0+U/bsPwmpB4Jx3oO3W1Wkf5tRUJN1xJzps2eyYfZv6wovQdOkCzwfH1zqurE/mozwxEbbSEljS0qBsF4zAd+ZA7uaGrE/mw3Q6BaKsDKZTKQj+3xco3bkTOV9+BUgSFAH+CHj9dSj9/GqE1URERERERNQ2tJW8rs0Hs1arFeXl5dDpdCgtLUXXrl0RGxsLL68GfgJdoa2c6Lpk5sdhTZY7AMCj5DhuSPOGskcw5IHuTv3SThzHlm++RGr8UUebSqtD9LXD0OP6MfAODm3BUbee4nILdp3MwZpDaVh/JAPF5ZZa+wW4aTCuVxAGt/dGnzCP5il5UF4IpO0DkjYD1nPqrEpy+w252vUHXAIuaDfCZoM1JwemU6dgPH4c5fEnUB4fj/L4eNhKShq9Hbm7O9QdOkAdFQV9Vz/o3XMhqdXOsxvDhgL+rTj7uigVOPJjzfaA3vYyCq2o8M8/kf/9MoQs/AqAvV5w+cmTyP78c7T75BPIVCqU7tmD9DfeRMQvq2qEkOfOeK2NJTfXMaO2bP9+pL36KiJ++63W9c+dMVuyazdSJk9G2PffQ9utK/KWLUPRnxsQ8tWXNfaT8tBkuN91F1xvGAVbSQlODB+ByLV/QOFR+z/sZH0yH3krliNi5UoovL2RNns2ACBg1ixkfTIf+T/+iPCff4LCywvG+HikPDQZ4T/9CKWfH7IXLEDpvn0I+fxzBrNERERERERtVFvJ69p8jVm5XA6dTgcAMBqNsFqtaONZdJMoq91Z3loRmFnTC2oEswEdOuKe2e/i0Ob12LZ0MYwlxTCVlWL/ujXYv24NInr3Q6dB1yK8dz9o9Aa0VQa1AiM6+2FEZz8YzVZsOZ6JtYfTsSspF2kFRke/tAIjPtuSiM+2JEKjlOGqcC8Mbu+NUV38EeKlO7+dq13tNwtrN8BeY/XsLqCwokassNpfn90FeETYQ1q/7oCs6YGwJJNB4eMDhY+PUy1bIQQsGRkwnz2L8pMnYYyLgzEuDqaExFoDW2t+PkpjY1EaG4u8au3K4GCoO0ZB3b491BlJUEVIUEdE1BsgXjQugcCAJ1t+v42g6dQJ5UlJSJs9G/p+/aC/dgiKNm5E+bHjSL7rbkc/a24uhMl0Xvswxh1FzoIFsBbkA3IFyhMSIUwmx8zWhqjDwxwzZLU9eyJn4aJa+7nddhsKVq6E6w2jULh2LfT9+9cZylZyGToUCm9vAIDHXXfh7PSnHcsMQ4dCUfGPa6W7dsMwdAiUfn72vvfei+zPFvB7noiIiIiIiC55rR7M/vXXX3jvvfewd+9epKWlYeXKlRg3bpxTn08//RTvvfce0tLS0KVLF8ybNw+DBw9u9D7y8/MxZMgQnDhxAu+99x68K/7PPgFKudrx3CbZLwdbYVmtfSWZDN1H3ID2/QZi23dfI+6vjbBZ7T/rP/lvLE7+GwuZXIGwnr0R0qU7wnr0hle7kIt/EK1Eo5Tjhq4BuKFrAIQQSM4pxbYTWVh3JB3/JOQ4+hnNNvwVn4W/4rPw1u9HMSDCE/3CPNEl0BVXt/eGi0bZtB0r1EBQX/ujNBtI3WsvdWCtCOfyTtof8WsAv65AYD97AHk+ZQU2vAQMfQ1QqCFJEpT+/lD6+0PXp4+jixAC1uxslCeeRPnJRJgSEmFKToI1+wz8x0VCkkmADDClFyPt+/0wnz4N8+nTCB7nAuPZeGAvYNoL5O4pgsw3CurwdnANSIOkVkFSqiDZTEBZHnDty4DyPEPty5AqOBiRq39Dya5dKNm+Axnvvw/D1VfD/fbb4PPkhYfJwmTC2SefRMg330DbtQusxcWI79sPwmxudDArqaq+PyCTAZbaZ5C7XH8dMt56C5asLOT/vBLejza+nEzVzqquX5mu2nUghHPJjMu5fAYRERERERFdUVo9mC0pKUGPHj0wadIk3H777TWWL1++HNOnT8enn36Kq6++Gp9//jlGjx6NuLg4hITYQ78+ffqgvLy8xrrr169HYGAg3N3dceDAAWRkZOC2227DHXfcAb+K2VVXOoVMCQgzIMlhlVUEhGVmWBIzIY/wqbVGqM7VDaMeexJDH3wYhzf/iT1rVqI4JxsAYLNacHLvbpzcuxsA4Obnj4D2HRHQPgr+7TvCNzwSCmUTg8jLgCRJCPfWI9xbjwcHhiGzyIjtCTnYdiIbfydkIaOw6vrceTIXO0/mAgAUMgk9g91xQ1d/DIjwQpdA16bVZdV5A+1HAaGDgTO7gFN/2W8cBgDmEnvbmV2A1gPw62n/eX4zh5uSJDlm2OoH9K9aYLPAkpeL8oRklB8/DrXvUfiP1yNtyS6IMnv4n/zfvyCcavbuAQCkV7yS+3jD56bu0Ia7o/T7H6EKD4c6MhKKgIDLu35tI5jT0yF3dYXL8OEwXHMNijZuhNvNNyP1hRfhfuedUAYEQNhsMB6Jq1HXtTFsJhOExQJlgD8AIO/bb52Wy/R6WIuLHTOZZQYDbMVF53UsMrUaLjeMQtYn82E+fRr6axouE1G8ZSssOTlQeHkh/8efoB80sNZ++oEDkPPll7BkZUHh44O8ZcuhHzCgzV8fREREREREdPlr9WB29OjRGD16dJ3LP/jgA0yePBkPP/wwAGDevHlYt24dPvvsM8yZMwcAsHfv3kbty8/PD927d8dff/2FO++8s9Y+5eXlTiFvYWFhYw/lsiSXayCJQgjJALPcFQJmSAAs8RkAAEWkb53rqnU69LnxFvQafRNSjx3F8Z1/48jWjTAbq2bcFmSkoyAjHcf+2QoAkMkV8A0Lt4e0YRHwCAyCd7tQaAxtq/yBr4u9xuy4XkEQQuB4RhF+P5iG1QfTcDK76mf/FpvAnlN52HPK/mP/MC8dhnb0xTXtvTEg0gsGdSM/okodED4MCL0WyD8FnNkB5MRXzaItywOSN9uD27Ch9lq0apdaN5WXl4fff/8deXl58Dh7BDf2zIe7ly+QsBYoyQK63QPIGzGjUqaAwssXCi9f6K/qCxxdCSi0cJ0eA/Pp08CJBfB5+mmUJybBeOAgyk+cqLEJa1Y2dIFA5o87UHzwZ0e73Nsbul49oe3RA+oOHaCJjobCx6dx79Vlojw+Hpn//cB+Azhhg9vNN0PXrx98nn4ap6dNA6w2CIsFhiFDziuYlRsM8HnyP0i+8y4oAgPgMmy403LPSZOQMmEiJI0GIV99CbdbbkbqSy+hcO06x82/6nNy3K0I/vxzKP3s3yHut92G5DvvgtcjD0OSN1xiQzdwANJefgXms2ccN/+qjbpDB/g88zRSJtv/G1F58y8HBrRERERERER0ibqkbv4lSZJTKQOTyQSdTocffvgBt956q6PfU089hf3792Pr1q0NbjMjIwNarRaurq4oLCzEwIED8f3336N799pvOPTaa69hdsWNZqq73IsJ18VqLcOKI2th1NpnOo4ps8LtbL5juTzcG4qO/o2efWa1WJBxMgFJ+2KRcvggMpMSYTE3XP9S7+4Bd/8AuPsHwsM/EO7+gXD3D4CHfwBU2rbz8/XKkgfxGUX4JyEbm45l4kxe7aUjFDIJvULccU17H1zTwRvd27lBKZfV2rdWphJ7mYPMw0DRWUDYnJf797Df5MojEpDJUVhYiGefeRrfLl2K6DA/BHkbcCYjB0dP5+OBMQPx/ktT4NrnLkCSATkJwInfa9+vd0f7LF4AsFmA3Z8CxjzAEAD0fBBQaOzLNrwEuATZ6+N6RsLi2Q+mlLMwnUqB6VQyjAcPAUVnEHhvNE68sg6w1f9VJff2hjoyEurICHtY27Ub1B3aQ6bRNP49o0tC1ifzYSsthd8Lz1/QdnK+/BKm02cQMPu15hkYERERERERXRLays2/LulgNjU1FUFBQfjnn38waNAgR7+3334bX3/9NY4fP97gNvfu3YvJkydDCAEhBB5//HE8/vjjdfavbcZscHDwZX+i6yKEFSv3vI4C9ykAgKH+HgjOLHLMmAUAeYgnFNGB5/XTYKvFguyUZKSdOI60hONIS4hHXuqZJm1D7+4Bj4Age2gbUPHwD4SbfwCU1WtcXoaEEEjMKsamY5n49UAq4lIL68wftUo5Ovq74NoO3rixeyCi/AyNPyclmcDJjfabhuGcHWg8UODRC8PuexreqnK89/Bg9IisKvVxIDEDz/1vK7ItOmz565/z+xzYLMDx3wCtp/0GZgBgzAc07vZZvUdX2mvndhrnvF7cz7BZJRjNYTAlJcGUdBLGY8dRtn8/bMXFjdq1MjgY2l497WFtx4722bWsM31Ja45gNvPDeSjasAFB778HTefOzTg6IiIiIiIiam0MZi+CuoLZ7du3Y+DAqvqCb731FpYsWYJjx45d9DG1lRNdn992PoJsL/ss4V5eLujhYYA1JReWuFRHH5mPC5RdgyA19UZVtTAWFyPjZAJyU08j58xpZJ8+hby0sygtyG/ytgyeXnDz9Yebjy9cff3g6uMLV29fuPn4weDpBUUjb2J0qSg0mrEjMQd/n8jG3wnZSKpW9uBcrhoFrgr3Qt8wD/QP90S3IDcoGppRW5Jpv1FYxkHAXDVT99EP/0ByegHWvHUnlIqaPzM3W6y48dWVCO85BJ//78vGz5itriAFiPsZGDi95rL8U/Zwtvoyqwn4623gqqmA3rmkhrBaUZ6YCGNcHMrjT8B45AjKExJgzclBY8h9vKH0D4AmOhqarl2g6RwNdVQHyC6z66UxirduReaH82q0ez/6CFzHjGn5AVWw5OQ4yg9Upx80CH7PP9cKIyIiIiIiIqLLRVvJ6y7pYLY5ShlcqLZyouuz/p+xOOv7OQAgUKfGqCAvAIA1NR/mA6erOipkUA2IhMzl4vw03FRWirz0NOSnpyE/PRV5aanIz7D/PZ/QFgA0eoM9sPX2hd7dA3oPD7h4esPFywcGLy+4evlAeQn/1P1MXin+ScjGPwk5iE3ORVqBsc6+epUcfcM80T/CEwMivNAtqJ7SB1YzkH0MOPUX8s4mIOje+djx0XinmbLn2p+QgUFPf4fUtHS4u7s3PHhjvr32rVxlL6Nw4nfAVAx0vcceCsvkVcvi19jbut5VtX7qXuBsLNDvsYb3VcGSlwdTRWBrjDuK8qSTKD96DMLUcDkNKBRQR0ZC07kzNJ07Qd25M9Tt20Pu4dFqN5I62qkzOu7d47gBV1OZU1OR/vobMCUnA5IEj/vug+f4BwAAZ558CmX79sGSlVVjHykPTYYlOxuQySDT6+H/ysucdUpERERERESXjLaS17X6zb/qo1Kp0KdPH/z5559Oweyff/6JW2655aLuOyYmBjExMbBarQ13vsy5qeRIt6TCqghEWmk5yq02qOUyyAPdAQkwH0kFzFbAYoNpRyJUAy9OOKvS6uAXHgm/8Mgay8pLS5CXloq89FTkpZ5Ffnoq8tPTUJCVUW9oaywphjGpGJlJiXX2Uev0MHh6OR4u1Z4bPOx/da5ukGRNqO/aTNp56HB3vxDc3S8EgD2oXXckA3+fyMKupFyUmqquzxKTFVvjs7A1PgsAoFPJ0SfUAwMivDAgwsu5Rq1cCfh1A3y74veDn6JzqG+9oSwA9Gzvh87BHvj9l59w34TJDQ++OB1IWGd/LgTgEghEja0YbCZwbFXFMpt9Wcexzuun7gGC+ja8n2oUHh5Q9O0LXd+q9Wwmkz2sPX4c5fEnULp3DyzpGbBkZtrHVcliQfnx4yg/fhwFq6ptM9A+s1bbpQuU7dpB07kzVOHhjbqBVWsSQuDME/+B16OPwPWGGyCEgDU727Hc45674T9rJk5cfU2NdYPmfQh5xX/YijZsQOrLLyPi559r9CMiIiIiIiKi89fqwWxxcTESEhIcr5OSkrB//354enoiJCQEzzzzDMaPH4++ffti4MCB+OKLL5CSkoLHHmv8LLrzMW3aNEybNs2RwLdl7m49oSv+C0Uu90AAOFVsRJSb/YZb8gB3SK5amGOTIMrMgNUG895TUA2MhKRuuctHrdPDP7ID/CM71FhmMpahMCsThdmZKMzMREFWBgqzs1CSl4Pi3BwUZmVBnHvjq2rKS0tQXlqCnDMpdfaRyRXQe3jA1dsHLl4+0Lq6QufiBr2nJ1y8fODi5d0is2/beegw+ZpwTL4mHFabQFJ2MXYl5WLnyVzsPJmDrKKq+silJiu2ncjGthP2ME6rlKNXiDu6BbmhS5AbBkZ4wcdFjTyjQDtf90btP9DXAzmFpY0brHcn+6M27qHAgKfqX7/vlMbtpwEylapiFqzzjE9rcQnKj8ah7PARlB87ap9he/IkcM4/xlhS01CcmobiDRurtqnT2csgdOtmL4UQHQ1VaOgFh/d5eXn4/fffkZeXBw8PD7SvGIsQAln//S/KTyYh6L/vQ6bVNrit0h07IGk0cL3hBgD2XyQofHwcy/XV6nafS17tXxutRcWQpJb/RwkiIiIiIiKitq7Vg9k9e/Zg2LBhjtfPPPMMAGDChAlYvHgx7r77buTk5OD1119HWloaunbtit9//x2hoaGtNeQ2x9W1J3SZi1Dkcg8AYF9uIUL0amgqao3K9GqoBrVH+ZbjgNUGUWaCaWciVNdGtdpPvKtTabTwDg6Fd3Dt14TVYkZpQQFK8vNQnJuDotxsFOVkozin4m+uPcC1mOv+ubvNakFRdhaKsrPqHYtSo4XBwwN6d0+4eHlXzLr1hMHTC3oPL3s5BXd3KNUXHuDKZRLa+7qgva8L7u8fCiEEkrJLsPNkLnYl5WDnyRxkFFYFtWVmK7Yn5mB7or0OqyQB4V56uKeW4FRGYaP2eTarCDKZDPn5+dDr9VAqL7zmcGuRG/TQ9esHXb9+jjab0YjyEydgPHoUxqNHYUpIRNmhQxBG5xISttJSlO7Zg9I9exxtMoPBHgBX1q3t0sUe1jZiZm1hYSFmPPUUln73Hdrr9fCVK5BhtSAhPx/3PfoonnVzg0dAINrN/wSSTIaS7duRMfe9WrdlGDIEvk9PR3lCIuSeHjj7zDMoT0qGMigQfi+8AFVwcKPen9QXXkDJrt0AgJD/fdGodYiIiIiIiIio8S6pGrOXorZSs6I+5eUZ2PbP1cjwXQCTOhoAEO2uR38f55nC1vQCmPdVzSqVB3tA2bVdi471YhFCwFhc5Ahpiyr+FlfMui3OyUZRXi6MRY0LMBui0mqhd/eAzs2jIqy1P3Tu7jC4e0JX+drVDbLz/Mm8EAKnckqx82ROxazanFpr1FqNxUj/dDz2fPJAgzVmr37mO+zZu88xi1ytVkOv10OtVl8SIf3FIGw2lJ9IgPl0CsoTT8J45AjKDh+CJTWtwXVlOh3UnTtD08VeCkETHQ1VRIRTWFtQUIChgwZBn5qKGW7u6FRt1vUxoxHvZ2eh0NUV/xw71qTvoJyFi5A9fz7Cli+DukMH5C1fgfwff0T4Dyuc+jVUxzZ/5SoU/vE7Qr5gOEtERERERESXhraS1zGYrUP1GrPx8fGX/YluyP4DDyEj/zhSA74HJPtE6l6eLujhaXAK3Kxn82A+eMbxWtk7FHK/tvu+nMtkLENJXi5KCwtRWpiPktxcFOVkVZROyEVxfh5K8nJhKmvkz/0bIknQubpB7+buCGurQlwPGCr+6t09oNbpGwxHMwuNOJJaiN3JufgnIRtH0wphtgoUrvsI3ZCIDXNuh1JRMwg2W6wY/crP8O80AO+99x7O/dpQKBQwGAzQarVtNqA9lyUnB2WHDtlvNHYkDsYjR2BJT29wPUmrhaZTJ/vM2i5d8My33yJ+8yZ86usHZS3vnVkIPH72LKJvuxX/++YbAGjUjNnCteuQ++0ShH37LQDAVlaG4336otOhg07BcGNuMHasR0+037IZCg+PBo+PiIiIiIiI6GJjMHuFaCsnuiFnzizF8fiZyHedjEK3iY72gb5u6ORWFdgIIWA5mgbrqRxHm6JrEBTBni053Eue2WhEYU5FWJuXi6KcbJTm56EkPw8lBXkoyc9HaX4eyktLmm2fcqXSUULBxcsbOjd3aF1coXVxtYe7Hp4weHpC7+4BucJegiC3xIQ9ybnYcewMYp6+B529BT6eMsRp5uyBxAw8uWAr9mXY4HnPXHQM9sXVYS7o5qdGkKsavq4aqBT2GqQymQx6vR56vR6yVrhZWmuz5ORUBLVHHGGtOTW1zv4FViuGJSbgu5BQp5my5zpqNOL+0yk4c/IkvBpZxsVWWoqTt4xD6LdLoPTzQ+H69ciO+RQRv6xy3vY5way1uBi2klIo/XwBAIV//omMN99C+y2br5jQnYiIiIiIiC5tbSWvYzDbgLZyohtitRpxJG4GMrPWosB1EgrdHgIASBC4LtAL7fRVoZEQAuZ9KbBVq0uq7NYO8nacTddUZlM5SvPzHYFtaX4eivPsf+0Brv1Rmp8Hq8XSbPvVurhC5+YOvbs7dG72kglCqcKn3y3D7xs3o307bwR4ueB0dhESzmRDHz0ULkMfhkytc2xDAhDoIkcnbxU6+mgR4K5BsIcO/m4auOvU0Gq10Gg0UKlUV3SgZ8nLs4e0jsD2CMxn7LPOfysswNe5ufgxLLzB7dyZehYPR0XhibVrofD2btS+i7f9jcz//hcQAnIXF/jPmgl1B/sN9E4/PhXGuDhYMjKg8PWFKjQUoUu+gTktDWeefMpeV1cmg9zTA37PP1/j5mlEREREREREraWt5HUMZhvQVk50YwhhRfyJt3DmzNfI8XgBJYabAAASrOjhoUMPL0/IKgI2YRMwxybBlls141MR5Qd5hM8VHcJdLEIIlJeUOILayhC3pKJ0QkmBPdwtboY6uGUmM46mZaLUZIZOpUTXsGAYPLxRojAg26ZBllmJYpkGZXINjDItyuQa6AwG+Ph6ItjNHsLqVHL4uKjh76pGgLseQT5uCPJ2h0LR6vcbvCRY8/NhPHoU8z/5BOvX/I75AQENrjMtOxsjnp6OKVOmQKlUOh4KhYKfOSIiIiIiIrqitJW8jikJOUiSHB2jZsLXdzTijv4fTssMKNMNhYAc+/PKkVKYgJEhEdAq5JBkEpS9Q2E+kAJbVjEAwBKfAVtOMZTd2kHSqlr5aNoWSZKgMRigMRjg1S643r5moxFFuTkoKyqseBSgND8fxXm5KMm3l1YoyctFaX4+LGZTjfW1KiV6hwZVNQgbTLmZUCITAQDqihBtJySUKXSAWge5zoA8tR45aj0OqvWwqfVQGwzw8PJGu+AABLULRLd2HlDXUs/2SiB3d4d+4EC0O3kSmWvXNmqdTKsF7u7uMJlMMJmqzpskSVAqlVCpVI6/MpmMYS0RERERERHRJY4zZhvQVhL4prJay3DsxNs4WuKOIpe7AMkeoMlFOcJddOjs4QFvjQrCZoMlPgPWpOyqlRUyKLsEQRbgxnDoEiaEgNlYhpKCfJTm56O0MB+lBQUoLah4np+P0kL76+LmvKEZACGTw6TUw6LWQ+3iDg9vTwQE+CEowBcunp7QuXlU3PDMHSqNttn2e6nJy8tDkL8/lgYENlhj9oG0VOw/dAgGg6HB7cpkMkdQWz2sJSIiIiIiImoL2kpex2C2DjExMYiJiYHVakV8fPxlf6LPV1FRHP5N+AqntBNglTvXtQzTqxDmokc7vRqytAKY41IBi82xXNIqIQ/1gjzUG5KMAe3lzlRWiuK8XJQWFqCssABlhYX250WFKMrJQkFmRkVt3HwIm63hDTaSXKWGwd0dOreqh97xvCrA1bm5Q63TX3b/GPDIpEk4umoVPvX1g7KWsZuFwNTMTETfOg5fLFwIm80Gs9kMs9kMk8kEs9kMq9Xa4H7kcrlTWKtUKhnWEhERERER0WWJwewVoq2c6AshhBWJZ37EvpwilGiHQsh0TsslAN08DAhXKaE/lgaRU+K8AZUC8gA3yAPdIblpL7vgjJpG2GwoKy6qqIGbj5J8ew3cgpxcpJxNR1l+LkzFBbAWFwKmMjTn1SBXKmsGt+7nBroe0Lm7Q6M3XBLXYmFhIYYMHAh9aipmuLk7zZw9ZjTivwUFKAkMwNYdO+r8DrJarU5Brdlshq0R4Xj1kFalUrFeLREREREREV0W2kpex2C2AW3lRDcHozENJ5I+RXypCwpd7gYkZY0+Lgo52kFCUH4pvNOLaoRukk4FWaA7FO08WIf2Cmaz2WA0GpGelYvTZ9ORkZWL7KxcFOblobSwACWFxTCVFkNhKoXWWgadtQxam7FZxyBXKKB1CnGrBbdublDr9dC7ecDg6QWdqxukizi7tLCwEM9On46l332HCJ0OvnIFMq0WnCwtxf333Yf3581r0vePEKLWsLahr/vKerXVw1q5XM6wloiIiIiIiC4pbSWvYzDbgLZyoptTfv4eHDr2OvKFF4r1N8GoHVhrP40kob3RgqDMYniUW5xDWpkEmb8b5H6ukLnrIGlqhrzU9gkhUFZWhpKSEpjNZgCAxWpDdrEJWcXlyC4xY/fpEmxOLEBxmQlaqxFaayl01jLoHH/LHG3ukhF6Wxnk5aUAmu+rTSZXwODpCa2LK7QurtB7eMLg4Qm9hyf0bu7QGFyhc3ODwdPrgsop5Ofn4/fff0dubi48PT0xZswYuLu7N8sxCCFgsVhqhLUNkclktYa1RERERERERK2lreR1DGYb0FZOdHMTQqCo6BDOnv0ep7N2oFRzFUq1g1Gu7gFIihr9dQCCjRYE5pTAy2iB8pyrTtIqIbnrIPPQQ+auhaRXQ1Iw/LmSmM1mFBUVoby8vMbMTkmSUCKUOJFtwvaTuTh4Jh9H04pgstb+c31J2KCxGaG3lMJbYUKEXiBAbYa33AS9rQwqUylMxRU3OisogBDNVxNXqdbA4OkFvbtH1UzciucGD0/7c3d36N08IGvlgFMIUSOotVgsDa4nl8sdIW1l3VrOqiUiIiIiIqKW0lbyOgazdeDNvxrPYinCyZPzcDZ1OSxCjjLt1SjRXQ+jpi8g1QyeZMIGL6MJvmU2eBst8DZaoLLVvAwlFw1kblpIBg1krhpIrlpISoa1bZ3NZkNZWRlKS0trzOiUy+VQq9XQaDSQ5Eok5ZQgIbMYB88UYO+pPBw6U1BnWHuuADcNugS6IjrABZ3c5QjVW6G3lqGsIB+lhYUoLy1GSV4einKzUZybg+LcHBiLi5svxJUkaF1cYXD3gM7dAwYPT/vfytfu9td6dw+otC1Xm7ny5mKVYa3JZGqwXq0kSY6QtjKo5Y3FiIiIiIiI6GJhMHuFaCsnuiVYrUZk52xCauoK5OXtglnmhjLt1SjTXAOjpnetNWkBAELAYCqAl9EEn3IVPMsluJpqzqoFAKgUkAxqyAxq+6xavRqSTgVJq4Ik44y9tsZkMqGgoKDWn9wrlUro9Xpoq4WWZqsNCZnFOJJaiPiMIhxPL0J8RhHSChpXn9ZDp0SXQDd0CXRFe18DIn0N6BroBpXCHjJW3tisJD8PJbk5KM7LRWlhAYzFRSgtyEdRTjaKKkJcs7Gs2d4HhVoNvbsH9O6e0Lu7V/z1gN7Do1q7B3Subs0+C1cIAZvN5jSr1mQyNVivtnrpA9aqJSIiIiIioubUVvI6BrMNaCsnuqVZrWXIL9iL7OxNyM/fhaLSdJRq+sKo6QejujesCv/6NyBsUFuyoTcVwc1khHu5DZ4mFTxMLlBCXbO/BECttJdE0FQ8tEpIGpXjOZQMhi5X5eXlKCkpgdFYM2CVyWTQ6/XQ6XR11j4tKDPjREYRjleEtcfSinA0rRBF5Q3/bF+rlKNvmAc6+bugc4AregS7I8RTB6W8/hmh5nIjSgvyUZKfj5KCPJTm59mf5+dW+2tvtzaifEBjSJIMWlfXitDWE3q3c8Nbd3u7uwdUGu1578dms8FqtcJkMsFkMqG8vLzBWbUymcwxm5azaomIiIiIiOhCtJW8jsFsA9rKiW5tNpsZJSUnUFh4AEXFccgpPotciw5GZUeUq7vBrIystexBDaIcSksWVJZ8aM050JkLoTeVw8UiYDDLoRKuUMIFCuECBfSQQwcJFeGPXIKkUQFqBSSVApLa/oDjubLqNWffXpJsNhuMRqPTzcKq02q1cHFxgUJRs85xzW0JnM4rxeGzhTiSWoAjqfa/2cWmBtdVyiWEe+sR5eeCjn4uiPK3/w321EHexGtHCAFjSTFK8uxBrdMjLxelBXkozrMHuMaS4iZtu95j0GgrZt9Wzbh1PDyqXmtdXSGT1f/ZFEI4gtrqZRAaHMM5NxVTKBQt/o8nP+w5jU3HMvHZA30AAB/8GY81B1Nh0Cjxy7Srm7y9D/+Mx7Rh7R2zrNcdSYefqwY9g92bc9gX3eiPtmHl1EHQsHwMERERERFdgtpKXsdgtgFt5URfioSwwWg8i5KSEygoPonMsiLkmxUohTvK5CGwKEIgZLXMjq1zgyYoLKkVj3TIbTmQW3KhshRAbS2DxmqByiZBKQxQwACFMEABFyiE3vFXCRfIhQFKpRvkKp09pFXJIakUgEpuvyGZUm6vdas857VCxhm5LUQIAaPRiLKyslpn0apUKuh0Omg0mibNyhRCILOoHHGphUjKLsHBM/nYeTIX6YWNK4WgUcrQ3teAfmGeGN7JF1F+LvB1UTfbdWExmVBakI/ivFyUFOShJC/PMeu2OD+34q/9tc1qbZZ9SjIZdG7u0Lq4QqM3wODpBRcvb0dNXL2bO3Ru9hBXrdc7jvV8atVWzqptyZuKnRvMdnzlD2x/cTi8DE347qkm7MU1ODJ7FPRq+z8OzFhxAN3buWHCoLDmGnKzsFhtUDQw65uIiIiIiOhS1VbyOgazDWgrJ/pyY7NZUFp2GtlFKcg2lqLADBTZNCgVbjBJ7hAyzfltWJght+ZCbsuFzJprf17xWm7NhcyaA7ktHzJbERRWI5TQVQS4BsiFDnJoIYcGcqGFHFrIHM819mVyPRRyHeQKPRRKA2QKA5QqAySVuiK8rRbqOr2WQeLPus+L1WpFSUkJSkpKatQ9lSQJWq0Wer0eSmUdNY4bIITA2fwyJGWXYO+pPJzILEZiZjFOZpU06kZjrhoFovxc0MHPBR39DI7n3gbVRQsdK2vhVg9qiytm35bk5zvNzi0vLWm2/SqUKug9PWHw8ITewwsunva/Bg9P6N09oXVzg1JngJDJYLFYGpxVK0mSU51alUrVYNBuNFsx44cDOJ5eBIVMgo+LGksm98fP/57B1ztOwWK1Qa9W4I1buqKjv4tTMHvbp//g35R8dPJ3wYAIL7x2c5da9/HltpP49UAqLFYBpVzCazd3Qa8QD/zfykP4blcKOvm7QJIkPHxNOF5fHQetUg4PvQoTBoYi1EuP11f/P3v3HSdJXed//PWt0Ll7piduzrvkjICgCOgpgiBGFBTDeSY8lWQ4z5zuVDAu+jPriTnnCEgSECQsaYENbJrZyZ1DdVX9/uipmu6enrQ7u7M7+3k+Hr3dXV3d/Z2dqQ7v+tTn+wgnLW/lnq3D2I7LtS8/jmOXtI57nt5UkSt/fD9DuTJLkhEc1+VZ6zonDHm3D+W58Eu38dKTlnD31mHypQofvvAoTl/T4d/22tNXctuT/TzvqAU854hu3vfLDQxmyyileOdz1vK8o6ptZhoDZiGEEEIIIYQ4kMyXvE6+cYkDkqYZxKIriUVXsqLhNtd1KVQqDBZGGC5mSJVKZCoOWdug4IZxJvuzVia20Y1N97TGoZwcmpMZPaVHz7M110fQnO31t7sZNCuHssZCO+Wa40JdvSHU1VQYXYtiaNVg1zsZRgzdjGGYMYxAHD0YQzMjaKZRrdI1qtW6aOqQrNjVdZ1EIkE0GqVQKJDP56mM9mx1XZd8Pk8+nycQCBCNRgmFQjP6f1JKsSQZYUkywjPXdvrLLdvhqcEcG3uzbNyd4fHRicY2D9QHnelihXueGuaep4brlicjJuu646zqjLG2K8ZRixIcuShBPLRnAXLdmDWNSKKFSKKFjmUrJl3XKpdG+9+OVeBWq3GH6loqFLNZKtbkbR4qVpnU7l5Su3snXc+fzKwlSTjRQjCeIBiLE4hGCcYShOKJ6vJYHNd1KZfHntcwjLqgtnFSsZs39pMuWPz1ymcBMJIvc8/WIX7zwC5+/KbTCBo6d28Z4h0/vI8/vvPMunH9/K1nsOI9v+Nnbzl90kDyRScs5g3PXAXAv7YN8+6fPcifr3gWn3jRMXz/rm11979j02Bdxew/Ng3yxO4M//uSY/jYRcfwvTuf4tN/2sj//fup457nQ79+mFNWtvHO56xj22Ce53/+Fp61rnPcerWG8xaHLUjwvvOP5F/bhnnjd+/llned5d+2oiPCO55zOgAvXH87F5+8lEtOXcaWgRwvuv52jlncwqLWPe8/LIQQQgghhBBi+iSYncD69etZv3499iwdDixmj1KKiGkSMTtZmqgPKVzXpWA75Co2hUr1vHrZJm/bFKwKBdum6EB1xrDJuVoUW4tiM8VkZePu6KDcsVBX90PdZqc0yu1Fc/IoN4/mFFDlPKo8STWmq41V6bqhsepdFcFQYXQtgqZFqiGvHq2ejEg15DWi6EY16NUD3nkUzQwe1CGvruvEYjGi0SjlcplisUg+n/eraL2JqjRNIxKJEIlEptWLdiKmrrGmK86arjjns9Bf3pcucvPGfh7fneHxvixP7M7QkxrfCmE4b3HXliHu2jJUt3xlR5QjFyU4elELhy+MszQZYXn71JON7fHPEQjS0rWAlq6p/8atYpHM0ACZwQE/tM2nRqrBbmqE7NBgNcSdohdupVSaVoALEIhGCSdaCMVbqu0UEi1+cBuKJ4i2ttHS0UGsNUkgEODwBTE29WX5719u4NSV7Zx9eBd/eWQ3j/ZkuGj9Hf7jDuXKlCtTVzw38/CuNF+66UlG8mV0TeOJvizliuP3lZ3Kqs6oXyF74rIkX7t1c9P1/rF50K/aXdYe4fQ1HVM+dkDXePEJi/3H7owHebQnTVc8RNDQuPC4RQBkSxUe3ZXm4qctBap/dycvb+OfW4d44fGLp/VzCCGEEEIIIYTYOxLMTuDyyy/n8ssv90ujxcFBKUXE0IkYk09Y47guJduhYDujoa1DsWJXr9sOZduh5DiUbXf03GFGEY7ScFUcW4tjA1NPg9TkIZwiyi2guXmUU6hedvKjywooJ++fj1vmjlQDXqeIskcfxykC1oRxtHINdELVgNc7V9WTpoKjFb0hdC2EpofR9TC6Vj3XjEj1uhFGN6JoRhjDjKCZUXQzgm5G0YwQaj/04VVKEQwGCQaDxONxCoUCuVzOr6J1HIdsNks2m8U0TaLRKOFweNbG1ZUI8fLRsMuTKlg82Zfh8d1ZHt+d4Ynd1Srb/kxp3P23DOTYMpDjdw/2+MsCerV37eEL4hy+MM7hCxIcviBO5yz2r50OMxSibdES2hYtmXQ9q1wiNzxMdqga4GaHBskOD5EdGhxrpzAyTDGbmfI5y7kc5VyOVM+uSddTmkYwGiMUi/Om1g52Pr6EXz3ewYd/HuZpC4I8f1WUK56zhmhrEt3Y86rkcsXhLd+7lx++8ekcs6SFTNHimA/9GcuefjAbrHl90jVFxd7XHYWqfyORwFh1sbfDovGv52DbISOEEEIIIYQQBzMJZsUhSVOKsKETNnQITh3SuK5LxXXrgtra4LZkO5SbXC85LmXbYU9iF1cL4RLCIbkH957oQSsot4hyS9Xg1i3615VbRPMuO02WudWAVzkjKLuIVimNu3/1VG4e/roaGgF0gmgE0VW4GvhSDXt1FUIbDX51LYKm14TAfvAbQdPDGGYUzYxUl5shNCOCZoYxzDBKjYVemqYRjUaJRCKUy2VyuRylUskPpSzLYmRkhEwmQyQSIRqNzmiysOlqCZuctLyNk5a31S1P5S02D2R5tCfDw7tSPLQrzWM9aUoNlZxl2+GRnjSP9KThvrHlbdEAhy+Ic9iCOEcsSHD4wjhru+KEA5PvmNjXzECQ1u4FtHZPXoVbsazRoLYa1votFUZqWypUe+RO1UbBdRyKmTQDeZtg726C7gOsQ+POpZcQvvNX/LTjHNxff564nSMUT5BKruCwpMEGYzkDdpL7/tRDrLX6+8kM9BPs6sBo0pe4VLGxHJeFrdU+19+5Y2vd7bGgQaZY8VsZxEMGmeKe7JqB01e38+N7tvP2Z69l+1CeO54c4BlTVM2WbYdf3r+TF5+4hPu3j9CfKXHEwjiD2fr/v3jI5IhFCX76rx28/OSlPDWY496nhvjIC5v31RVCCCGEEEIIMfskmBViGpRSmEphahBlZqGXF+qW7LGgdizcrV63HJey41BxXCzXxfIuj16vOA6zUlSnDFwVwyWGs6+yO9cZDWrrg1vNX1aqhreTXs6g7AFUpXZ57XoN69dE38rVRwNfLwAOjlb8BqvBrwoCJhBAqSBKC6KpAEoLovQQphklEIwRCMUxzDCaMRoIm2EMPYymjT6OXg2SldrzILclYnLCsiQnLBsL3yu2w6b+HA/vSrGpP8vWwTwbezNs7s/iNPwNDOXK3LFpkDs2DY79/ApWtkc5fGGcw7oToxW21ZYImnZgVUMapkmio4tER9ek67muS7mQrwtta0PczNAQudQwxUyabeUEt7U8DZTCRXF49nEWF3t4+vDd/K77+TgoHKWxIrON4LZ/sCuWYyiynBvv/3P1yVa+hW9d9RYCboVwPEE02eZPXhZrayOabOOywyK84HM3s6glxHOPXlQ31jc8cyWXfO1OgqbO//37KbzohMVc/ZMH+N2GXn/yr8k8//O38u3XPY3uRIgPXnAUV/74fn6/oYeVHVHOmEYrg2TEZOtgnheuv518qcIXXnE8kYDBIOOD7c9ffDz/9YsNfPO2LSil+J+XHFvXX1aKZ8VEHrr5r2z+191ceOV/AXD7j2/g8X/cSiAS4dKPXzfjx7vjJzdw6ote7lezP/HPfxBLtrFwzWGzOu59qXfTE9z7u19y/tuvmeuhCCGEEEKIg4hyG6cxF3Xmyyxv4uDnuNWgthre1gS3jkvFdcZua7i94laXV2ov++ezFPjONT+0rT+fWRjcLPRtvK3mOt6h4Ga1CtgPf6vnulYNbqshbjXM1fRQtfJXD1UDXz2MZoxd1vXQWKVwTfir6yHKtsnmAYuNu3M81pNm4+4Mj/ZkGMiOb4fQTCSgc9iCakjrtUI4fEGClsjeTzZ2IHEch1xqhNRAP6n+PjKDA2SGhyikRiimUxTSIxTSKQqpFI5dmZXnDMUTxBoD3NYksWS7H+xGWpNNK3Bn4qofP1A3kVij7UN5LvzSbdz3gefu1fMMZEuc8T838thHz5XWBqKpxmD28696Mf9x/beIJPas9dO1F7+A//zOTwiEqjsG/nj9Z+letYYTzr1g1sY8GxzHRtPm9ogEIYQQQghRNV/yOqmYFeIgoSlFUFcEdWCGVbuT8Sp6bS+wHQ1t7YYQ13bHB7uN61Tc5vezHHeP2jlMmwrgqsC+fY5GexIGV0ooq4RyUyi3b4LQt9wQDJdRroVyy0CZNgI8MxrgWatDaGuCKBXEdkzKjknRMsiVDTIlnULFoGSblGyTom2Sr5gUbIPeXSZbtpv82q7eJx6Ksaq7neXtrSxKtrIkmWR5R5J4OI6m7d8etrNB0zTiyTbiyTaWrK1W2zmO408IZ1kWlmVVq3DzudGQdjSsTacopkcoZTOjIW6K3PAQdmXyALeYSVfbKGzbOul6kwe41fPZCHD3xgPbR3jbD/7F25+99qD73Ys9Z5VL/PH6zzGwbSu6rhNpTfLS932UR265kfv+9Fucik0gHObZr38zHctW1N33+++/mopV5icffR9LjzyGc173pqbPcc9vf8Fjt9+C49jous45r3szC9cexl++9iUAfvD+a1BKcdL5F7Hpnrt4asP9bLjxzxz/vBeQXLCQm77zNRatO4JdGx/BcRzOfesVLFi9dtzz5EaG+evX1zPS24Prupz4/As59jnnTvizP3TzX3n0tpsJhiOM7O4hGIny/LddSaKji4du/isb77iFSKKFwZ3bOed1b6KUz3PbD76L49iEojGe84a30r5kGdsffpC/f++bvOqTn9vj34MQQgghhDj0SDArxCGutk3DvuSMhra2H+BSFwh7t3kBsO0Hu4wtq729YV3bYezy6H3nXRgMMC60HX+KumVilMeFvPjrFFFueuw+VNcjX2Znrsyu7TWP57hogKE0DE3DUF4l8GhVcE1Vr1cRrOkhdMPrDzxaGayF0PRgTTXwaFuImgphTQvsVWuIiWiaRigUIhSq9oX1g9polFhrEssa3wP2jq0pvnNHD6yo7rxwHRvXcXjpcjgpnK5OajZcndQsNzxMbnhw1gNcL7T1Atw3L28nmgxwzffv5qG+4rheA7946+l7XS173NJWbn3XOXv1GOLgs/X+eynlsrzuui8DUMhm2PnYIzx2xy1c/KH/xTBNdjz6EL//4me47NNfqrvvJR/9DNde/AJe+dFP+xWvzRx55jmc/IIXAbDr8cf401c+z2uvvZ5/+4+38eBf/1h3/+0PP1hXMbv94QcZ3LGN577p7TznDW/lgb/8ntt++F1e+r6PjnueG7/9VdoWLeGFV/83+dQI//fed9K5YuWkbRF2PfYIr/7UF2hbtIS7f/VT/vq19bz4vR8GYOdjj/Dq//08yYWLyadG+NZVb+XlH/gEnctW8OitN/Gbz/4Pr732+hn8bwtx8BoevpMnnvwkpzztV+Nu68mX+OdAmguXde7Vc9zaO0x7KMCRrZO3/pkN9z/wetat/SCRyPJZf+xfPdXH+Us7MQ6wNlJCCCEOPBLMCiH2C00ptP0QANdy6gLe6YfBFcfFsm2K5TKWbVNxwAFswHGrl12lcJTCqQ2OXfZ9UKuCuCq4/wNhz4TBsDVaFWyBZdUsT6Pc/oYQePzJC40110XHRVdgoKEr0FEYSkfXNHQ9MBYI60E/CPbC4Gd9eRF/fXOeWCjot4IYawsx1h9Y10PE42E0raWuovabd2zjtw8PYGiKgK644qwlHNkdpT9r8fG/PsUPt5X5udHBqo7lfPI/jqU9Xg2SNvdnufJH9zGUKRLRXd5xpKLNGt4nAe6C0VNjgPvPn24ercg9cCpwxcGhc/kqhnbu4K9fv54lRx7NqhNO5sl77qT/qS18/31X+uvlM2nsyp5NZte3ZRN3/eLHFDJpNF1ncOd27Irl95WdSnLhYr9CduHaw7nnN79out62Dfdz1qe+CECkpZW1pzydbRsemDSYXXT4kbQtWgLAsc85l3/85Pv+JJGLDz+S5MLFAPQ8uZGu5SvpHK0aPuKZZ/O3b36F7PDQtH4GIQ509mCWymO9BM9Ys1+er7JlAH1RKyq4b76SOk4FTZv4sY8/7pv+5dJtTxB4+mqUPr0Pqo7rok1yZMkLl0/eP18IIYTwSDArhJi3ZiMMdl2XQqFALpdrqKx0ARfDMIhGo4TDYTRNGxcGe4FtYxjcWN1bXRc/KLZrg+PRZU7jbV4oTDUs3i/mPBiuNLR3sFCVMsoq+9dvGzQIGQUUqYbg12oSEFtoOGiuS08qxg33nMxHn3MrEcPmH9sW8Yk/L+ejZ91NuhjiBStiHNmVRdcNvvPAKt77w8d55xnDmGaYd/2hkxccpXHhsUFufELn+nsqfO81a/zqYq9iWFMByoUK+eGUP3lZdmhwv1bgVkNbCXDFmNbuBbz2uuvZ9tCDbNtwP7fc8C1WHHsCR5/9b5zx8lft9ePbFYtfX/dJLv7gJ+letYZSPs+XXvdy7Epl2sGsEQj4lzVNw3HsiVduzEr2oi2HOVppD+C6zR9L2n6Ig93fblzN6lVX09fzB8qlQY4c+hRDw3cwOHgLjyT/H0fHS/SWY+StRXQah3PPQJrt2Qy5/HaONv+FlforabWUSvKdQCcZq8Jvtg2wJhFmd6FMrtjLEcGtHLe8WgX/xBOfQNejLNr2PB4v/ZhH1PFYhDAqPWh6lGTn0UCUQsXmjr4Uaav6fnhka5TDWqL89o5XUum8hiXpT1KwijzZ/hVO72rlsJYov7rr7QQ7LqUz9Xm2xq9mcWIR24e3UiJE1HqYU9o0Fi9+JQC333Emxx37NWKxwwg+Y2113Fv7/HFXXJdTO1tYFAn6P9ORrVF25Ussj4VYGg1xR98IRdtBAce3x1keq+6w/dYTu3jV6gWY2n6sSBBCCHFQkmB2AuvXr2f9+vXY9iQf/IUQ855SikgkQiQSwbIsisUiuVwOx6lGoZVKhVQqRTqdJhKJEIvFMHV9v1YGQzVA9oLaceHtBIFu4/KKFwI7DrbjkitVSBUtCmWbsl2dUM5VoOsKXVdomkJpCqVrqP31xUMZuMqoC4ZzqREe/PuN5FMpsg8PM3zRa2mJJ/nrP3cxmCrxkrNXYBpTj6+fPGW1ic3xK4mFTbb39hNIZHi87Wx/nQ2j5+bCfp54bJB/aieQyxR4tL+fi7pi/GV3GWIltgy53PDwZtqiDdXBfiBcQXNttKiDFnXRl7qEFEQIo6sIulpSnWCuYoNtY5fKOKUSlWIRK1eknMlRzhYppvNUyi6ureFUFE5l7NytKBxbo1waZnBHioFtWxifWI0JxRPEWpOjE5aNBbjeBGYS4M5fmcEBQtEYa04+lZXHn8iT/7yTI595Dn+4/jqOOed5JDo6cR2H3Vs2Ne3rOpVK2cKxK8TbOwC474+/qbs9EA5Tyuf8VgaBcIRSPr9HP8uyY47nwb/+iTNefin5dIon776TC658z6T32bXxUYZ27aRt0WI23Phnlh59bNOwddG6w/nzVz7P4I7ttC9ZymO3/51YWzvR1iRDO7fv0XiF2B+Kf9iAsa4be3cayhWMo5fgDGZx+jOjexxAN2KctO779D78Yx548E0ctfY6lqTP55EkpLb/mecWz2bjkhT/ir6ZZ4cDHBV1+eNDv6O342U87+j/YONDt7O7sI3CP4tYWphSIkAyaHKS5XJ3/y/Z0H4Ja257gtC6bgCcoSwUKzxZOhbDuoMXrLyYcuwYfvHUTvo2/47So6dzR3uYlvYYz17URaFi8+vt/bQFTYKl+xmmjeMO/x5P3P8A20uPsW3bMlaMuBTjJ7NA9XLC4d+md1MvPaWHOGboJlYseSe/Nrt4eOt7CG6KE9cOw3Wduv8j96zDKDkOyaDJKZ0t9BXK3NgzxEtWVKtfS45D3NQ5b2n1tew32/pZ1xLhsJYoqXKF323vpz0YIGbKJIFCCCGmT4LZCVx++eVcfvnl/ixvQghhmiamaRKLxSgWi+TzeUqlElANRnO5HLlcDtM0/TB3f1VSKaUwFBiTBG+zIVO0eKIvy+O9GR7rzfDAjhE27EhRcVw0TWHoCl1TGLqGoSsMQ8PUNXS9uixgaHTEg3TGgyxpDbEwEaQjHiQZNtE1Va0mHg2Gx1Uf42C7TjVIBjLZDN/95Me44ze/JNi1HD3WTnG4j2ue9SUWPu25nPTqd3Lxcw5DKcXmnWn+fPeupj/T2qUJnn3yIha0R3j60V18/kcPEw4aGLriteePD6Ecx+Wfj41w2LI2HL2N4VKeWDSNE1pDeXSdRGwjfZV1hCOxfffLGK0eNvzQ12roJdwweZxbRjkVlGOBUwHbqp4qFbDLuJaFU7GwrCGGKrsZtGzotXC2W7iWjVuu4FgVDGUQMEMEAxFC4QThWCuReDuRRAfRli5irV1EW7sJBOOjfYOlmvBAN7BtK7d+/9vV4wAchyPPPJslRx7NM15xGb/69MdwHRvbtll14tP2KJgNRiKc/rJLueF9VxJv72T1yafW3X7S+S/iJx95H0YgwEvf91GOfObZ/PHLn+PxO2/zJ/+azHff9Z+8+D0fItbWzjmvfSN//fp6vnPN23Adh1Nf9PJJ2xgALDnyaP7x0+8zuGNbdfKvy69sul4k0cLz33YVv//iZ3Adm2A0xgVX1Ia+8rcuDmCGTvD0Ndg9Kax/bcU8fhnmYQuobO6HrdDddT5kIKqtQbkaHcmzKFsbAYhqt2Me/WpiDz6KWtjO0miIQgEC1iYsfSHWI7uItRyOqmymtDyN8xBoiQBr4mEIBUiMpAkaFTJHLUG/bxvuEtDbYpA1SIUXc2ygSKh9KcamPrpdi4Kxi+DT1tL7xC5OGshDVythQ2d5LExPvoTmlmkLBugrWwwFOlhcvonexEqMxd0U+07gsM7RPreuSyD3K1ac8l+EQp0kH9pGKH4O2cR22jr/De60cUsW1LxVa4rquIGucICwrjFUqhAxqu2VVo3eZjkOQ2WLtYkIAC0Bg65wkL5iiZgZ2V+/VSGEEPOABLNCCDFDSinC4TDhcBjLssjn8+RyOf92y7JIpVJkMhlisRjhcBhdnx/VE/GQyYnLkpy4LOkvK1VsdgwXRk95/vXUCFsGsmwdzNM/XBz3GBsneOxFLSEOWxBn3YI4hy+Ic1h3gtVdUYLG+P+7VCrF6c94ITuKOp2XfppA1yr/tnLfZob//i0e+9IVfPElNxGJxbAXtfHWk5Zj2w4V26m2hbCd6mm0ergnVWDHtjRfesUxJEIGf3h4N7+/eQtXn7tiNBB2qLgOP7yzj5jhcvZhEdxKHtMuoXDRnAqO0vfqsOkZ8auH9+8XwMroyatnVE4JansHj6RRw4MNwXClWiVMBeU66Mqp9g/WtWqQr6qtR3SlMDQNXdMwlI6h6eiaianpGJqJoZuYWgBDD2DqAUw9hOH1GPZaRmhS0TtTK084mZUnnDxu+RHPOIsjnnHWuOVHn/Ucjj7rOf71q3702ymf45QXvpRTXvhS//rTLnixf/n0l13C6S+7xL8eaWkdN6HWqz75Of9yx7IV/MeXxnpDXjbaUxYg2prkhVf/95TjqWUGg5z/9mvGLW/8OQFWHn8SK48/ady6+XSKcDw+o+cVYn/SF1YLTVQiBCj0rsTo9WrQqGlBwEWhoWmjrUO89zOniJaMoAwN5Y612VE4uLg4Qzm0VR3QAypooiXH3pfcQhl3sIDbZlF5vBfKNo5VAGP85F727jRurAKWQ+m2J6AzAoXmfa0XhBQ9RYu+sMEKZzs5vcBmQ6Hbw0RKC/DeGpWyQCncio2yHIgGgSJaNAiawkkXoX3y/zvvXd1Qmr+zcbTQuMnuGNlBI4QQYmYkmBVCiL1gmiYtLS1EIhGKxSLFYtHvRes4Dul0mnQ6TTAYJBKJEAqF5l0FYdDQWd0ZY3VnteTk0lPHZjcezJZ4eFeah3el2dyf5eFdaXpSBYbz479o7UoV2ZUqctPGfn+ZoSnWdMVY1x1neXuEwxckOHxhnA+/60p2FHVaL/oASq9/Kwt0raL9xR+k5xcf4YPveTff/PrXuO2JAT7++0ebjv+cwzu55nmHc+ctmzhpRRvnHrMUgOOXd3LUB//EOetWoI/OqvzBXz1EUAvynTedTGC0PcJAtsT//WUHl6xeDK5DIV/gi/ky57YadBs2juPiOmPtJkBD03Q0w8D1W0042A7VNhKuTcWtUHHtahiMVyXsYKvRSegAW4GjNBylYSsNR+m4SsNRBq7afzsCXC0IzHLfYe/o0ul0E3JtlJuqqxpWroXCRnMr1XNsNOWgUQ2FNUBX1UnmNKUw1Gilt9LQlYauG6PBsIGhG5iaieEHwiaGHsTUggSMajCsa/Njx4vYM/f/6Xfc+/tf8uzXv2WuhyLExEbfx1Bq7DLsVY7ouhaWNQJAPr8Z160Qix1Oin4cYFOmwNL7t1PpeBolM0D301ZSvPFuBkduYWG4unOmld1sLYY5ojxAVlP0hcIsC64leNhaFvUMsaUjzglAsWKzLVvk7IVJ+gEj+zc28hxCTols6k6WLPwv7h/MECrdAxzh/6zx+BHs2vlDVi79TxwsMpmHSK59+YQ/k+NWx70mEaG/WKZgOySDBkW7vpt/QNdoC5g8mS6wtiVCulyhr1DmtE450lIIIcTMSDArhBCzwGtzEI/HKZfLZDIZv80BQKlUolQqoes64XCYUCiEaZrzLqRt1B4Lcua6Ts5c11m3fCRfZmNvhsd3V1sibOzNsHF3hkyxfsKriuPy2GjbBI9dzLLzezew4NWfHhfKepRuEH3ma/ne997FdZ/5NM9Y28Ef3vHMSce6rC3Cz/+1k1ypQjRo8NdH+1jTGfND2Q/9+mG2Dub56mUn+aEsQEcsyJGLEvzy/l287OSl/GXbAMvaY5xy9AqKxSKFQoFyuezP8l7z02GaJqFQmGAwuEd/D67rgu2C44Dt4FabCuPaNhW7WhXsjFYGey0ibNsLgl2/dUTFsai4FrZboeLYVKgJhWsDYTUWCNtK+cGwo+k4jIXCzmg1L2o/9B5WOq4K4xLeu8ep/c49+bxro0qjpxS41mgYbI2GwdXqYA2nGgpX/3fQlDsWCMNoGKyqYbBWDYUNzcDwKoQ1o1olrJsYWnC0Sjg4epuGPlplrKkDYwKqzff9k9t+8N1xy0+56GUcfvqZczCiMd977ztxGuYNaF+yjPPffs24qtiZOv5553P8887fq8cQ4oBU877ljOTBqt+GDCOO45R50Hw36e0rCXVehVY2cEby6BGN4YLikfYwlfC/sSzzFe69406Cbjvb2z9Hm3s3ytBZXbmHRyOn87Mt2zCj24iXInQsqPZ2P7Utzh27R/hltg8XOLYtRmeoWs3bouUolLKESr9jTcdVRFu6uHekj87iPaj4f/g79hYveiXZbR/g7n9dQDr0RpYEn0FL4jicXAkcFy1R/94R1LTRib76qbguZy5IYmrauGAW4FkLktzRN8LDI1kUcEZ3i/SXFUIIMWPKHf9NUdTwesymUikSicRcD0cIcZBwXdefLKxQKDSdSNAwDL/VwYEQqsw113XpSRX9kHZjb4ZHdqV5sj+L7Yy9VWUfvonMP3/Jwtd+fsrH7P3OO/nsR9/HW97w2mk9/6f+tJE/PdxLQNeIBQ0+dOFRHL24hXu2DvHSr/yD1Z1RAqOtFZYmw3z1surh35v6s1z9kwcYyVvEggbXvvw41nWPHdbsOA6FQoFisVgX2NfSdZ1QKEQoFCIQOPB6s7quWy0lsh1wXNxqiW/95dGQ2LVd3IqNbTuUSyWKhQLlUhmrbGFVquGv7VSwlY2tbBwNXAMcXeHqCkdXOJpLZTQItpWLoylspeoD4dFKYcevFDb88+opMNf/bfuH6zQJhe2aCmEHTYFONRTWtdG2EUr5bSN0TR9rGaEZY9XBWgDTCFSXKVW9/2ggrGsKjQMjFBZCTKz4hw0E/+1IlKHj5MuU73iS0HOOBMAezFJ5rJfgGWvqLnvr6cvbq5OE2Q7GkYvQ26tHx6RuvZUH3Hdy5pn34hbKWA/uwLVsVDRA1nH4YyLIJWsXYe8cpvLEbgiZaK0R7J4UgZNXoMVDVLYPsfnJz+KoEutO/DAqFqLy5G6c3Wm/jYKxsgN9cbLu5/nbjat51pkPopXNCcfY+HMCOLkSlYd34pZtXNfmTvtinv70vxIMdlL8wwbKZx3Gb3cNccnqBXv1/12o2Pxk625evXqhvD4KIcQ+NF/yOqmYFUKIfUApRSAQIBAIEI/HKZVK5HK5ulCuUqkwMjJCKpUiHA4TjUYxD+EZ75VSLGoNs6g1zNmHd/nLi1a1h+2m/iyP7Erzi95buDfRMa3HjLR2MLR7J7t378YwDEzTrDuv/cKklOLd5x7Ou889fNzjnLyija3/M3FF3OrOGL946xkT3q5pGtFolGg0im3b5PP5urYXALZt+xPIaZpGMBj0g9oD4YudUorRVK96fRr3MYEQMNXHJNd1Keay5IaHyA4PkRsaIjcyjDMygpVOU0pnKOVyWPk8CoWhV/vNBozquWGMnnSz2obAv2yimSZaIIQydZSp4ZoamAo3oHCN0SBYc6loNrZyqCgXW7nYmhcKg+MHwoy2jRgNhDUdB92vEnaU6QfCrjKrwfD+aCuhNFyC2Co49bouTVtEDP33G0m+/wuoYKhmqTV6yo2/g/94DpRyZH74NSq7tgGw6L0fQFMOOi7KKrPrG9+ASgUFmIk4y17yYqLtHVAq8/D661FKoQDHssgP9POC675OJJHwA+Att9/EbV/7Iude9T5WnXjKAVcpLMSBLvT8Y/zLWiRQF1bq7TH0M9aMu+wx13bD2u5xjxk4eSXcM7r9BQzMp61EaQq3aOHctQlaqq8l+uJkXbBqHj42mZ+xtA3D6sC2c37lqrluAaybWTDabIyNPyeAFg0SOGUVpdJu/vWvV7Cg6yKCwU7cUqXa3kHf+9eT/mKZm3uGOb4tLq9PQgghpkWCWSGE2MeUUn7A5jgOxWKRfD5PuVwGqqFUPp8nn88TCoVIJBIYhrw8e0KmzpquGGu6YjzvqAV09R3L5Tf+clr3tbODtLa2YtvVWeUbq1V1XfdD2okC29mm6zrxeJx4PI5t235v4tqxeRW2hUIBXdeJRqPzahK5RkopwrE44VicjqXLJ1xvXIDbcJ4d7qteHxnCtppPGDPBCDB0A90wMXSjLuyNxFqItSSJJVqIxFoIx+KEImFC4SjBYJhAMIxpBlAO1Yph2wHHwak4uHaGilPCcopYlKstI7CoUKGiHCpa9dwPhDV39Ppo2whNNQTDXoVw7cmrFB4Lg6un4GgwvI9fS5SGa4QIPfP5qEiMzLeuo6Da/Jtd3SHyunf5gW/xjr/y+K/+TPySaj/W4Fs/6K9buO3PGFse5/ZsBbJDADipYbJ//B360lXctjvF3Vt2jz23645VB6uadhGMtYwYq+7VMGqqg712EaY2+jvXtNGKYIWhjVUGe/2H/ccZrRqWwEUc6sLhJZz5zHsAcHMlrAd3VG9wXFpXdXHJ0rZJ7j1m1ap3NF2+ecsX6e//07jlxxy9nmefs6n6VKOfo2bq3pEg/Yt+RD/w2OZe3IKFWtXGCwxjr6tlO0MBXrZyfJAthBBCTES++QshxH6kaRqRSIRIJEK5XCaXy1EsFv3+o15AFwqFCIervUclAKh33nnnUfj3NxDp20yga9WE65V3b6Y4sINzzz0XpVSTHq/4gW0tpVTTwFbX9Vn/XXihazQaxXEcSqWS/zfgOI4/Rm8SOdM0CYfDRCIRNG0/9G89wMxGgDsW4tYGuC4V26JiW4xrNNG/fVpjC8XixJJtRJNt1fPWJNFkO7G2NqKtbf5thmk26Q1c3wbCC3f95XbNek7NehXHby/hOg5uxcG1S9hOHtsZwrGLVNwSFVUZPdmjYbDNUC7LzXfdTyqbIxoPo5ctThz4M25AZ8ftD1MczrLo/FMgaGL77SLGwmCnpl2Eq4KYy2JURnKAi3LyfiisNA1GQ1nXdXGLxbGZ3huU/3U74edcVLcs96v/I3Ley8n/6efN/iBwMHGoOdLAbTiflFcRPHPa6KRyut87GP801iJC80Ng02sRoWmjQS9+EKzXhL/+Mq3+dkOrVhXL+4HYX7yK0289sYtXrV6AOcl7jpYIE3zG2nHLh0oWd/alKNoOSkFXKMBpnS1+7/b+Ypk7do9QcV2ihs6ZC5JEDJ2S7bBBuxi6LwbAKlpkFby4P4/+rzK7jE3cGw9gx4KotR08LV9iUWQaRwyMOr27dWb/GUIIIcQ+JMGsEELMEa/VgeM45PN5crkctm3juq5fLWkYBtFolEgkIl/IRyWTSS659FJ+fsu3MS/6QNMJwFy7Qu6273DppZeyevVqXNfFcRwsy6JSqfjnlUplXGDruq5/Wy2llD/JmxfYGoaBpmmz8rvRNI1wOEw4HMZ1XUqlEtls1q+sBrAsC8uyyGQyRCIRYrHYvK2i3RuzGeDmRqrnU1XgFrMZitkMA9ufmnS9aQW4bdUAdzb4IfBo2JsaHuHKd13N93/yIxa1tZIIBUkVSuwcGOAVKsF5R65hUfsyznzVxSgU2554mNv++jPArQk9q5eXLzuCU572bBy7hGMXSWWG+ZXt8IItG3AoU8HC0iwqyua2399GejiNGQrwtAtORN/149GWES4VpUj3jpDODrKs63Gc4U04SmPo/u1EWoq0tj5GyR4iWHqAUCGFS3BcZfD+rhR2qLaxqFD9r5k8CHYYmyxub7jVZ1Uu2mi/YEPzKnu10argakWwrnTM0bDXOzc0DXM0BPb7BY+rBlYYNaGwvO8cGryerYYRnfF9s9mNbNz4QcrWIEoZtLScwGHrPoimBdGV4uhIDz2b3kvFLrK75SoeUCdwYtcCXNfl7z1DLCv/isrQjxiOXMBfssfywsOfTVDXOM79KT29Pyef30ps3U/JaEtJrFuM67rcunU3Z3YnWRgJMlK2+NPOQV6yvBtDk79XIYQQBx+Z/GsK86WZsBDiwOc4Dtlslnw+71dLeryetZFI5IDpOTqX0uk0Tz/jTHYUdaLPfG1d5Wy5bzO5W7/DklCFf9x+y6Sv3a7rYtu2H8ROFthORNO0ca0QZjOwrVQq5PN5SqVSXU9aTyAQoLW1Vdpf7EOzGeBOVygWJ9qaJNbWPhbgtiaJJtuItiSJtCaJtbURCIWnfrBRqVSKZ55xBpXUEM8/cg2LWse2jV0jaX774GM4gRD3bnhoxp95XNcltbuXG953JW9d/926CeFcrzLYsrn7z78kmxrmnBe+unrbaGD8t199l2AwzBnPuqgaIA8N8Idff4OXvORtGMrk579cz/HHPJPli1fjuCVsSjiUcdToec1lezQUtlWFimZXK4VHewdXtNEewsqlonl9gxW20rC1+hYRjmbgUhv+1p43C4eD+6en8H6kjQbBtdXAhqahKw1T10dbQ4y1fDAa2j8Yo0FvbVuI5usotEP8fW1fqp38q5naYHZ4eJjf//73DA8Pk0wmGTn8RF5jBTDKNvfHA2TCJmetXoBpVh8r2/so5ce3EXVXQEjj8dDnibccyYoVb6V012buyV3GGuOdtAZO4K62TaQcmwsOfz59/Wlu7N/KsX3fYpX+FjhiIT8eyfCq1QvRlCKVuh/TTPLoY+9lc8vHOaV7ActjYYq2zY+39HHZmrFetb94qo8T2uOsiE3/9VAIIcTBb77kdfItbgLr169n/fr1TWdSF0KIfUHTNBKJhD9ZWG21pFdBWSqV0HWdSCRCMBjENM1DMqRNJBL84/ZbeOeVV3HDDe8m3LkUPdaOnR2k0L+dSy+9lM9dd+2Ub9BKKT9IrVVbNesFtpZlNX1PcByHcrlcV9kK9YFtbWg708DWMAz/56hUKmSzWQqFgh8cl8tl+vr6iMVixGKxQ7LFwb42kwrcUi5HdniwPsAdqU5oticVuIM7tk26XjASJd7eQaytfbQKt92vuo2NXo+0tKIbBldecQWV1BCvPe149Ia/k0WtCf79GSfzjdvu4Z1v/0+++e3vAPDUg/fz9+99o+lzrzrxaTzjFZf5/0dK00ApVMiccHK44150Ed94xxt57juv9JdZxSJPPHYvl3z8OgKLlwIwcNtT5MpZvv+z6wDIjQxz812/4oyLX8UxZz+vpt3DWKsHHLcmEHZwndo2EGOtI/xAuPYxKk3u49g4WDiUcChhqyI2BRzSOMoLh8cC4golbKqtImzNqfYRVg62BhWvb7BSVJTC0dRoiwhVDYAnC369CmEtWHNbEMevDp7+Idwz5aDhuGCN20/ljJ5mZ0cEgPJ7A4+2etAUhtKr/X81bTTEZbQHsPJD4rHr+G0havsFGw3n1YnjDr33zVqN4WssXm2Zc82738kN3/sui1evom3hcoZ7e9j+5OP88+Wv4KXv+yDtra2cuTON2jbIroUt3N2fws3FUEuOQ+kabrlCufJmWkqbWAEU16bQn4jRfepLKfaO0DdsEk59BNt+NsNP7oDYFtY942NoRZ3yP7diLmslX7GJmQYtLccDkNOWY7k6S6PVlighXSesa2zNFFgRD9NfLJO2KmQt+c4mhBDi4CQVs1OYLwm8EOLgZFkW2Wy2rudoLdM0icfjh3Qv2pGREX7/+98zNDREW1sb5513Hq2trfvkuRzHGVdZW6lUZrQTz2uJUBvaGoYxox62XhVtLperq+z1wv1wOHzI/j0cDGYzwJ0WpSAY4r3f/wX/ec7pdZWyjXYOp/jSTf9g25atLFiyZEZPk+rbzff+6wou//r3/WW5kWE0wyAciwNw7+9+xeN33sYrP/ppf52Hbv4rG278M6/8yKcmfOwfffg9nPyCF7P6pFNmNKa94bqu38PXD3eb9P/1e/829gJudrmhirgaElcaAuAyDkVsitiqMFYR7K2jvMtlf31b2VjKxlZOtRpYOdXJ4zSjWg2sNGxl4moGjgpP2AKiPhQO4qpQXRjsqiCuFtpvv4N9ZawVRE0biJrg1qip7B23rKFCWK9dp2H5XE0U1xi8Pjuwgu4XnUY6l+WKN7yJH/zm56xYGaWjQ2dgwGbz5hESrV0sWLOGl777Qyw74ij/sbY9+jA/+dRHKQxu42vrDyNiKnBcOpf8Gyvb34q1YQfBZ64DoFLKcPdt57PmmP+mq+u59PX9kV09P+bYY7/BXzfvJpotYadfytOO+xlPPriNR1tTnBh6mKHh21FZl8eXfIkXrlhEzBzbWfqrh39OMr6WM5cd4y8bKlncM5CmYDskAwb5is2yWIgjW2P77z9ZCCHEnJsveZ1UzAohxAHMNE2SySSu69ZNFuaxLIuhoSECgQDRaPSQbHPQ2trKJZdcsl+eS9M0vzdwLS+wbQxtmwW23u+yscLWq95tDG2bBbZeFW00GiWVSvl/E47jMDIyQjabpbW1ddw4xYFBKUUoFiMUi82oAjefGiE3Mlw9jQa6mcEBMkMDkwe4rsu/Nj5JVzw6aSgLsDjZwsLWVj74xtfyoa98g1iyDU3T0HW97rzxb/L/3v0OciNDlLJZ/t9bXsPSo47lvLddRXZokD//vy/iODa4Li3dCznvP6+qu+9DN/2ZY87+t6n/4/YzpbwZvarVxfvqldUPgBsD3LoKYLcmEG4W7o5VANddrzSs59pjgS6l0fYPta0hctgMjQbAJWyKNQHw2Lq2crA0Z7QC2K2GwGrsNFbdG8BpFu6OBry1AbDTbB0V3Cf9gh1UtRLYrl6rnmafgtEqX23C6t2myyZcRxu3Tm34m06nuerKq7jhhhs4YsVqFrV1sXNwN/+xdRMve+lL+dfd/yDU0s/nv9jF6tVjldYf+rBLf3Elb//a98f1t152xFG846v/x+ffeBnf+/5SvnzZuzAOX8DueJBf9wxDZwT1VB8uLsXiDtSCzxN0VtA1en87bfG3DdsI2S6nre7m9gdABQzClChrrUSja1jV8Tb6776Jh10HwxkBOgCwHIcR83iODe6qG1Nb0OS5i9v96z/f2kdrYHb6cgshhBD7mwSzQghxEFBKEQwGCQaDVCoVv9WBF/x5QZ+maQSDQWKxGOYsTR4kpjadwLY2tJ0osPVaJtSqbbdQG9zquo6u67S1tVEul0mn037YW6lUGBgYIBaLEY/HD7mwfr6YSYBbzGaqlbZD1RA36/fBrV63d/XTGp5e/8XWWIRlZ5yNa5hkMpmm63ghrfe3+KL3f9wPbb2WHQDdq9bw6v/9/KTP94oPT1wp67n4g/8zrbEfjMYCYMDU91kADFRbNTjjK4DrwtzptIFo8hhuxQuWbVzHmiD89QLfMg6DdYHv2G2luvtW20M4VJTC1kbbQWheC4hwTVXvWPXvWMg7epsWHr0tVBMUh0ZPgZoQeHZbwbhU20BYtgP78Eh7QymsXIZPvPIlLAvHufkz3+XYVYf5tz+4eSMv+chbaVuY5WMfX4BhjP2VZTI2d9+V479+/NEJJx00TJOXXfM+PvGKF/LyF/eQfErR0X4WF0T/A3vHMMa6bh566D8xzSQrel5FaFV1ErFAcBFb2l7Ggo4WTis7lDZuxrazBAKdLD4c3P6dlLcdQyU8SE/30cRL91PIdxMKVoPZrdkiIbuHuF6/IzNfsYmM9svdmMphaIqFYdkRKYQQ4uAkwawQQhxkvIAuEolQLBbJZDJUKhWgGgQWCgUKhYI/WZgc1j53phvY1p4aTTewjUajRCIRcrmcv242m6VSqZBMJuVvYB5TShGOJwjHE3QuW9F8pRtu4K6rr2x+W4NUoThlOxDHcXAcp+mEdB4vqPVC3Nqq29rb5G9z/1GaAk0HYx8HwOPaQDhQGTuvb+1Q3yu4fnlDIFypqSbGHg14q20fHFVsEv6WcBjGVr2ThL/V+9kUqy0glEtFA1tpuFrAD3AdL8zVQjS2eaivCh67j6t5QXCobtlsq7gu//fxD7M0HOdXH/oSplEfsC7tXMhAaoT3f6yrLpQFuPuuPItWrahrX9DMsiOPZvGqtfxx43lcfdLLCOkamXwBM5Nlw/1vxdDjLOp6Fzy1g5xVQjkWu6ylZAIdkB3mt3qEcsQhEv0UrqsRbFvAip3Xc0/n60ELEMhmSQ6vJ7Lue/5zPpHK02bdDRxXN5aNqTybM3lcoDVgcM7CNnkdEUIIcdCSYFYIIQ5SSinC4TChUMhvc1AqleomhfIqKSORCNFoFF2fXzOGH6wmCmxrJx1r7GPbaKLAVtM0f7I4TdP8Ct329naZGOwQdt555/GGf389u0bSU/aY3T2S5hWveAXxeBzHcbBte8Lzydi2PeU6SqlJg1vvsoQuB5d93QbCdZv0/m0MdSujlb2VmirfilNt7VCZ5L4VL/h1cbFGQ9ziaO/f0ct1AW8RW2VwGGi4rVhTETwWGldUCVuBrcDRTD/gnSj4HQt2vfYPjctDZDMW//j1z7nl2v8bF8oC/PGeW1m+LFzXvsCTyTokFyya1v97srubxPBuCukURaVQgG7fzMjIjQTMVTy04eWooIv5+DF0Jd9O3IXTncfo230d2CVClSQdqz5IT08PWsVlactFGL3X4JRG0GxF57I3ksloZLOD7O77NrGBn1KpDPPwSBSlAhxz9I8IBNo5LKxzeCThv34oHCoVtzoJYc1JCCGEOBhIMCuEEAe52jYHruv6k0LVVtFms1my2SzBYJBIJEIwGJSQ7gDkTQxmmibhmsPOawPbxtC2keM4KKXQdd3vPVsoFEin036Li8aJx+RvYf5LJpNccsml/OEvf+S1px2P3uR3bjsOf3x0E5deeint7e1NHqWe67p++Fob1jZen+5jTKaxz61U3x7alFJg7LvK37Hg152ggrdJNW/TcNgdC4Qbgl/An9CttnK3ebhbxGEYh3LT237/t80ctnRxXfuCWiPZNJ0dzXfMxmMaw727mt427nF6e1lWiZDoLdKfrIafwbazOHnoWagi2Dr0dWg4pkYpX6C7z6bFXcJCdR22phju0CmXNcrlHC3DNu3ZIO3uh7ACGiMLTGxXMTIyAkAwcBFLFr8IGOuhm0o5KDVQt6z2vPHyRKfa14tmt020rPb5hBBCiNkiwawQQswjSin/kHbLsvzJwrwq2lKpRKlU8tcLh8PSi/YgUBvY1qoNbEulkn/ZcRxM00Qp5f/+K5UK+XyeYHB8xZTXK7Sxh60EXfPLZz/3OZ5x+ul8+877ef6Ra+oqZ3eNpPnDI09itLRx3Wc/O63Hq22nMRHXdSestm1cNhmvdcJU45HqWzEbxoLffTPpWzX4dScJe2fQ3sF2uK3wXZZ13Drh87XGEvQPNN/5ccqpEa69bivbHn140nYG2x55iF1bN3H8i5/DcMz0v0RWIrAr7I77+So67Fyoj/28AIwG08BgHAbjNTuIXAvqW8nWbauN2+1kt9Uumyy4nez2ic4nC26nWl57vfayEEKIQ5sEs0IIMQ8ppfxD5W3bJpfLUSgU/Ko013X9KtpQKEQ0Gm0a2IkD20QVtrU9bPP5PKlUyu8H6oWutbyQrFQq1S33JnJqDG0lsD04JRIJbrvjDq684gquv+EGFiRbaAmHSBWK9A6nuPTSS7nus58lkZi41cFMedXbU7VRqa2cbRbcepfHAp7JH2MyU1XfeuGt/I2LfaUa/CowtFkJfju3HMbOP/58wtvPPfmZvPULH2DTptK4dgbxuM6znxPnJ//7Ad7xtR82nQCsYln89NMf58UvfjGLFi3yt8OZnE/3tsZtvPZ64+WJXg+UUriu6583Lqtdr/F+E91Wu3wmAe901p0quJ3stmahsBBCiIOLcif7hCtIp9O0tLSQSqVm9YuKEELsb67rUi6XyWaz4wI4gGAw6Ae08sF+fhkZGSGfz+O6Lpqm0dLSMm4CsqmqEWvVBrZeWGuaprREOIiMjIzw+9//nqGhIdra2jjvvPOmnPBrrnnVt5MFt9Opvp2Oyapva6/La6U4EAwPD7N40WJu/sx3J2xn8JYvvJ8NfTfy8U92jJsALJdzuOLKAcy2o3nl+z5SVzm77dGH+dmnP4bKprnl5puJx+N+KOqdvJ0m073euMwzUfA63aB3OutP9FwT3QbjQ9ragHeysHey4Ld22UxD3ckqevcm1G28baIxCyHEgWK+5HUSzE5hvvyihRCiVrMqWo+maYRCIcLhMIFAQD6UzwOu6zI4OEi5XD1OtLW1lUgkUreO4ziUy2U/qLVt2z+fLi+wre1jK4Gt2N9mo/p2uppV3zZrnSCvo2Jf+483/Adb7n+EX3zwi00nABtMD3P0G5/PilWKt7y1ta5ydtOmEv/vKxm27QiQzeRZvHoNrd0LGNndy67NT3LppZdy7Wc+s0++CzUGttMJcmcS9nrPMdH53oS/UwW6zcLfxsrdxspez56EvXtTtTvR7VOFuhOdNwt+hRBits2XvE6C2SnMl1+0EEI047ouxWKRdDrdNIDTdZ1IJEI0GpVw7SBXLpcZGKhOmhIKhWhra5vW/Wora2snHduTwLbxJBWHYq5MVX3rXd8X1bcTtVGQbUHsjXQ6zVlnPos2I8InXndFXeXsg5s38l/f+iz9pQwnnHQcP/7xj1m+PEJ7h87ggM1TT+Wr4eu1n8NxnIOumr7RdMLePQl/G5+j8fmaXZ7q9omC4cbLU902HdMJexsvz0bbhqmC25kEvkIIUWu+5HUSzE5hvvyihRBiMq7rUiqVKBQKdZOF1QqHw0QiEamiPUi5rsvu3btxHAelFAsWLNir3+Pwz35GYePjtLzj7XXB7UyCrOJ3voORTJJ4xSuoPPAAI//7KfRolBU//tG0x5a76260cIjwsccCkPnrXwmsXk1w5co9+rn2JWt3H32f/jSLP/PpuR6KmKaJqm8br8+G6U5cJq+/YiLpdJqrr7qaG264gcOWrWRRexe7BvvYuG0Ll156KZ+5tlr1ejC2Mplr+zrs3ZuK3anC3sbnmOzydM1myNt4eaLbpgpta88nq+QVQswf8yWvk8m/hBBCoJQiFAoRCoVwHIdisUihUKjrRVsoFCgUCmiaRiwWIxwOTzmhjzhwKFWdEM4L3m3bxjD2/GOAUtWwKBqN1i33Kmy96lrvvGlg64JtO+TzeQq//S3GJa8kcM457N69e1wP24kmHcvffTd6MlkTzP6NuK7PaTDr2jaqybZhdndJKHuQUUr5f38TaVZ926x1wlQ7Lbx1LMuadL3aytva4LYxwBWHnkQiwVe/9lU+9elPTRq8tra2cskll8zdQA9C+yLU2xctHGYS5u7J7Y3jb1xe+zrXrGVDoz0JeL3z6Ya6tZ8dJgttJwp2a6/La6sQYl+QitkpzJcEXggh9oRlWX4v2mZvF6FQiFgsRiAQmIPRiZnKZDJkMhkAkskkpd//geEf/hC3VCL+nGfT+fa3U96xkx2XX07o8MMpbNhA8LB1LL7uOpRSZP72N/o+9WmMzk7MFcvR4wm63/2uuudI/e53DKy/HmWamIsXs/T69Ti5HD0f/gilTZtwHYfElVegH3UU6a9+DTceR0UjFNdfD7EY5sknEb7iinFjrzzyCMUvfxnKFnoiTvvHPoYO9Lzq1SjDQE8m6f6v97LjP9+OHo+jxWIs+9Y32fn2dxA+7lhyd96FWyqy+LrrCK5d6z+u6zhset65rPrVL9EiEZxCgc0XXMjqP/2xabj65DnPJnHe88n94060SITFn70Oo6ODp159GeETTiD/r3tpfelLUUox+PVvANBy0Qtp//d/p7xjJzvf8Q5W/uyns/hbFQeL6UxcNpvVt7VBrdc6RMJbIea3mQa73vXa5Xsa9E52udlttY871eXagBcm7rU7nX68tSHuROHuRIGv99o5WXjb7LqEukLsG/Mlr5OKWSGEEBMyTZPW1lZaWlooFovkcjl/AimAYrFIsVgkEAgQjUYJhULywfMAVlv1V3ziCXK33MKKH/4AlGLHWy8nf999GJ1dlDZvZvFnryOwciXbLnsNhXvvJXT00fR+7OOs+P4N1SDyNa8lfNxx455j8Cv/j6XXryewYgX2aAg88JWvEP+357D4U/+L1dvL9je9mVW/+iV2NIrekiD28pfT+/DDBM86G+O0U5v2sNWXLyf6uc+hdJ3y3/7G4Ne/Qfjyt2Kcfz6qpYXIS19CyTQJPeMZxJ77byTOOQd99OdVgQArf/oTRn72Mwa//W0Wffzj/uMqTSP2zGeQ/fvfSTz/+WT/fgvRZ5zRNJT1x9Lewcqf/ZTBb32b/vXrWfjBDwLgViqs+N73sHbv5qlXvZqVP/kxKhxm6ytfSeTU09DlUOFDmvdFfTrVt5P1vPUuT8Zbt1KpTDqe6VTeymu6EAeP2d5mm4W1k4W5ky3zHq/xfKaBrveYtWNsdnmidWpNpzK3dtmehrteOOu9xk4W6Da7TQgxv0kwO4H169ezfv36WatcEEKIg5lSinA4TDgcxrIsCoUC+Xze/9BbLpcpl8sYhkEkEiEUCu3VYfJi36htPZG/8y4K99/Plpe8FAAnn8favh2js4vgihUEV60CIHTkEVg7d6JFowRXrsRcuBCAxLnnYvX0jHuO8Ikn0vPBD5F4wfkknvc8AHK330H277cwsP56AOyREVw/4K+2WDB0g0gkTLy9vTqehpYI5eERUp/8H+xdu8C20bq7657XC6+sikU2m6XU349SirJlETr5ZNLpNGr1Gsq//s24Qyrjz30ewz/8IYnnP5/0n/5I8uKLJ/1/TJx3nn++/U1vGlt+bvXnLW7YQPS0sSA28dznUfjXvcTOefakjyuE98V9qjYxjeFtszC3NgyZ6DGmap1QO55mPW8lvBVifqudlGtvTFS1O1mQO1HY6z1e4/l0gtzGiuLa8TUba7PbG/9/Gi9PtmxPwt3aQLfxJK0XhJgf5FvzBC6//HIuv/xyvzRaCCFElWmamKZJPB4nn8+Ty+X8qqxKpUI6nSadThMIBIjH4wSDwTkesfDUhuW2bdP68pfRefnldeuUd+xE1bam0HRc2wHXhWl8wF/woQ9SuP9+sjfdzJYXvZhVv/stLi5Lv/JlzEWLpj1WTdMIBAJ+m4xdn/gk7c99Lq0Xv5zsvffS/9nPEY/HqZgGyqh+YWmsIvS+WJVdFzubxc5lKReL9PT0+H1DDcNAP+pICg89hDU0RHHDQ0Q+85nJB1f731D7BSwUmuAObsOdhNg73hfvyXh//14F+kSTmE31GN7kfhOR8FYIMZXGdgB7qjHgnSrEbRb6Noa0jY87nWWTVezORqg7WSXvVCFubT9drz9/s0rd2su1t8trtRD7nwSzQggh9ohSimg0SiQSoVQqkc1m69oclMtlBgcHpQ/tAcT78G3bNtpxx5L++Cdoe9Wr0FtasHp7UZOE6IFVqyht2YzV24vR0UH6T3/yJ9yqZW3fTuSEEwgffzyZP/8Ze2SE6NOfzvD3v0/X1VcDUHzsMUKHHz6jsdu5LMaCbpRS5H7/B3RNIx6PU27vwLUs2ru7sW2b3ckkhutihkKThkmNYZN2wgk89d/vRzvxRIZGRuqC28aJxzJ/+ANtr3kNmT/+gcgJx4977NAxx7L7fz+FnUqhwmEyf/krCz7y4Rn9vELsLe8L9mSvva7rjmuT0Cy8nazydk/C22ZtE+SQXSHEVGYj4PVezyYLdqdzm/dYMw13Z9LaYbL/h9rzqZZ5l6dzarajrfY1u/a6vGYLMTskmBVCCLFXlFKEQiFCoRCWZVEqlcjn8/6XdK8PbTAYJBwOVw9blzYHc8Y0zWowu2IFra9/HU9d9hpwHLRolMXXXTvh/bRQiAXvex/bXvs69I52Qkcc0XS93Z/6FNZT2wCX+POeh7lgAZ1vfSu9H/s4my+4ENdxiJ52Ggve/98zGnf761/Prve+l8Gvfo1wTRgaO/ssdr7jnaT/8AcW/e//krzwQnre/wHU925g2be+ST4QoK2tDS2ZJL97NyVNwzRNCn//O5WNGwm97nXV/5dnPYvc1VcT/cxnKJVKlEqluudXSmEYBrbjUBgcZNOLXowWibDkc58d/3/c3UXH5W/lqVe9GnBpuegiwkcdRXnHzmlVHQuxv3h/15PxgoLaoHZfhreNYW1jQCBBgBBib9QGu3sT7noBbWNlbmOA2yzsnW6Y23jdC4SbVexOp6du7c8/k9YLE50aX6MnC3SlGleIiSl3sq1WzJtZ3oQQYn9yXZdCoUAmk2l6qKxpmsRiMZksbA4UCgUsy8I0TYLB4F73jTtYeUGTFxbVniabWCn9ykuIf/MbqHDYX2YYBqZpjquybfzbLmx4iP7PfpZl3/zGPvu5hJgLzcLbxuB2qvB2OqYKArwAQAghDlTTDXCbXa99DO98qnC3NgxuVpE70blnugGudz5ZiFv7+m0YRtMQV3bCiZmYL3mdlCwJIYSYdUopIpEI4XCYfD5PNputC2gty2J4eBjDMIhGo4RCob3qOSamz5vE7VDnVQo2Vgt6X2KaBbYT9eNsVgnoVf95z+Fs3sLANdfQ9e537bOfSYi5UvvF2zTNpuvUhreNbRNq++BOVXk7VW/c6fS8lfBWCDFXpjvJY6OJ+uVO93rj40x2mmrCtYlC3olC3KmqdCcKcb3w1vssZZrmuDBXAlwxH0jF7BTmSwIvhBBzyXVdyuUy5XKZQqHQNMTy+tVKm4NDW/bW2+i7tr6lQuTkk1nw3+/br+Po/chHyP/rvrplXddcTfCUU5qGtjP5OKXrut/SY7IKWyEORY39bptV4E5W1T4dzapum01YJoQQ80Fjq4WZhLszCXJrz73nnewcpq6+rV3mXa5tjdBYgVsb4srnq/lvvuR1EsxOYb78ooUQ4kDhui7FYpF0Ot206ikQCPhVtPJBShwMvCo+y7L2OrBtDGsbJx4TQtA0uG0Mb/f2K850WibIdimEmO+mCnBrJ5BsbLsw3RB3Oq0WPM3aJtTe1hjmNlbf6rqOaZr+yQtx5fX84DRf8jopSxJCCLFfKaUIh8P+ZGHZbJZisejf7lXWem0OIpGIfFgSB7TJ2iI09rH1wttmoZEXKDVOPOZ9oWg8SVWfOFR5X7Sn0zahMbT12iY0Ht7byAsXLMtqenuzycqk360QYr7Zk9YLjcFr7etws4C3dmdabUDrLZ9oorVm1bmNY5+oArf281Pt67b3+ao2uA0EAvKZS+xTexzMPvnkk2zatIkzzzyTcDiM67ryhyqEEGLalFIEAgHa2tqwbZtCoUAul/OraCuVCqlUilwuRyKRIBAIyJdccVCZLLD1wiEvqPUuN/ti4TiOv8Oi8fG9LxGNk4/JZzJxKJtuz9uJKm9rg9yJeNvxZCYKbWt3rMi2KoSYb7zPJ9PVrNXCRJW4tTvWJmvR4IW6jVW5E413sgDXOzW2SQgEAn6AKzvjxN6YcTA7ODjIxRdfzI033ohSiieeeIJVq1bxhje8gdbWVq5t6AsnhBBCTEXXdWKxGNFolHK5TDab9asGK5UKQ0NDAMTjcWKxmHyRFQc1pZT/QT4UCvnLZzrxmBcMVSqVuqpzGKuy9b4s1PZek+1HiOlVgTWbrMw7edvlbFTdei1LGoNbaZcghDgU1O5Mm47aIyImq8xtFs42q96dKrj1xlgqlca1UKg9esL73BUIBPzQtvY1Xl7PxURmHMxeccUVGIbBtm3bOOKII/zlF198MVdccYUEs0IIIfaYUopgMEgwGKRcLpNKpeq+0GYyGbLZLJFIhGg0KhOFiXmlNigKBoN1t9Uehl3bEmGiYGiyKtvGCTJqe9kKIcZ428tkmk1W5m2b3mkitTtXJjJZn1up0BJCHIomOiKpUW0oW7uTrdm59zpcqVSaVuo2tk6o5X1Xaaywrd3h5gW2wWCwbpnsMBewB8Hsn//8Z/70pz+xZMmSuuVr167lqaeemrWBCSGEOLQFAgE6OjoolUrk83m/ItB1XXK5HLlcjkAgQDKZnNHhUkIcjCbqqdmsj21tKNTIdV0sy2pawVdb7SGTjwkxPbUTyzTTWMnlbaPTnahsqnDX++JfO7FN7UnaJQghDlW1Ael0QtzGIySm2uk2UWXuVGOpPTrCC2xre9tKW6pDz4yD2VwuRyQSGbd8YGBgXHWHEEIIsTeUUoRCIUKhkB/QFgoF//Zyuczu3buJRqPEYjEJaMUhZ7KqkcYvEpVKhXK5PGHIM1GVbePkY96XBglshZjaVC0Tpup16335n8h02iU0hra116XiVgghpneERGMrhIkmeK19/W7WC7f2KIlmYa138loieCf5zDV/zTiYPfPMM/nud7/LRz/6UaD6h+Q4Dp/+9Kc5++yzZ32AQgghBOC3OEgkEuTzeXK5nP9l1augDYfDxGKxCSd7EeJQ4n3IDwQCdctrv1g0fqFoFgBNN7Bt7JMphJjadHvdel/2G6tvp9MuYaIqeaBpb1tplSCEEONN1UKhMXwtl8tYluWfe8HtRH1va5+n2WesYDBINBolFApJSDvPKHeyY2eaeOSRRzjrrLM46aSTuPHGG7nwwgt5+OGHGRoa4vbbb2f16tX7aqxzIp1O09LSQiqVIpFIzPVwhBBCjLJtm0wmQ6FQGHcYaDgcJpFISDgkxAzVTj7mfYGYaPKxyTT7QlE7E70QYvbUfrFvdpqsh+1UmrVKqL0s27MQQkyPd3RDqVSiVCpRLBb9wLY2qK091fas9V5/I5EInZ2dUojC/MnrZhzMAvT29vLlL3+Ze++9F8dxOPHEE7n88stZuHDhvhjjnJovv2ghhJivbNseV0HrCYfDRKNRTNOUL49C7IXZCmxrq00aT7KNCrFveF/4Gw+xnU7F7VQmqraVdidCCDE9tm1TLBYpFosUCgVKpVJdD9va0Narpg0EAixZsoRwODzXw59T8yWv26Ng9lAyX37RQggx3zmOQy6XI5vNjqugNU2TWCwmh/4IMctqA9tmgc9MNDtkT4IdIfa9ZhW3jdv0npqo0tbbGSPbtxBC1HNdl3K5TKFQIJ/P+0FtbSsEqO4YW7JkCfF4fJ+1nXkinWe4ZHFKZ8uM75uxKtzUM8yFyzr36Ln/sGOA0zpbSAYnrgw+UPK6bdkiGavCUcnYHt1/Wj1mH3zwwWk/4LHHHrtHAxFCCCH2hqZpxONxwuEwuVyOQqHgV9BalsXw8DCBQICWlhY59EeIWeJVbTT2sYX60Lbx1Eyz22onLmrWz1YIsfem0zexWWBb2/d2IpNt840TkzULboUQ4lCjlPLn1mhtbfUraguFAtlslmw26/cb7+3tJZ/PE4lEiEajU05gJvacF4g3e29aFgvt1WNP67d2/PHHo5TyS6cnG9je7FEVQggh9pZhGLS0tJBIJCgUCuRyOX/Sk3K5TH9/v988PxgMyhc/IfaRiUJbbyKjZqfGavfJJi6q7WOr6zqmaWKapvSWFmKW1Qa3wWBw3O21E5I1C28nOkBzOhOTNQtupU2CEOJQous60WiUaDRKMpkklUrR09ODbduUy2Vs2yaXy5HP50kkEkSj0br7P5HK81gqh+26LIuFOLE9Qcaq8LddQ7QFTQaKFsmgwVkLkiileCpb4J6BNGFDJ2EaBLTxr7W39g5jaIqRcoVcxebM7iSPjGQZLFksj4U5uaO+evWJdJ5t2SIVxyFbsTmqNcbhrfXjdFyXf/Sl6CuWaQ0YVJyx944HhzJsyhQAODYZY3Uiwu27R+jSqjsGbxrKs7KiOLYtzp19KRZFgpia4sGhDIamMVK2WBoNTVj56/1/JAMmQ2WLzlCA07ta0JTi+5t6WdcSoSdf4swFSR5L5diVL6EBT+tsYVEkuFeVxTDNYHbLli3+5fvuu4+rr76aa665hqc//ekA/OMf/+Daa6/lU5/61B4NYn/I5/McccQRvOxlL+Mzn/nMXA9HCCHEPqaUIhKJEA6HKRQKpNNpv6rHa7qvaRrRaJRYLCZf8ITYT5RSfohay6vKq53JeLKJixzHoVwuUy6X65Y3TjxWW20r27kQs0/TNDRNa3o0ijeBTbNK26kmJvO28Wa8iXBqQ1uZZFAIMd/puk5bWxv5fJ5MJoPjOBSLRSKRCK7rkkqlKJVKxGIxAoEAIyWLHfki5y/tQAF/6xmir1AmbGikyhXOWpikxTT4485BdhfLdAQD3NWf5rwlHUQMjT/sGKQz1PxIQ8txef6SDjal8/x11xAXLusgbOj8fGsfRyej49YfLFlcNNrW4Nfb+1kaCxE1xnamP5UtUrAdLlrWyXC5wq+39QMwUCyzNVvkgqWd2K7Lb7b3szASpCscYDCbwwgEcV3oK1bfL/qKZY5vjzNcshgsVXjx8k5MXeMXT/VxZGuUmNk8Bh0uV3hGdyvtQZO/9QzzVLbIyniYkuPQHQ5wckeCrZkC6XKFi5Z1kq3Y/HHHIC9a3rU3v1JgmsHs8uXL/csve9nL+MIXvsB5553nLzv22GNZunQp73//+7nooov2elD7wsc//nFOPfXUuR6GEEKI/aw2oM3lcmQyGb96x3EcMpkMxWKR1tZWaXEgxByqrcoLhcYOCasNbBtPzQ6hniiwrZ3RuDG0lSpbIfYNr49ss3YnML6/bWNoO1GbhNrK+2YaK2xlB40QYj5JJpPkcjm/rZNXiAL4E4klk0l2lWz6imV+MxpyWq5LxqoQNgIkAgatgep3n7agSdayMbUKLQGDmFn9XLQyHiJrNT8qflm0+lktGTRJBHQ/8EwEDHKWQ0Cvf61dHAkS0KvjXRgOMlAsE42NTV7WVyyzMladD6QtaPq9ZXcXyiyPhTA0hYFiYThIf7FMdyjAxuEMq449nq6ATspxKdsOtusSGn2errBJaDT8TQZMshV7wmA2Yep0hKrvVStjoep44mF0pVg6+rPuLpZZFQ+jlCJuGiQCBmlr4h2M0zXjBhQbNmxg5cqV45avXLmSRx55ZK8HtC888cQTPPbYY1xwwQU89NBDcz0cIYQQc0ApRSwWIxKJUCqVKBQKFItFoNqDtr+/n1Ao5O9hFkIcGCbrf+kdQl2pVLAsyz+kr1lrrcmCHK/ar7YtghwuLcS+N53+tpNV207UJmGyScuatUbwLkvvaiHEwSASiWAYRt1kqy0tLaTTaf91cWRkBCcY47BElOPb43X3z1gV9JrPNwpw8f6ZHu/+quby2GO5o5emzwWm+5FLoUgEDPK2w2GnPJ32gI5yNR5P5+kKjX2P02rHpWCCt4xJGZMNak8esIkZv/McccQRfOxjH/O/zEL1kNCPfexjHHHEETMewC233MIFF1zAokWLUErxy1/+ctw6119/PStXriQUCnHSSSdx6623zug5rr76aj75yU/OeGxCCCHmH03TCIfDtLW1kUwm60KXYrHIwMAAg4OD0jNdiIOAF6iGw2ESiQQdHR10d3ezYMECOjo6SCaT/qSApmlOGLI6jkOpVCKXy5FOpxkcHGT37t309vbS19fH0NAQ6XSaQqGAZVkThkFCiNnltT4JhUJEo1FaWlpoa2ujs7OTBQsW+Nt6a2srsViMUCg06bYO+H0ZvUOBh4eH6e/vp7e3l97eXgYGBhgeHiaTyZDP5ymXyziOI9u9EOKAoZSivb3dv57JZIhGo3R3d/sFJq7rEquU2JItULKrRx/kLJuiPfGEjS0Bg1S5Qs6ycVyXrZnihOvO1M58ibLtULYdegolvzrV0x0KsCVTxHVdhksWw6Vq//HucICnskVsx6Xk37daTdtiaDzzZa+k3dTpDgd4eCRLV3jPCmzSls1AsYzrumzNFusC3roxZgu4rkvWqpC2bBITVODOxIwf4Stf+QoXXHABS5cu5bjjjgPggQceQCnFb3/72xkPIJfLcdxxx/G6172Ol7zkJeNu/9GPfsQ73/lOrr/+es444wz+3//7fzz/+c/nkUceYdmyZQCcdNJJlEqlcff985//zD//+U/WrVvHunXruOOOO2Y8PiGEEPNXOBwmEAj4k4R5YWypVKKvr8/vPysVNEIcXLyJxxrVtkWobY9gWVbTQ6Ynq7KtbYlQe5LXCyH2j9o2Cc0mGayttvW246laJEzW11bTtKaVtrLdCyHmQiKRoK+vz59I0XVdNE2jra2Nvr4+HMchUClzeCzKH3YM4AKmpjhrQXLCxzQ0xamdCf64c4CwrtMWnL02b92hADf3DpOxKhzdGiNq6OQrNrftHuG5i9tZHguxM1/il9v6aQuafvjaEQqwIhbi19ur7RhOaIsTGW1P0G7qFDJpApqiKxQgX3GaBqrT0RYweCyVo79YnfxreSw0bp3lsRC7i2V+ua0fDTijuxVDG6sc3lPK3YNdf/l8nu9973s89thjuK7LkUceySWXXDJu9rcZD0YpfvGLX9T1qT311FM58cQT+fKXv+wvO+KII7joooumVQX73ve+l+9973vouk42m8WyLK666io+8IEPNF3fmxDGk06nWbp0KalUikQi0fQ+QgghDn6u65LNZslkMnXLNU3zWyDIFy8h5ifXdevaItSebNueUaVcbWDjtUSQ4EaIA0vj9t7YJmGmGicdlInIhBD7w+bNm7GsamXpsmXLCIerPVsLhQIjIyMkEgkikcicvw49kc4zXLI4pbNlVh83nU7T0tKy13ldxqpwU88wF45OTjZTG4azOK7LcW3xqVduYo9qbiORCG984xv36Alnolwuc++99/Ke97ynbvlzn/vcaVe/fvKTn/QD3G9/+9s89NBDE4ay3vof/vCH93zQQgghDkpKKf+Q50wm4zfQdxyHdDpNNpslGo0SjUYlYBFinqmd4b1Z5V1jYGtZ1oT9Lb1gp/ForsYKOy+4ldcTIfY/TdP8ViiNJqu0nSi0nWjSQRhfXe+9FkgPayHE3goGg34wu3PnTpYvX+63f+nq6pLJTfeDx1M5nkznec6itj1+jBkHs9/97ncnvf2yyy7b48E0GhgYwLZturu765Z3d3fT29s7a89T673vfS9XXnmlf92rmBVCCHFoMAyDZDJJJBIhl8v5PdUdxyGTyZDL5SSgFeIQMtHkRF6VrRfS1p6aHSY9UWDrVdo1C24ltBFi//P62k4U2ja2Q/FeAyZqjzDZpIOy7Qsh9kYymSSfz/tHAfT39/vzNx1IoezaRGSuh+D7zbZ+nIYd6+cu6djjatl1LVHWtexd94AZB7PveMc76q5blkU+nycQCBCJRGY1mPU0vjG5rrtHb1avfe1rp1wnGAwSDAZn/NhCCCHmF+/9wLIscrkc+XweGAtoi8UiLS0tTftYCiHmv9oq21peYNtYXTtRcDNVT8tAIFDXEkFCGyHmzkQ7aoC67b6x4rZZdf1k235jWOu9BsgOYSFErUgkwpIlS9i2bRuAH9LKa8XELtjDAHZfmnEwOzw8PG7ZE088wVve8hauueaaWRmUp6OjA13Xx1XH9vX1jauiFUIIIfYF0zT92Z5rWxxYlsXAwACRSISWlhYJSoQQQH1g27izv1kP26kOjy4Wi37lvkcmHhPiwOPtSGnWDqUxtJ1q259OSxRd1+sCW/kcIsShKRwOEwqFKBaL2LZNsVgkEjlwKlTF1Paox2yjtWvX8j//8z+86lWv4rHHHpuNhwQgEAhw0kkn8Ze//IUXvehF/vK//OUvvPCFL5y152lm/fr1rF+/fo+avwshhJh/vBYHsViM4eFh/7DEfD5PqVQikUj4DfeFEKKZiQJb7xBE27b9ClvvcrNKu6kOjW6ssJXQRoi5M9nOGq+fbbPTTHpY11bymqbpP59pmrLtC3EICIfD/k7cbDYrwexBZlaCWah+0Ny1a9eM75fNZnnyySf961u2bOH++++nra2NZcuWceWVV/LqV7+ak08+mac//el89atfZdu2bbz5zW+eraE3dfnll3P55Zf7s7wJIYQQUK2g7ezsJJfLkclk/H5zw8PD5PN5Wlpamh7iKIQQE6mdiCgUCvnLZxraTDQBkdczU2aMF+LAMlE/25m2RHFdF8uysCzLP7LH4z2+V23rXZZtX4j5Ix6P+0e3p9Np2tvbD6ges2JyM/7m+Otf/7ruuuu69PT08KUvfYkzzjhjxgO45557OPvss/3r3sRbr3nNa/j2t7/NxRdfzODgIB/5yEfo6enh6KOP5ve//z3Lly+f8XMJIYQQs0EpRSwWIxAIMDIy4leulUolBgYGaGlpIRQKyZceIcRemUloM9ms8a7rTiuwlcOihTgwTFZl29gWobbKvhkvsG18/Nptv7ZFgmz7Qhx8QqEQwWCQUqmEbduMjIzQ3t4+18MS06TcZrvbJ9HYu0opRWdnJ+eccw7XXnstCxcunNUBzjWvYjaVSpFIJOZ6OEIIIQ4wrutSKBTIZDJ1X4pM0ySZTEr1rBBiv5poxviZtOeqPSxaZosX4uBQW2Hvbf/lcrlp25PJNJt0TLZ9IQ58qVTKn5/JMAxWrlw573vPz5e8bsbfFpsdOiGEEEIcqpRSRCIRQqEQIyMjfn8nb3KweDxOOBye9x+MhBAHBq/KrpHXx7a2ys6rtG1Ue1h0o2Y9bCW0EWLuTVRh71XY1u6smWjbh+Y9rL2dNd62753LodJCHDgSiQTDw8OUSiUqlQr5fJ5YLDbXwxLTMONg9iMf+QhXX331uGbChUKBT3/603zgAx+YtcHNJZn8SwghxExomkYymaRQKJBKpfxDjVOpFLlcTno9CSHmVG0f21oTtUSYqMpuotCmdoZ46WMpxIFD0zQCgcC45bWBbe3Eg816WE+0s8Z7TWncYSM7o4XY/5RSJBIJ+vv7AchkMhLMHiRm3MpA13V6enro6uqqWz44OEhXV9e8CzLnS2m0EEKI/ce2bdLpdN0EHJqm0d7ePi4UEUKIA5E3sWFtWDPZxGPNNM4U751LD1shDlzetl/bCsU7ny5d1wkEArLdC7GfWZbF5s2bAQgEAqxYsWKvtrtdPT8ll32ctWv/a8b3LRR2sOGhyznlab/a4+efymznden0g+zu+x1r17x3FkY3fTOumHVdt+kv9oEHHqCtrW1WBiWEEEIczHRdJ5lMEolEGB4exnEcHMdhYGCASCRCIpGQLydCiANabahaq1lg61XaTlZl17ijqnaWeC/AkddFIeZe7bYfCoX85d723BjYNmt1aNs2hUKhbrv3Wi1427tU1Qsx+7xtt7ZtSbOK+UOd69ooNf5IxkTiWBKJY/f7eKYdzCaTSZRSKKVYt25d3Quobdtks1ne/OY375NBCiGEEAejYDBIR0cHw8PDWJaF67rkcjkcx6G1tVW+jAghDjqTBbaNlbUTVdk5jkOpVBq3XNO0ppMOSRsYIeaeUopAIDAu5HEcxw+Aarf9ZjtqyuUy5XLZX+btpGlshyCfj4TYM0opQqEQ2WwW13XJ5/MMDPyanTtvwHFKdHT+G6tXXUGhsIMHN7yJWOwI0ukHicUO4+ijvoBSiv7+v/DEk/9DMNhFyFmIYYfHPc8jj1yDrkfJ5h6nVOrlqCM/w7bt3yaTeZiurnNZs/oanIKFm62+1+/q+SkDA3/DruQolnaxdMnrWLLk0rrHHB6+k+07/o9jj1kPwIMbLmfpkleTTJ7GLbeeTHfXCxhJ3UMotJijj/osmhNE682MG1uzdXU9wu13nMnChS9jaPDvrFp1BenMQ/T2/gJQrFj+ZhYseOG4MUzEenw3ekcMrS26h7+petMOZj/3uc/hui6vf/3r+fCHP0xLS4t/m1ci/fSnP31WBiWEEELMF4Zh0N7eTiaTIZfLAdW+7LZt09raOi7cEEKIg9FEEw95gW1jpV2z9meO44wLbkD6WApxINM0jWAwSDAY9JdNd7v3dtLU7qipraz1eldLRb0Q0xeNRslmswCMpB4lk76Zk0/+KaB48ME3kUr9i0Cgi1xuE0cf9QUikVX8675LGUndQyJ+DBsf/zAnn/RjAoFO7r3zYhL6EU2fp2JnOenE79Pb+yseePCNPO3kXxEMdvKPO/+NZUv/fdz66fQGTjv1DwDcffeFdHQ+m1BwwbR+JssaJpl8Oocd9iE2Pv5hduz4P5Z2vg6tNzutdZcvfxMAptnKySf/lHT6Qfr6fsfTTv4VjlPgn/98EcnkaXWPM1G3AABzXfe0xj1d0/42+JrXvAaAlStXcvrpp8/7Hnky+ZcQQojZomkaLS0tmKbJyMgIAOVymf7+flpbWwmHx++JFkKI+WCqmeIty6JcLvsTEE0U2DaGN8C4CccksBXiwDDRdu+1QfG2+2atEJpV1gJ+2xNpfyLE5Gq/V6TTd5Mavpe7bj4XAFsrkS9sw7CThFmMuSlAeeQJIvpSCoXtGHqUiLEcdVcGK1ikUz+TEv3jnsMeztEeOoPSXZsJFIKEQ0vRN9pYqS2EjIWUSj1oRPz1naE8SfdEnH8N4BbLtIZPJJN+kFDnWDDrui5uqkDp1sdR0SBOOovTVoYkKEwSj6+ktPFxOhefzfbUDSwcfj6qYHHnF3+Mtj0FRyWoPNlXXXfjSirFQbq7X8BTT32V5aPP0d31fACGnrqVNus07Ht3+eNJpx9E5Q2coRzl+7bhliwCJy7HenAHbtECU8c8ZglaJED5we3oC1rQu2ZnHqppBbPpdNpvpHvCCSeM6xdTa75MkHX55Zdz+eWX+82EhRBCiL0ViUQwDIPh4WG/H+Pw8LA/SYYQQhwqvJniA4EA0ejYoYBeYDvdPpbNglyvd60cHi3EgUXXdXRd93vXuq7rt0LwtvVSqdR0e/deFzy14a9pmoRCIdkpIwTUH41XslhgnMeaZ/43ANa9T2EEOym4fShbx1jViYoGUbeCmy3ihm3cXJnA01dC0MS9vQITfEVRrkHw1FUUNw/CFgfzuAUQNOBmG6dcRlORuvXdso156jIAnFuLuFbDTpnhIq5rE3jGWtxcCefO4thzKUXwjLVQcUjf8QgkFca6BVTSBU77z5eTSqWw+zO45QpKUwROX4111xYIVVCMvfdr2lho7RYtzGd64yngWk51zYqNecRCVMjEengnWlsUY1Unds8IlUd3EThpxQx/I1ObVjCbTCbp6emhq6trwp54XpmvVJgKIYQQEwsEAnR2djI8POxXfw0MDBCPx4nH43M8OiGEmFu1gW2tiWaJb+xj6a1r23bTCtvaqjuZfEiIuaWU8sPaWt72XjuBkWVZdes0q6xt3CkTDAbRNE22cXFI0TQNTdNwHIe4fTQ9xf9l0W3nY6g4pUofgYwLMVC6hhar7iRRQQPXqhBmCQW1g5IaJEAnA/odJGjeykBrqQavKhoEXUOFq+/bStdwSxUI1a8/ot2HrfIApPQNrHLeWXd7IN9KXm3HdW2sQJaMttG/zXHL9Nz6Xdq10+kv30xL9Phx43EGsjh9aRynTO/t36PdOZXdvb+jpeXEceu2hI/ncePjuHoF2y6S0h9ilXMlebaDqaNC1Wp/ZzhPYHVX9edd0IL1SM8U//t7ZlrB7I033khbWxsAN9100z4ZiBBCCHGo0DSNZDLJ4OCg/0Ujk8mglCIWi83x6IQQ4sDjhTeNfSxrA5za02QVthMFtrXVdxLYCjF3mm3vje1PvBYotZpt496kgqFQSFogiEOCt8PDcRyCahlLk5fxkPV+XNfBCEU5uusLkB8/AScu6CrE6uDl3Hf/qwmYHcQDh8FEtZf+djS97SlhHsOGh/6TQmE7S8IvJ2h2Uyrt5tHH3svxx32TkN5Ne/QZ3HX3eUSja4iq1QDYg1kMEuQW7GL7yBUErBbWdn2oyTO4GGu7MTcnyS3sYfvQ1YRYzJol7xm3Zjx0JJ3Bs7j7ny8EFMujryNodpG3t0/++rCPXjqmFcw+61nPanpZCCGEEHtG0zQ6OjpIpVLk89W9x+l0Gtd1pXJWCCGmQSnl95Wt1ezwaG/ioekGto0Tjkk7BCHmVrP2J17P2lKp5G/njdt4s0kFa3fCyORiYj4Kh8PVnuuGS/vOM1jy9MtQpo5bsEBTuCGH4wNf8tdf1fI2VCyEigVJlk5mwWkXQ9CgfPdmtNbIuMc/LP5e9ES15WcsspZjw9f6tx0d+R+MaDeYRt1zmKU4x5xyPQDlO55EawkTDCY4/rhvAqAlI6zs+XfWnfph3FyZ8s7HCSTW4ubLoGDtuvfijOQp/2MTuh5BGRrKHtvetfYYlc0D4MLaNe/ByZZQYROlV1ucnHH6LXU/w6L8hax8xlV147GyIxhq7HuYloxg7xrBWNWJ05tGa9k384Ls0VTQIyMj3H333fT19Y174bvssstmZWBCCCHEfKeUorW1FU3T/NlTM5kMuq4TiYz/ECSEEGJqEx0e7VXYegGOF9ZOFOY0m3CssXetaZpyqLQQc2SynrXe5GLedl6rsTWCt5PHNE2/qlZ2xIiDWSAQqE6mZSq0lXHKd20G1wVDJ3D80gnvp3QN88hFlP+5GQIGWmL2gkgVMrHu34abL6Ov6ECFTNyihfXQDgInr0Rb0ILTn6V82xOoRBgVD1XD144YuFC67QlUIoSKj7ZfCBg4sQD/vP5naNtT6Ectxc2WcLfa1QnEAgbmJP1gtWSkbjyDudvZ0vtF1gXe5a9jrO3GenAHpV0j/uRf+4JymzVmmsRvfvMbLr30UnK5HPF4vO7FSinF0NDQrA9yLqxfv57169dj2zaPP/44qVRq3kxsJoQQ4sCTzWZJp9NA9f20o6Nj3GzGQgghZp8X5nhBjndqVl3bjHeodONkYzIRkRAHhkql4lfVNutX24w3Mat3kqBWHExs20Yp5Z/mWmXHMG62iHn4wknXcysOytBw8mWsu7cQeNa6Ccf/4INvJpt7ig0bNnDMMceg6zpHHfVZYtG1szaeiZTv2YqxuhMtGZ165WmYcTC7bt06zjvvPD7xiU8cEtU86XSalpYWCWaFEELsU67rMjw8TLFYnYFU13U6OjrGVXwJIYTYP2onHKutsp3u1ydd1+vaIEjvWiEODK7r+tt1bWXtZJRS/vYcCoX8ankhxNSmG4SW7twEFQdcF+Pwheidk7d329O8bm+CWeuhHbj5MubJK1Ha7LyfzziYjUajbNiwgVWrVs3KAA50EswKIYTYXxzHYWBgwP9yYJomHR0d8iVeCCEOEK7r1s0W7503Hio9kdpDphura+W1Xoi5U9sCwauunSoq8VofeCfZmS7E/jVVXudkilgPbK9bpiIBAicu319DnJYZ95h93vOexz333HPIBLNCCCHE/qJpGslkksHBQf8LgveBQwghxNyrrZoLh8d67zWbbKxZsFNbqVdL13V/IjPvJBV5Quw/mqYRDAYJBoPE43F/J0xtC4TGqlpveS6XA6T9gRAHGi0eIviMqVsbzLUZB7Pnn38+11xzDY888gjHHHPMuP53F1544awNTgghhDjUmKZJe3s7AwMDuK5LLpcjGAz6E1sIIYQ48NSGOp7GycZq2yE0sm0b27brJhurDYG9CYmkFYIQ+0ft9uexbbuuorZxB4tt2xQKBQqFAlB9XfBaH4TDYdnRIoRoasatDCZ7MVFKTfswnoOFtDIQQggxF3K5HKlUCqjOAt7Z2SlfxoUQYh5wHMcPaL3AtlwuT6t3rTfRWGM7BAl8hNj/vEkDvdNk7Q+8oNcwDCKRCKZpyuc6IfbSfMnrZlwxO93ZSYUQQgix5yKRCNls1p98Jp/PE43OzsyfQggh5o6maf6hzh7XdesCW8uyKBaL47571QZBtWqDWploTIj9Q9M0QqGQf1ST16qkNqz1tmHXdf1l+Xx+3GRigUBAtlchDlEzDmYPFevXr2f9+vXzrgJYCCHEwUEpRSKRYHh4GIBUKuUfyiqEEGJ+UUqh6zq6rte1Q/CCWO+w6YkmGmvWIqE2+JG+tULse0qpup0uXlCby+Uol8t1225tUJvL5VBKEQwG/c96EtQKceiYcSuDL3zhC80fSClCoRBr1qzhzDPPnDczEs6X0mghhBAHH9d1SaVS5PN5oFqZ0dnZOW/eY4UQQsycV1lbG9ZWKpVptUIA6kJaLwSSsFaIfc9xHIrFIsViccJ+0x5vx4rXu1paHwgx3nzJ62YczK5cuZL+/n7y+TzJZBLXdRkZGSESiRCLxejr62PVqlXcdNNNLF26dF+Ne7+ZL79oIYQQByfHcejr6/MPhYtEIrS2ts7toIQQQhxQvBnkvcnGJquubUbXdb+61jvJTkAh9i1vMrFisUipVJq0baTXAsWbEFa2TyHmT14342D2Bz/4AV/96lf5+te/zurVqwF48sknedOb3sQb3/hGzjjjDF7xilewYMECfvrTn+6TQe9P8+UXLYQQ4uBVLpcZGBjwr3d0dNT1JhRCCCGa8fqUe6dyuTzt6lpd18dNMmYYhlTtCbEPeDtXanvUTlZRGwgECIfDBAIB2S7FIWu+5HUzDmZXr17Nz372M44//vi65ffddx8veclL2Lx5M3fccQcveclL6Onpmc2xzon58osWQghxcMtms6TTaaD6Ybyjo2OORySEEOJg5LquX6lXW107na+FSim/FYLXS1NCISH2jUqlQqlUolwuT1pRq+u6X00bDAalmlYcMuZLXjfjyb96enqa7rmpVCr09vYCsGjRIjKZzN6PTgghhBAARKNR8vm8X/FUKBQIh8NzPSwhhBAHmdpw1eOFtV5I64W2jWGtN5mRZVkUCgWgeoh1bb9aaYMgxOzwttNoNFo3WVihUKjLZGzbplAo+NtkKBQiHA4TDAalf7QQB4EZB7Nnn302b3rTm/j617/OCSecAFSrZd/ylrdwzjnnALBhwwZWrlw5uyMVQgghDmFKKeLxOMPDwwCMjIz4h5YKIYQQe6M2rPV2+rmui+M4/uRitRON1XIch1KpRKlU8pc19qw1DANd16WyVog9pJTyK2JjsZhfTetV1NbuRPEmGFNKEQgECIVC0pdWiAPYjIPZb3zjG7z61a/mpJNO8r8MVioVnv3sZ/ONb3wDgFgsxrXXXju7IxVCCCEOcV4FRKFQwHVdMpkMbW1tcz0sIYQQ85BSCl3Xx4U5XtWsV71XLpfHHWJt2za2bVMsFv1lXmVt4wRjEtYKMTNKKX8bisVifjVtqVQin8/726Prun54m0qlME3TnzzMNE3Z9oQ4QMy4x6znscce4/HHH8d1XQ4//HAOO+yw2R7bnFq/fj3r16/Htm0ef/zxg75nhRBCiPnBcRz6+/v9mba7u7ulAkIIIcScadaztlKpTDrDvMer6JM2CELMDtd1KRaLlMtlisWi/3mxkaZphMNhIpGIHH0lDlrzpcfsHgezh4r58osWQggxf2QyGb+XezweJx6Pz/GIhBBCiDGNPWu903TCWmmDIMTscF2XSqXitzawLKvpeoZhEAqFCAaDBAIB2dbEQWO+5HUzbmUAsGPHDn7961+zbds2yuVy3W3XXXfdrAxMCCGEEM2FQiE/mM1kMv4haUIIIcSBoFnPWsAPa8vlst+3trGib6I2CN57nXeS8EiIydW2PIjH49i2TalUolAo1PWErlQqZLNZstmsH9KGw2H5bCnEfjLjYPZvf/sbF154IStXrmTjxo0cffTRbN26Fdd1OfHEE/fFGIUQQghRwzRNIpEI+XweqO4tbm9vn+NRCSGEEJPzetaGQiFgrKKvtqq2WRsEx3H89zyPFzh5rRCkqlaIyem6TiQSIRKJ+Ds/CoVCXbFdbUhrmqYf0sr2JcS+M+Ng9r3vfS9XXXUVH/nIR4jH4/zsZz+jq6uLSy+9lHPPPXdfjFEIIYQQDRKJBMVi0Z8N27Zt6csnhBDioFJb0edp1gahcdZ5wL/NC2w1TcMwjLqetfK+KERzuq4TjUaJRqN+JW0+n68Lab1tLJPJ+FXrkUiEQCAwhyMXYv6ZcY/ZeDzO/fffz+rVq0kmk9x2220cddRRPPDAA7zwhS9k69at+2ioc2O+9KwQQggx/6TTabLZLFANamOx2ByPSAghhJh9ruuOC2orlcqU99N1vS6oNQwDTdP2w4iFODjZtk2hUCCfz0+4jQWDQaLRKMFgUKpoxZyaL3ndjCtmo9Go349k0aJFbNq0iaOOOgqAgYGB2R2dEEIIISYUCoX8YNbrNWsYe9Q+XgghhDhgKaX8gNXjOA6VSoVSqeQHtc361RYKBQqFgr+stqpWWiAIUU/XdWKxGLFYzJ84rFQq1fWk9a57PaTD4bB8/hRiL8x46znttNO4/fbbOfLIIzn//PO56qqr2LBhAz//+c857bTT9sUYhRBCCNFEIBAgGo2Sy+VwXZdUKiW9ZoUQQhwSNE0bF9batk25XParai3LGtcCoVKpUKlU6log1Aa1MrGYEFWGYfghrdeTNpvN+jtAKpUKmUyGTCbj96ONRqNSlS7EDM04mL3uuuv86pwPfehDZLNZfvSjH7FmzRo++9nPzvoAhRBCCDGxeDxOsVj0+4M5jiMfiIUQQhySdF33K/hgfAsEb3Kx2rDWcRyKxSLFYhEY63vrBbVyuLYQYz1pI5EIxWKRfD5fV0XrbV/ZbJZgMOhPGibbjhBTm3GP2UPNfOlZIYQQYv4aHh72D9OMRqO0tLTM8YiEEEKIA5PrupTL5brKWsdxJr1PbVDrTSomgZM41NW2CrEsa9zt0upA7GvzJa/bq60jm82OexM7mP8zhBBCiINROBz2g9npTIYihBBCHKqUUgSDQYLBIFANaiuVih/Wlsvlcb1qvWrAXC7nP4YX1gaDQQKBgAS14pDT2I82l8tRKBT8jKi21UEgECASiUgVrRBNzDiY3bJlC29729u4+eab/cM9oPqGppQa9yZ2sFq/fj3r16+fNz+PEEKI+SsUCqFpGo7j+P305EOvEEIIMTUvZDVNk2g0Coz1qq2dWKxWbdVtNpv1JycLhUL+Y8n7sDiUGIZBS0sLiUSCUqlENpulXC77t3vbSyqVIhKJEI1GpYpWiFEzbmVw+umnA/COd7yD7u7ucW84z3rWs2ZvdAeA+VIaLYQQYn4bHBz0e311d3ej6/ocj0gIIYSYHxzH8VsfeKfJCng0TaurqJWgVhyKKpWK3+qg2RFdwWCQeDxeN4GfEDMxX/K6Ge+iePDBB7n33ns57LDD9sV4hBBCCLEHDMPwg9lisehX/QghhBBi72iaRigUIhQK+cu8STe9U22LP8dx/OWZTGZc6wPTNGWiTjHvGYZBPB4nFov5rUCKxaI/+Z63jUQiEWKxmFTQikPWjP/yn/a0p7F9+3YJZoUQQogDSDAY9HvfSTArhBBC7Fu6rhOJRIhEInV9ar2wqfbA1MbWBzA2oZh3LhOKifnKa/URCASwbZt8Pk8+n/erzr3rwWCQaDRKMBiUbUEcUmYczH7961/nzW9+Mzt37uToo4/GNM2624899thZG5wQQgghpsebxARkAjAhhBBif2rsU+u6bl1FbbPWB15LBI+u635IGwqFJKgV85Ku68TjcaLRKLlcjmw2O66C1jRNEolE3WdbIeazGQez/f39bNq0ide97nX+MqXUvJv8SwghhDiYeF8KvS9/MgGYEEIIMTeUUhiGgWEYTScUawxlvdtt26ZYLJJOp9E0za8y9E5CzBeapvkBbT6fJ5fL+VmSZVkMDg4SCASIx+MS0Ip5b8bB7Otf/3pOOOEEfvCDHzSd/EsIIYQQc8MwDP+LnmVZ8iVOCCGEOEDouk44HCYcDgPVPrSWZfktDsrlcl37A8dxKBaLFItF//7BYNAPaaUfp5gPNE0jFosRjUYpFApks1n/yK9yuczg4KDfq9bbdoSYb2b8av7UU0/x61//mjVr1uyL8QghhBBiDwWDQQqFAlDt1yXBrBBCCHFg0jSNYDDoVwN6fWpLpZIf1NZOKFbbmxPGWh94YW1ji0EhDiZKKSKRCOFw2G9x4P39VyoVhoeHKRQKhMNhQqGQFAiKeWXGwew555zDAw88IMGsEEIIcYDxPqi6rkupVJrr4QghhBBimmr71AJ1E4oVCgUsy6qrqK1tfQD4IW8wGMQwDAmuxEFJKVVXQZtOp/2A1qsg13WdWCxGJBKRv3MxL8w4mL3gggu44oor2LBhA8ccc8y4PXMXXnjhrA1OCCGEENOnaRqmaVIul7FtG8dx0DRtroclhBBCiBlqNqFYbduDxtYH3sRJMPZ5IBgMEg6H0XV9rn4MIfZIbQVtPp8nk8n4Aa1t26RSKbLZrN/iQAJacTBTbu2r+TRM9gVvPk7+lU6naWlpIZVKkUgk5no4QgghxKRGRkb8wxw7OjqknYEQQggxD7mui2VZlEolcrlcXduDRoZh+C0PgsGg7LQVBx3HcSiVSuTz+XFHhZmmSSKRkEnCDkHzJa+bccXsZC/4QgghhJhbtUeyFAoFCWaFEEKIeUgp5U8EFovFJu1PW6lUqFQq5HI5lFJ+y4NwOCwhrTgoaJrmT55nWRbpdNoPaC3LYnBwkEAgQCKRkM++4qAjUzkKIYQQ80httUA+nyeRSMjhXUIIIcQ8NlF/2kKh4Ae1Htd1/V6dqVTKnzjMq6iVoFYc6EzTpL29nUKhQCaToVKpAFAulxkYGCAYDNLS0oJhSNwlDg7ylyqEEELMI4ZhEAgE/N5ztm3LB1MhhBDiENIY1DqOQ7lcplQqUSwW69oPesGtV00bCAQIhUKEQiHpTSsOaOFwmFAo5Ae03t91qVSir6+PeDxOLBaTAgVxwJNvakIIIcQ8EwwG/eoYy7IkmBVCCCEOYZqm+WFrIpHAsiwKhcK4kNZ1XX8SsVQqha7rhEIhwuEwpmlKwCUOOI2ThGWzWf9vOpPJUCqVaG1tlc/C4oAmf51CCCHEPFNb4TLfJuUUQgghxJ6r7U3b0tLiT6rknWo/N9i2TS6XI5fLoWma35s2GAxKNa04oCiliEajhMNhMpkMuVwOGGtvkEgkCIfDsnNBHJAkmJ3A+vXrWb9+vXyhFUIIcdCprQqQ9zEhhBBCTKR2UiWvN22xWKRUKmFZFq7rAtV2CIVCgUKhAFT7fEYiEWl5IA4omqbR0tJCIBAgnU5j2zaO4zAyMkKhUJDes+KApFzvlXYGNm3axLe+9S02bdrE5z//ebq6uvjjH//I0qVLOeqoo/bFOOdMOp2mpaWFVCpFIpGY6+EIIYQQU7Jtm927dwPVL04dHR1SISCEEEKIGfHCWK+adqLowDRNv1WC19dWiLnmBbLFYtFfppSitbWVcDg8hyMTs2W+5HUznnLx73//O8cccwx33XUXP//5z8lmswA8+OCDfPCDH5z1AQohhBBiZjRN878YWZaFZVlzPCIhhBBCHGw0TSMajdLW1saCBQtob28nFouNqzi0LItMJkN/fz8DAwNkMpm6alsh5oKmabS1tdHW1uYXKLiuy/DwMMPDw3JUmThgzDiYfc973sPHPvYx/vKXvxAIBPzlZ599Nu8raK0AAHEXSURBVP/4xz9mdXBCCCGEmDmvz5anVCrN4WiEEEIIcbBTShEMBkkkEnR2dtLZ2UksFqvLBKDa09MLaXfv3k06nZ602laIfS0UCtHV1UUoFPKXFQoF+vr6yOVy8rcp5tyMm2ts2LCB73//++OWd3Z2Mjg4OCuDEkIIIcTeqa1mcRxnDkcihBBCiPlEKYVpmv7RObZtUygUyOfzVCoVfz3Hcchms2SzWTRNIxKJEAwGCQQC0mJJ7Fe6rpNMJsnn82QyGRzHwXVdUqkUpVKJlpYW6ZUs5syMg9nW1lZ6enpYuXJl3fL77ruPxYsXz9rAhBBCCLHnNG3soBgJZoUQQgixr+i6TiwWIxaL+SGt15fWUxvS6rpOOByWkFbsV94RZeFwmFQq5U9kVywWsSyL1tZWgsHgHI9SHIpm3Mrgkksu4d3vfje9vb0opXAch9tvv52rr76ayy67bF+MUQghhBAzJMGsEEIIIfY3L6Rtb2+nu7u7adhl2zbZbJbBwUF2797tT9Akh5SL/UHTNFpbW2ltbfU/L9u2zeDgIKlUSv4OxX6n3Bn+1VmWxWtf+1p++MMf4rouhmFg2zaXXHIJ3/72t+dd+fd8meVNCCHEocV1XXp6eoDqbMmdnZ1zPCIhhBBCHKps26ZcLpPL5SiXy03XUUoRiUSIRqPjJhgTYl+wbZuhoaG6iXIDgQCtra3yN3gQmC953YyDWc+mTZu47777cByHE044gbVr18722A4I8+UXLYQQ4tCze/dubNtGKcWCBQvkUEEhhBBCzDnbtv1WBxNVypqmSTAYJBKJSEAm9inXdclms2QymbrlbW1tdROGiQPPfMnr9vgVbvXq1axevXo2xyKEEEKIWWSaJrZt47oulUrFn6RDCCGEEGKu6LpOJBIhEonguq4f0BYKBT+ktSwLy7LIZrN+QBsKhWQns5h1Sini8TjBYJChoSG/BdjQ0BDRaJREIiF/d2KfmnEw+/rXv37S27/5zW/u8WCEEEIIMXtM06RYLAJQKBQkmBVCCCHEAUUpRSgUIhQKEY/HyefzFAoFKpWKv45XXatpGuFwmEgkIp9pxKwLBAJ0dXUxNDTkt9vwWm+0tbXNu7ad4sAx42B2eHi47rplWTz00EOMjIxwzjnnzNrAhBBCCLF3QqGQf1iWbdtzPBohhBBCiInpuk48Hicej2PbNvl8nnw+73+GcRyHXC5HLpcjEAj4VbS1E54KsTc0TaO9vZ1UKkU+nweqmdfAwADJZJJAIDDHIxTz0YyD2V/84hfjljmOw1vf+lZWrVo1K4MSQgghxN6r/aIiM8wKIYQQ4mDhhbSxWIxyuUw+n6/rR1sulymXyyilCAaDhMNhaXUgZoVSipaWFkzTJJ1O47outm0zODhIR0eHVGuLWbfHk3812rhxI2eddZY/A/R8MV+aCQshhDj0OI5Db28vAMFgkPb29jkekRBCCCHEnnEch0KhQC6Xq2t14PFaHYTDYUzTlJBW7LVKpcLw8DCWZQFj/Wij0aj8fR0A5kteN2vTG27atKnpi6MQQggh5kbtB0ZvIgMhhBBCiIORpmlEo1EikQiWZVEoFCgUCv5nnNpWB4ZhEA6HicViEqCJPWYYBu3t7QwODmJZFq7rkk6nqVQqtLa2zvXwxDwx42D2yiuvrLvuui49PT387ne/4zWvec2sDUwIIYQQe0cphaZpOI4jrQyEEEIIMS8opQgEAgQCARKJBKVSiXw+T6lU8j/vVCoVMpkM+XyeWCxGKBSSyZvEHvH6zmYyGXK5HAD5fN6ftE6IvTXjYPa+++6ru65pGp2dnVx77bW8/vWvn7WBCSGEEGLveVUiEswKIYQQYr5RSvkBmdfqoFAoUC6Xgerkp6lUilQqRTgcJhqNygROYsY0TaOlpQVd10mn0wAMDw/T3t4uf09ir804mL3pppv2xTiEEEIIsQ94waxXNSuH8wkhhBBiPvJaHUSjUcrlMplMhlKp5N/uhbamaRKNRgmHw/K5SMyI97flTUQ3ODhIS0sLkUhkrocmDmLa1KsIIYQQ4mDlHbbnuq5UzQohhBDikBAIBGhvb6ezs5NYLIamjUUflmUxMjLC7t27/397dx4fVX3vf/x9Zs06kz3siKBIRFFDVLBq1UpFUdF6pWoRtbY/bnGh6NVyrXsr1ut1aQlaayvWumAVqa1WxYVFEUUERQEVRQEhJGSZyT6ZmfP7Y27GDCGQgSSTM3k9H495OHPmzJnPZI5jfOczn6/8fr9CoVACK4WVGIah7OxsOZ1OSZHfr2tqalRbW8vv2dhvneqYPfroozv9l6QPP/zwgAoCAABdp+08tVAoFPM/JgAAAMnM6XTK6XQqMzNTjY2Nqq+vV0tLi6TIt4nq6upUV1enlJQUZWRkyOl00kWLvTIMQ3l5efL7/dGZs7W1tTIMQxkZGQmuDlbUqWB28uTJ3VxG93I4HBo9erQkaezYsXr00UcTXBEAAD2jbTAbDAajf+EHAADoKwzDUFpamlJTU9XS0qL6+vro19ElqampSU1NTXI4HEpLS1NaWhp/zEaHDMOQx+ORzWZTbW2tJKmuro7zBvvFMPtAv3VeXp527dq1X4/1+/3yer3y+XzyeDxdXBkAAN2rqalJVVVVkiJzsbxeb4IrAgAASLxQKKSGhgbV19crHA7H3Gez2aJBLn/Uxt5UVlZGZxnb7XZlZ2ezIFgPSZa8jigfAIAk1vYXw9YVigEAAPo6u92uzMxMFRYWKisrK+Z3ptYxBxUVFaqqqoqOPwB2l5WVFe2SDYVCnC+IW9zBbCgU0r333qtjjz1W/fr1U05OTswlXsuWLdPZZ5+tAQMGyDAMLVq0qN0+8+bN07Bhw5SSkqLi4mItX748rufw+/0qLi7W9773PS1dujTuGgEAsCqbzRbt9GhpaWnXEQIAANCXtY45yMvLU35+vtxud8z9TU1N2rVrl2pra/k9Cu3Y7Xbl5eVFg/1wOKzKykoWlUOnxR3M3n777brvvvt04YUXyufzadasWTr//PNls9l02223xV1AfX29xowZo7lz5+7x/gULFmjmzJm66aabtGbNGp144omaOHGitmzZEt2nuLhYo0ePbnfZvn27JOnrr7/W6tWr9fDDD+vSSy+V3++Pu04AAKzK4fhupDz/QwEAALBnTqdTubm5KigoUEZGRrQT0jRN1dbWaufOnaqrq+P3KcRwOBzKycmJNkOEw2HV1NSoD0wORReIe8bs8OHD9fvf/15nnXWWMjMztXbt2ui2lStX6qmnntr/YgxDL7zwQsxiY8cdd5yOOeYYPfTQQ9Fto0aN0uTJkzVnzpy4n2PixIm68847NXbs2E7tnywzKwAAfZfP54uuGtv2L/oAAADoWOtIg7q6upjthmEoPT1d6enpMQutom8Lh8MqLy+PBvc5OTlKSUlJcFXJK1nyurg7ZsvKynTEEUdIkjIyMuTz+SRJkyZN0ksvvdSlxQUCAa1evVoTJkyI2T5hwgStWLGiU8eorq6ODmLetm2b1q9fr4MPPrjD/Zubm+X3+2MuAABYWdvVYenwAAAA6BybzSaPx6O8vDylpqZGt5umqbq6OpWXl6uuro7OSEiKnC9tF9qtqqpSY2NjAiuCFcQdzA4aNEg7duyQJI0YMUKvvfaaJGnVqlXtZrEcqF27dikUCqmwsDBme2FhocrKyjp1jA0bNmjs2LEaM2aMJk2apAcffHCvs3DnzJkjr9cbvQwePPiAXgMAAIlGMAsAALD/XC6XsrOzlZeXF9MBaZqm/H6/Kioqog1h6NtSUlJisrG2zYLAnjj2vUus8847T2+88YaOO+44XXvttbrooov05z//WVu2bNEvf/nL7qhRhmHE3DZNs922jowfP17r1q3r9HPNnj1bs2bNit72+/2EswAASyOYBQAAOHAul0s5OTkKBoPy+/1qamqSJAWDQVVWVsrpdCozM1Nut7vTmQWSi2EYys7OVk1NTfT8qK6uVkFBQczv5ECruIPZu+++O3r9ggsu0ODBg/XOO+9oxIgROuecc7q0uLy8PNnt9nbdseXl5e26aLuK2+3u8s5fAAASiWAWAACg67Qu9hQIBOT3+xUIBCRJLS0tqqqqktPpVFZWVnQxKPQtNptN2dnZqqioUDAYVDgc1q5du5SXl0c4i3biPiMaGhpibh933HGaNWtWl4eyUuSvUcXFxVq8eHHM9sWLF2v8+PFd/nwAACQjglkAAICu53K5lJub2y6EbWlpUUVFhaqqqhQMBhNYIRKltXO29ffw1i5rYHdxB7MFBQX6yU9+oldffbVL/ueurq5Oa9eu1dq1ayVJmzdv1tq1a7VlyxZJ0qxZs/Too4/qL3/5izZs2KBf/vKX2rJli6ZPn37Az703paWlKioqUklJSbc+DwAA3a1tMBsKhRJYCQAAQHIxDENpaWnKy8tTTk6O7HZ79L6mpiaVl5erpqaG38H6IKfTqdzc3OhYi4aGBoJ6tGOYcS4fuHDhQj399NN66aWX5PF4NGXKFP3kJz/Z7wBzyZIlOuWUU9ptnzZtmubPny9Jmjdvnu655x7t2LFDo0eP1v3336+TTjppv54vXn6/X16vVz6fTx6Pp0eeEwCArmSaZnThTqfTqfz8/ARXBAAAkJzC4bDq6+tVX18f08xmGIYyMzOVlpbG19n7GL/fr7q6OkmR38Xz8vKYQdwFkiWvizuYbVVbW6vnnntOTz/9tN566y0NGzZMP/nJT3TLLbd0dY0JlSxvNACgbysrK1M4HJbdbu+2Oe0AAACICIfDqqurU319vdrGLjabTVlZWUpJSUlgdehJoVBIFRUV0aA+KytLaWlpCa7K+pIlr9vvYLat9evX65JLLtHHH3+cdO35yfJGAwD6tvLycgWDQRmGof79+ye6HAAAgD6hdbZoU1NTzPaUlBR5PB45HHGvyQ4LamxsVHV1taRIOJ+Xl8d7f4CSJa/b7/75pqYmPfvss5o8ebKOOeYYVVZW6vrrr+/K2hKKGbMAgGTS+pU50zTVBX+TBQAAQCc4HA7l5OSooKBAbrc7ur2pqUkVFRXtOmqRnFJSUqJd0uFwWFVVVbzvkLQfHbOvvfaannzySS1atEh2u10XXHCBLrnkEp188sndVWNCJUsCDwDo26qqqqKdGoWFhTELUwAAAKD7maapxsZG+f3+mPmzqamp8ng8/H6W5EKhkMrLy6OBrNfrVXp6eoKrsq5kyevi7puePHmyzjrrLD3++OM666yz5HQ6u6MuAADQhdouMhEKhfjFHwAAoIcZhqG0tDSlpKSoqqpKgUBAUuRr7o2NjUpPT5fH42FhqCRlt9uVlZUVHWlQV1entLQ03u8+Lu5gtqyszNJJNAAAfVHbILZthwYAAAB6VuuM0bq6OtXW1kY7KOvr6xUMBpWVlcUf0ZNUamqqGhsb1dTUpFAopMbGRhYC6+PinjFLKAsAgPW0/eU+2RbqBAAAsKKMjAwVFBTEBHPNzc0qLy9XfX19AitDd0pNTY1e9/v9/G7ex+334l/JjsW/AADJhGAWAACg92n9entOTk70K+2macrn88nn87FAVBLafSEwv9+f4IqQSHEv/tXXJMswYQBA3xYMBlVeXi4p8lf67OzsBFcEAACAtoLBoHw+n5qbm6PbnE6nPB6P3G53AitDVwuFQqqoqIiOGMvPz2cNpzglS15HxywAAH3A7ot/AQAAoHdxOBzKyclRRkZGdFtLS4sqKytjZtHC+ux2u9LT06O36Y7uu+IKZoPBoBwOhz755JPuqgcAAHQDm80WDWdZ/AsAAKB3MgxDHo9Hubm5cji+W6+9trZWlZWVhHdJJD09PTpuLBAIqLGxMcEVIRHiCmYdDoeGDh1Kpw0AABbUGszy33EAAIDeze12Kz8/X5mZmdFtgUBAlZWVamlpSWBl6Co2m01erzd6m67ovinuUQa//vWvNXv2bFVVVXVHPQAAoJu0LigBAACA3s8wDGVmZsasDRAIBLRr166YObSwrrYLgYVCId7XPsix711i/f73v9emTZs0YMAADR06NGYmhiR9+OGHXVZcIpWWlqq0tJSuIgBA0jFNU6ZpEtQCAABYQGpqqkzTjM4hNU1TVVVVysnJYVGwJJCamqqmpiZJUnNzczSoRd8QdzA7efLkbiij95kxY4ZmzJgRXeUNAACrI4gFAACwprS0NLndblVVVamlpSUazmZlZSk1NTXR5eEAuN1uGYYh0zTV0NCgtLQ0OZ3ORJeFHhJ3MHvrrbd2Rx0AAKCbtQ1m6ZgFAACwFrvdrtzcXFVVVSkQCMg0TVVXV8s0TaWlpSW6POwnm82m9PR01dXVyTRN1dbWKicnJ9FloYfEPWMWAAAAAAAAPc9msyknJ0culyu6raamhoWjLC4zMzO6UG9TUxMLvPUhcQezoVBI9957r4499lj169dPOTk5MRcAANA77d4xCwAAAOux2WzKzc2NmUVaW1ur+vr6BFaFA2EYRkzXc0NDQwKrQU+KO5i9/fbbdd999+nCCy+Uz+fTrFmzdP7558tms+m2227rhhIBAEBXYHQBAABAcjAMQ9nZ2TFhnt/vV2NjYwKrwoHIyMiIXm9sbGQx+j4i7mD2ySef1J/+9Cddf/31cjgcuuiii/Too4/qlltu0cqVK7ujRgAA0MXomAUAALA2wzCUlZWl9PT06Lbq6mrCWYuy2WzRLuhwOCyfz5fgitAT4g5my8rKdMQRR0iKpPmtJ8qkSZP00ksvdW11CVRaWqqioiKVlJQkuhQAALoEowwAAACSj8fjkdvtjt4mnLUur9cbM2s2HA4nuCJ0t7iD2UGDBmnHjh2SpBEjRui1116TJK1atSrmg8DqZsyYofXr12vVqlWJLgUAgC7BKAMAAIDkYxiGcnJyYsYaVFdXq6amhj/GW4zdbo+ZHcwiYMkv7mD2vPPO0xtvvCFJuvbaa3XzzTfrkEMO0aWXXqorrriiywsEAABdj1/SAQAAkodhGPJ6ve0WkKqurub3PotxuVzR683NzQmsBD3BEe8D7r777uj1Cy64QIMGDdKKFSs0YsQInXPOOV1aHAAA6DqMMgAAAEhereGs3W5XbW2tpMjX4evr62MWlkLv1jaYra+vV2pqqpxOZwIrQneKO5jd3fHHH6/jjz++K2oBAAAAAADAfjIMQ5mZmbLb7aqpqZEk+f1+uVyumMAPvZfD4VBqaqoaGxtlmqbq6+uVlZWV6LLQTToVzL744oudPiBdswAA9E50zAIAAPQNqampamhoUCAQkCRVVla2G3WA3svr9aqpqUmmaaqxsVEejye6KBiSS6eC2cmTJ3fqYIZhKBQKHUg9AACgmxDMAgAA9A2GYSg7O1sVFRUKh8MyTVM1NTVyOBx0zlqAzWaLhuumaaqurk4ejyfRZaEbdCpuD4fDnboQygIA0HsRzAIAAPQddrtdBQUFSklJiW6rrKxUXV0dvwtaQNu5wHV1dWRuSYo+aAAA+oi2X38Kh8MJrAQAAAA9wWazKTs7O7p4lGma8vv9qq6uJpzt5RwOR8yiX42NjQmsBt0l7sW/7rjjjr3ef8stt+x3Mb1JaWmpSktL+YsEACBpdGnH7PbVUt1O6dAz9/8YX74uudKkweOl6q+kjf+Q7C7p2BmdP0bVV5LdKXkHR26XfyqlF0jp+ftXR3fyb5N2fiwdcgA/MwAAgDgZhqHc3Fz5/X41NDRIkpqamuTz+eT1emN+R0TvkpmZqaqqKkmRYLZtFy2SQ9zB7AsvvBBzu6WlRZs3b5bD4dDw4cOTJpidMWOGZsyYIb/fL6/Xm+hyAAA4YL16lEHZR9KwU6V+Y+J7XPVXkVC1NZitWC8ZtviC2a5mhiM17M4zKHIBAADoYTabTVlZWXK73aqpqZFpmmpoaJBhGPJ4PISzvZTb7ZbdblcoFFJLS4sCgQAzgpNM3MHsmjVr2m3z+/267LLLdN5553VJUQAAoOu1G2Ww/QNp23tSOCjlF0nDT5caq6WP/ipl9pd826TMftLoiyTDkMrXS5tellweKS1XcqS2f5Kyj6Sv3pBsdiklWzrqUinYHOmGbSiXTEkjJ0lZB333mO0fSjvXSVVfRILWUXv4fcK3Rfr8JSnUIjlTpdFTIgHot+9FQtBvP4gct2KDVLNZ+nKxdMxPpY+fjIS21V9KoaB0xEVSRmHHP6TNSyJ1Fl2w53B16Z1S4ZFSzTdSanakDrtLevt30oCxUuXnkZ+j/1tpx//9znTQyVL/oyPdvdvelY68pDNvFwAAQJdLTY38/lZdXS1Jqq+vV1NTk3Jzc+VwxB0RoZsZhiGn0xn9NndzczPBbJLpkn/rPB6P7rjjDk2aNElTp07tikMCAIAu1rYTwmiokHZ9JpX8Z2TDR09Ewka3R6qviASYafnS6j9JNV9HOj0/e1EqmS65MiPbvUPaP8nmtyJhbFqeFGz6blvB4VLBhVKTT1o7Xzr+2u8eM+CYSHBaMFrKH7Xn4tMLpLH/LxKWlq2Vvl4WCWIHHhc7hiB/VPvj2BzSsVdFwtstb0tFP9rzc2x+S2qo6DiUlaSWBilnuHTYuZGfx9Z3I8GrJDnTIj/P1pEFx86Qwi3S+6WRxwAAAPQCqampCofD8vl8kqRQKKRdu3YpJyeH0K8XyszMVFNT5PfqYDCY4GrQ1brszyE1NTXRf6kBAEDv07Zj1u7bHOlCfW9uZEOoWWqsigSzaXmRIFSSMgdITdWSwx0ZD5CSFdleeEQkZN1d1lBpwwtSv6MiAakU6YSt/CzSSStFws1wnL9UtjRKnzwbqdEMf1dHZ+QXRf7pGRgJdffk21WRIPqIH3ccykqRkDf/8Mj1wjHSN8u+u6/wiMg/a76OvHa7M3LJGR7pPnaktDscAABAIqSnp8vhcKiyslJS5NtUu3btktfrVXp6eoKrQ1ttO5kJZpNP3MHs73//+5jbpmlqx44deuKJJ3TGGWd0WWEAAKBr2Ww22Ww2hcPhyCiDgcdKB58Wu1NjdSR8bGXYJNOMjCBQJ2aPHTY5Evju2ii99wdp3C8j24+aFl+YuruvFkt5I6VBx0WO/8UrnX+szR75p2FEQt09yegn1ZVJgXrJnbl/Ndqce95uqlM/OgAAgJ7kdruVn5+vqqqq6FflfT6fDMNQWlpagqtDK8Mw5HA4FAwGFQwGZZomM4GTSNzB7P333x9z22azKT8/X9OmTdPs2bO7rDAAAND1HA6HAoGAmlIHKnPn6zIGj4/MbG3yxQayu0vPj3zNv8knuTKk8k8kz+D2+zVWRbpmvUMi+7Q0SDkjIl/5P2RiZJ/aHZEZtvEINksp/7cY54428+4dbikY+O623R3p/o2Xd3BkDuxHj0tH/zTyM9mTcDCywFjB4ZFxBd6h7ffJOkjasEgaemJkJm71l9LwH0j1u+KvCwAAoBs5nU4VFBSopqZGjY2NkiLfiJZEONuLuFyuaCjb1NQUnRUM64s7mN28eXN31AEAAHpAazAbSs1TaND35Fj9SKQj1uGOzJXtiN0pHXq29OGjkWC2o2D1i5elhshX4lQwOhKmDjtV+uyf0rsPSDKl7OHSYefEV/jQE6VPn4sszpXVJgzNO0xa91QkJB19odTvyMgoha+XRRb/6kjF+sgCXcNP/25b7qGRkQkfPSEdfXnkNe/OmRbp2P3qjUgH8Igftt/HMygy1qB1TMTBP/i/2b0EswAAoPcxDENZWVmSFBPONjc3y+v1xozDQmKkpqaqoaFBkghmk4xhmqaZ6CJ6M7/fL6/XK5/PJ4/Hk+hyAAA4IK1fU7PZbHwFan8svVM6+eb9e+zOdZF5u6PO79qaAAAAuoBpmvL7/aqvr49us9vtysnJkdPZwcgm9AjTNFVWVibTNOVwOFRQUJDokhIuWfK6uDtmzzvvvD3+j5xhGEpJSdGIESN08cUXa+TIkV1SIAAA6Dp2uz3RJfRNuz6TvnxNOvw/El0JAADAHhmGIa/XK5fLJZ/Pp3A4rFAopMrKSuXk5MjlciW6xD6rdc5sS0sLc2aTTNwds5dddpkWLVqkrKwsFRcXyzRNrVmzRjU1NZowYYI++ugjff3113rjjTd0wgkndFfdPSZZEngAACyh8vP2C3tlHySNjHP0wYH66InIQmhtjZ4iZRT2bB0AAAAJ0BrIBoNBSZFg0OPxKD09PcGV9V1VVVVqamqSJBUUFMjhiLvXMqkkS14X97vYr18/XXzxxZo7d250zkg4HNa1116rzMxMPfPMM5o+fbpuvPFGvf32211ecE8pLS1VaWlp9CufAACgB+QeGrkk2pipia4AAAAgYex2u3Jzc1VdXa1AICDTNOXz+dTS0iKPx8Pc2QRo+823UCjU54PZZBF3x2x+fr7eeecdHXpo7P80ff755xo/frx27dqldevW6cQTT4yu5GdlyZLAAwAAAAAAxKM1kG1deEqKLCabk5NDMNjDGhoaojlbZmamMjMzE1tQgiVLXhf3nziCwaA2btzYbvvGjRuj3aUpKSnMugAAAAAAALCw1rmzHo8nmvMEg0GVl5ertrZWrCffc9rO+G1paUlgJehKcf95Y+rUqfrpT3+q//7v/1ZJSYkMw9D777+vu+66S5deeqkkaenSpTr88MO7vFgAAAAAAAD0HMMwlJGRIZfLpaqqKoXDYUlSbW2tWlpalJ2dTXNeD7Db7TIMQ6ZpRmf/wvriDmbvv/9+FRYW6p577tHOnTslSYWFhfrlL3+pG2+8UZI0YcIEnXHGGV1bKQAAAAAAABLC5XIpPz9ftbW10dEGTU1Nqq2tVWZmJuFsNzMMQw6HQy0tLQoGgwoGg4yTSAJxz5hty+/3S5KlZznsS7LMrAAAAAAAAOgK9fX18vl80dt2u13Z2dkxX7dH1/P7/aqrq5MUyeIyMjISXFHiJEted0DL6Hk8Hku/eAAAAAAAAMQnPT09ZvGpUCikXbt2qbq6mrmz3Sg1NTV6PRAIJLASdJW4g9mdO3dq6tSpGjBggBwOh+x2e8wFAAAAAAAAyS0zM1P5+flyOp3RbY2NjSovL2dxqm7icDhiFmGD9cU9jOKyyy7Tli1bdPPNN6t///7MEAEAAAAAAOiDnE6ncnNzVV1drebmZkmR7tmKigp5vV6lp6cnuMLk0nbObCgUkmma5HIWF3cw+/bbb2v58uU66qijuqEcAAAAAAAAWIXNZlNOTo4CgYCqq6sVDoclKTqDlnC2a7UGs6ZpKhAIyO12J7okHIC4RxkMHjyYeSEAAAAAAACQFOnkdLvdys/Pj1kAzOfzqb6+PoGVJZ+2QWxrlzKsK+5g9oEHHtCvfvUrff31191QDgAAAAAAAKzIbrcrLy9PGRkZ0W0+n081NTUKhUIJrCx5tJ3py8/U+uIeZTBlyhQ1NDRo+PDhSktLizkhJKmqqqrLigMAAAAAAIC1ZGZmSpLq6uokSQ0NDWpsbFROTg5fvT9Adrs9er11bASsK+5g9oEHHuiGMgAAAAAAAJAMDMOQx+ORw+GQz+eTaZoyTVOVlZUsCnaA2i72RTBrfXEHs9OmTeuOOgAAAAAAAJBE0tLS5Ha7VVVVpZaWFkmR0QaBQEAejyem+xOdYxiG7Ha7QqGQQqGQTNOMCWthLZ0KZv1+vzweT/T63rTuZ3WlpaUqLS1lXgcAAAAAAMB+stvtys3NVW1tbXQhsMbGRjU3NysnJydmsTB0jsvlUmNjo8LhsAKBAOMhLMwwTdPc1052u107duxQQUGBbDbbHpP41oQ+2YJMv98vr9crn8+XNKEzAAAAAABATzJNUw0NDfL7/WobRTHaIH4NDQ2qqamRFGmQbLvYWl+RLHldpzpm33zzTeXk5EiS3nrrrW4tCAAAAAAAAMnFMAylp6crJSWl3WiDYDAoj8fDV/I7yel0Rq8HAoEEVoID1amO2b4sWRJ4AAAAAACA3sA0TdXU1KixsTG6zeFwyOv18rX8TjBNU2VlZTJNU3a7XYWFhYkuqcclS15ni/cBr7zyit5+++3o7dLSUh111FG6+OKLVV1d3aXFAQAAAAAAILkYhqGsrKyYr+AHg0FVVlaqrq4ugZVZg2EY0a7Z1kXAYE1xB7P/9V//FV0AbN26dZo1a5bOPPNMffXVV5o1a1aXFwgAAAAAAIDkYhiGPB6PcnNzZbN9F0/5/f7oImHoWNtF01rHQsB6OjVjtq3NmzerqKhIkvT888/r7LPP1l133aUPP/xQZ555ZpcXCAAAAAAAgOTkdrtVWFgYE8j6/X7Z7XalpKQkuLreq+2c2WAwmMBKcCDi7ph1uVxqaGiQJL3++uuaMGGCJCknJyfaSQsAAAAAAAB0hmEY8nq9Sk9PlxSZoVpVVSW/3y+WRtozh+O7XkuCWeuKu2P2e9/7nmbNmqUTTjhB77//vhYsWCBJ+vzzzzVo0KAuLxAAAAAAAADJz+PxKBwORxcFq6urUyAQUHZ2tux2e4Kr613a/jwIZq0r7o7ZuXPnyuFw6LnnntNDDz2kgQMHSpL+/e9/64wzzujyAgEAAAAAAJD89rQoWCAQUEVFBeHjbmw2mwzDkCSFw+EEV4P9ZZj0hO+V3++X1+uVz+eTx+NJdDkAAAAAAABJr7m5WdXV1dHQ0WazyePxKDU1NRpI9nU7d+5UKBSSzWZTv379El1Oj0qWvC7ujlkAAAAAAACgO7ndbuXn50dnqYbDYdXU1MSEtX1d6ziDcDjMLF6LIpgFAAAAAABAr2O325WXl6fU1NTotqamJpWXl6upqSmBlfUONtt3sR5htTURzAIAAAAAAKBXstlsysrKktfrjZmpWlVVpZqamj7dKUowa30EswAAAAAAAOi1DMNQenq68vLy5HQ6o9sbGhr6dDjbOspAkkKhUAIrwf4imAUAAAAAAECv53Q6lZeXJ4/HE+2ebWxsVEVFRZ8MJtsGs8FgMIGVYH85OrPT+eef3+kDLly4cL+LAQAAAAAAADpiGIYyMjJks9lUU1MjKRJKVldXKzc3NxrY9gV0zFpfpzpmvV5v9OLxePTGG2/ogw8+iN6/evVqvfHGG/J6vd1WKAAAAAAAACBJaWlpMUFsIBBQeXl5nwooCWatr1Mds4899lj0+o033qgLL7xQDz/8cPQECIVC+sUvfiGPx9M9VQIAAAAAAABtuN1u5eTkqKqqSqZpKhQKqbKyUnl5eTELYyUrglnrM8w4JyTn5+fr7bff1siRI2O2f/bZZxo/frwqKyu7tMBE8/v98nq98vl8BM8AAAAAAAC9THNzc0weZbPZlJ+fHxNcJqudO3dGQ9mCggI5HJ3qwbS8ZMnr4v7zQTAY1IYNG9pt37Bhg8LhcJcUBQAAAAAAAHSG2+1WXl5edKxBOBzWzp07+8SCWCkpKdHrfeH1Jpu4Y/TLL79cV1xxhTZt2qTjjz9ekrRy5Urdfffduvzyy7u8wK6wefNmXXHFFdq5c6fsdrtWrlyp9PT0RJcFAAAAAACALuByuZSXl6eKiorotoqKCmVnZ8eEl8mmbYcsDZPWE3cwe++996pfv366//77tWPHDklS//79dcMNN+i6667r8gK7wmWXXabf/OY3OvHEE1VVVSW3253okgAAAAAAANCFnE6n8vLyVFlZKdM0ZZqmqqqq5PV6k7ZBr+0sXYJZ64l7xmxbfr9fknr1LIdPP/1U1157rV5//fX9enyyzKwAAAAAAADoC1oXAWv71f5kDWfbztfNyMjoM9lVsuR1+7VEXTAY1Ouvv66nn346Or9j+/btqquri/tYy5Yt09lnn60BAwbIMAwtWrSo3T7z5s3TsGHDlJKSouLiYi1fvrzTx//iiy+UkZGhc845R8ccc4zuuuuuuGsEAAAAAACANdjtduXn5ys1NTW6zefzye/36wD6E3ul1lxOUtK9tr4g7lEG33zzjc444wxt2bJFzc3NOv3005WZmal77rlHTU1Nevjhh+M6Xn19vcaMGaPLL79cP/rRj9rdv2DBAs2cOVPz5s3TCSecoD/+8Y+aOHGi1q9fryFDhkiSiouL1dzc3O6xr732mlpaWrR8+XKtXbtWBQUFOuOMM1RSUqLTTz893pcOAAAAAAAACzAMQ1lZWbLZbKqvr5ck1dXVqaWlRdnZ2TEjAKzMbrdHr4dCoQRWgv0RdzB77bXXauzYsfroo4+Um5sb3X7eeefpyiuvjLuAiRMnauLEiR3ef9999+mnP/1p9NgPPPCAXn31VT300EOaM2eOJGn16tUdPn7QoEEqKSnR4MGDJUlnnnmm1q5d22Ew29zcHBPyto5rAAAAAAAAgHUYhiGv1yu73R7Nd5qbm1VTU6Ps7OyYblOrahswE8xaT9x/Hnj77bf161//Wi6XK2b70KFD9e2333ZZYZIUCAS0evVqTZgwIWb7hAkTtGLFik4do6SkRDt37lR1dbXC4bCWLVumUaNGdbj/nDlz5PV6o5fWQBcAAAAAAADWk5GRoZycnGiI2dTUpIaGhgRX1TUMw4h2zRLMWk/cwWw4HN7jG71t2zZlZmZ2SVGtdu3apVAopMLCwpjthYWFKisr69QxHA6H7rrrLp100kk68sgjdcghh2jSpEkd7j979mz5fL7oZevWrQf0GgAAAAAAAJBYKSkpysrKit72+XxJE862BrPhcJg5sxYT9yiD008/XQ888IAeeeQRSZFkvq6uTrfeeqvOPPPMLi+w9TnaMk0zrnbzfY1LaMvtdsvtdsdVHwAAAAAAAHq3lJQUpaWlRQPZmpoahcNhZWRkJLiyA9M2IwuHwzFzZ9G7xR3M3n///TrllFNUVFSkpqYmXXzxxfriiy+Ul5enp59+ukuLy8vLk91ub9cdW15e3q6LFgAAAAAAANgbr9crSdFw1u/3KxQKyePxWHbmbNs5s3TMWkvcowwGDBigtWvX6vrrr9f/+3//T0cffbTuvvturVmzRgUFBV1anMvlUnFxsRYvXhyzffHixRo/fnyXPtfuSktLVVRUpJKSkm59HgAAAAAAAPSM1gXB2o7jrK+vV01NTeKKOkBtg9lwOJzAShAvw0xwlF5XV6dNmzZJko4++mjdd999OuWUU5STk6MhQ4ZowYIFmjp1qh5++GGNGzdOjzzyiP70pz/p008/1dChQ7u9Pr/fL6/XK5/PJ4/H0+3PBwAAAAAAgO5XX18vn88XvZ2Tk6OUlJQEVrR//H6/6urqJFn3NcQrWfK6uEcZ2O12nXTSSXr++eeVk5MT3b5z504NGDAg7hXgPvjgA51yyinR27NmzZIkTZs2TfPnz9eUKVNUWVmpO+64Qzt27NDo0aP18ssv90goCwAAAAAAgOSUnp4u0zTl9/slRWbO5uTkyOVyJbiy+DDKwLri7pi12Ww6/vjjVVZWphdffFGjR4+WFAlm+/fvn3Qt08mSwAMAAAAAACCWaZqqqqpSc3NzdFtubq6lFoZvaGiIjmLwer1KT09PbEE9IFnyurhnzBqGoeeff15nn322xo8fr3/84x8x9yULZswCAAAAAAAkN8MwlJ2dHRPEVlVVxf2N8ERqm8clW8Nksos7mDVNU3a7XQ8++KDuvfdeTZkyRb/5zW+SrlV6xowZWr9+vVatWpXoUgAAAAAAANBNbDabcnJy5HBEJn6apimfz2eZrItRBtYV94zZtn7+85/r0EMP1QUXXKClS5d2VU0AAAAAAABAjzEMQ7m5uSovL5dpmmpqapLP51NWVlaiS9untsEsHbPWEnfH7NChQ2W326O3v//972vlypXatm1blxYGAAAAAAAA9BS73a7s7Ozo7YaGBlVXV/f6LlRGGVhX3B2zmzdvbrdtxIgRWrNmjXbu3NklRQEAAAAAAAA9LSUlRZmZmaqtrZUkNTY2yjAMeb3eXru2EqMMrCvujtmOpKSkaOjQoV11OAAAAAAAAKDHZWRkKCMjI3q7oaGhV8+cpWPWujrVMZuTk6PPP/9ceXl5ys7O3utfCKqqqrqsuEQqLS1VaWmppVbhAwAAAAAAwIExDEMej0emaaq+vl5SJJx1uVxKS0tLcHXtGYYhm82mcDhMMGsxnQpm77//fmVmZkqSHnjgge6sp9eYMWOGZsyYIb/fL6/Xm+hyAAAAAAAA0IO8Xq9sNlt0rEFNTY2cTqecTmeCK2uvNZjtrV292DPD5B3bq9Zg1ufzyePxJLocAAAAAAAA9BDTNLVz586YTtT+/fv3unmzFRUVamlpkdQ76+tqyZLXdapj1u/3d/qAVv5hAAAAAAAAAK0Mw1Bubq4qKiqi22pra3td/rX7AmDJHswmi04Fs1lZWft8Q1vfdGayAgAAAAAAIFk4nU5lZWWppqZGklRXV6fU1NReNdJg92AW1tCpYPatt97q7joAAAAAAACAXiktLU319fXRcQFVVVXKzc2Vw9GpaK1HEcxaR6fOnpNPPrm76+h1SktLVVpaSgcwAAAAAAAAlJ2drfLycklSKBRSVVWV8vPze8XYgLY1EMxax34v/tXQ0KAtW7YoEAjEbD/yyCO7pLDeIlmGCQMAAAAAAODANDc3q7q6OroYWGZmpjIzMxNcleTz+VRfXy9JysvLk8vlSnBF3StZ8rq4+60rKip0+eWX69///vce76fDFAAAAAAAAMnI7XYrLy9PFRUVMk1TtbW1crlccrvdCa2Ljllrsu17l1gzZ85UdXW1Vq5cqdTUVL3yyit6/PHHdcghh+jFF1/sjhoBAAAAAACAXsHhcCg1NTV6u6qqSsFgMIEVEcxaVdwds2+++ab+8Y9/qKSkRDabTUOHDtXpp58uj8ejOXPm6KyzzuqOOgEAAAAAAIBewePxqKWlRS0tLdHO2ezs7ESXBYuJu2O2vr5eBQUFkqScnBxVVFRIko444gh9+OGHXVsdAAAAAAAA0MvYbDbl5uZGO1UbGxujM14TgY5Za4o7mB05cqQ+++wzSdJRRx2lP/7xj/r222/18MMPq3///l1eIAAAAAAAANDb2Gy2mIWnfD6fAoFAQmohmLWmuEcZzJw5Uzt27JAk3XrrrfrhD3+oJ598Ui6XS/Pnz+/q+hKmtLRUpaWlLGYGAAAAAACAPUpLS1NjY2M0kK2urlZBQUFMUNoTCGatyTAP8N1qaGjQxo0bNWTIEOXl5XVVXb2G3++X1+uVz+eL+SsIAAAAAAAAYJqmysrKooFoRkZGj2dIjY2Nqq6ulhSZf5uRkdGjz9/TkiWvi3uUwe7S0tJ0zDHHJGUoCwAAAAAAAOyNYRjKysqK3q6rq1Nzc3PiCoJlxD3KwDRNPffcc3rrrbdUXl6ucDgcc//ChQu7rDgAAAAAAACgt0tNTVVLS4vq6uokSTU1NcrPz5fNdsA9kZ3SdpTB7lkdeq+4z45rr71WU6dO1ebNm5WRkSGv1xtzAQAAAAAAAPqazMxMud1uSVIoFFJtbW2PPbfdbo9eJ5i1jrg7Zv/2t79p4cKFOvPMM7ujHgAAAAAAAMByDMOQ1+tVRUWFTNNUfX29UlNT5XK5uv2523bmEsxaR9wds16vVwcffHB31AIAAAAAAABYlsPhUGZmZvS23++PLgrWndoGs6FQqNufD10j7mD2tttu0+23367GxsbuqAcAAAAAAACwrPT0dDkckS+pBwIB+Xy+bn9OwzCi4Swds9YR9yiD//iP/9DTTz+tgoICHXTQQXI6nTH3f/jhh11WXCKVlpaqtLSUvzIAAAAAAACg0wzDUGZmpqqrqyVJDQ0NyszMjJkD213PK6lHOnTRNeIOZi+77DKtXr1aP/nJT1RYWBiz6lsymTFjhmbMmCG/38+iZgAAAAAAAOi01NRUNTU1Rb9x3tTUpPT09G59zmTN6JJZ3MHsSy+9pFdffVXf+973uqMeAAAAAAAAwPIyMjKiwWxDQ4PS0tK6NTylY9Z64p4xO3jwYHk8nu6oBQAAAAAAAEgKTqczOgK0paVFTU1N3fp8bYNZwllriDuY/d///V/dcMMN+vrrr7uhHAAAAAAAACA5ZGRkRK/X19cTmCJG3KMMfvKTn6ihoUHDhw9XWlpau8W/qqqquqw4AAAAAAAAwKpSUlJks9kUDocVCARUWVmp3Nzcbhlp0PaYpmkyc9YC4g5mH3jggW4oAwAAAAAAAEguhmHI6/WqurpakhQIBNTU1KTU1NRuea5WdOZaQ1zBbEtLi5YsWaKbb75ZBx98cHfVBAAAAAAAACSF1NRUNTY2RmfMVldXKyUlpcs7WglmrSeuGbNOp1MvvPBCd9UCAAAAAAAAJJ2srKyY2y0tLV3+HASz1hP34l/nnXeeFi1a1A2lAAAAAAAAAMnHZrPFjC/w+/1d/hwEs9YT94zZESNG6M4779SKFStUXFys9PT0mPuvueaaLisOAAAAAAAASAaZmZlqbGyUFJk1GwqFZLfbu+z4LPZlPYYZZ4Q+bNiwjg9mGPrqq68OuKjeoLS0VKWlpQqFQvr888/l8/nk8XgSXRYAAAAAAAAsqqamRg0NDZIkr9fbruHxQPj9ftXV1UmScnNz5Xa7u+zYvY3f75fX67V8Xhd3MNvXJMsbDQAAAAAAgMRqbGxUdXW1pEiDY0FBQZd1zRLMWk/cM2bbMk2TmRUAAAAAAABAJ7jd7mgQa5qmampqElsQEmq/gtm//vWvOuKII5SamqrU1FQdeeSReuKJJ7q6NgAAAAAAACBp2Gw25eXlyWaLRHLNzc0KhUJdcmwW/7KeuBf/uu+++3TzzTfrqquu0gknnCDTNPXOO+9o+vTp2rVrl375y192R50AAAAAAACA5dntdqWlpUXHDvh8PuXk5CS4KiRC3MHsH/7wBz300EO69NJLo9vOPfdcHX744brtttsIZgEAAAAAAIC9SE1NVX19vUzTVFNTk1paWuR0OhNdFnpY3KMMduzYofHjx7fbPn78eO3YsaNLigIAAAAAAACSldPpVGZmZvR2U1PTAR+z7SgDWEPcweyIESP07LPPttu+YMECHXLIIV1SFAAAAAAAAJDM3G539HpLS0uXHpsZs9YQ9yiD22+/XVOmTNGyZct0wgknyDAMvf3223rjjTf2GNgCAAAAAAAAiOVwfBfLNTU1KRwORxcFQ98Q97v9ox/9SO+9957y8vK0aNEiLVy4UHl5eXr//fd13nnndUeNAAAAAAAAQFIxDCMmnA0EAgmsBokQd8esJBUXF+tvf/tbV9cCAAAAAAAA9Bnp6eny+XySpGAweEDHajtjllEG1kB/NAAAAAAAAJAALpcrer2+vl7hcDiB1aCndbpj1maz7XN1N8MwDjjdBwAAAAAAAPoCh8Mhl8ulQCCgUCik2tpaeb3eRJeFHtLpYPaFF17o8L4VK1boD3/4A23SAAAAAAAAQCcZhqGsrCxVVFTINE3V19crIyNDdrs90aWhB3Q6mD333HPbbdu4caNmz56tf/7zn7rkkkt05513dmlxAAAAAAAAQDJzOBxKT09XXV2dJKm5uVlpaWlxH4cZs9azXzNmt2/frp/97Gc68sgjFQwGtXbtWj3++OMaMmRIV9cHAAAAAAAAJLWUlJTo9aampgRWgp4UVzDr8/l04403asSIEfr000/1xhtv6J///KdGjx7dXfUlTGlpqYqKilRSUpLoUgAAAAAAAJDEnE6nbLZITNfc3MwiYH1Ep4PZe+65RwcffLD+9a9/6emnn9aKFSt04okndmdtCTVjxgytX79eq1atSnQpAAAAAAAASGKGYUTHF5imGR1rEO8xYC2G2cmhEzabTampqfrBD36w1wHECxcu7LLiegO/3y+v1yufzyePx5PocgAAAAAAAJCEgsGgysvLJUVyuMLCwrjC1sbGRlVXV0uSPB6PMjIyuqXO3iBZ8rpOL/516aWXkrwDAAAAAAAA3cDhcMjlcikQCCgcDss0TbK4JNfpYHb+/PndWAYAAAAAAADQtzkcDgUCAUmRDlqXy5XgitCd4lr8CwAAAAAAAED3cDi+66FsamqK67Ftu2s7ObkUCUYwCwAAAAAAAPQCKSkp0etNTU0ErEmOYBYAAAAAAADoBVrnzEqRUQahUCjBFaE7EcwCAAAAAAAAvUTbubL19fWdfhwLhVkPwSwAAAAAAADQS7jd7uj1xsbG/RpnwAgEayCYBQAAAAAAAHoJt9stp9MpSQqHwwoGgwmuCN2FYBYAAAAAAADoRdLS0qLXm5qaElgJuhPBLAAAAAAAANCLtB1n0Nzc3KnHMGPWeghmAQAAAAAAgF7E4XDI4XBIkgKBgMLhcFyPZ8asNRDMAgAAAAAAAL2My+WKXm9paUlgJeguBLMAAAAAAABAL9M2mA0EAgmsBN2FYBYAAAAAAADoZeINZtvOmGWUgTUQzAIAAAAAAAC9jN1ul91ul7R/c2bR+xHMAgAAAAAAAL2MYRhyu92SIh2wTU1NCa4IXY1gFgAAAAAAAOiFUlJSotdDoVACK0F3SPpg9rPPPtNRRx0VvaSmpmrRokWJLgsAAAAAAADYq9ZRBhLBbDJyJLqA7jZy5EitXbtWklRXV6eDDjpIp59+emKLAgAAAAAAAPbBZvuup3JfwSyLf1lP0nfMtvXiiy/qtNNOU3p6eqJLAQAAAAAAAPbKZrNFA1c6ZpNPwoPZZcuW6eyzz9aAAQNkGMYexwzMmzdPw4YNU0pKioqLi7V8+fL9eq5nn31WU6ZMOcCKAQAAAAAAgO5nGIacTqckKRgMEs4mmYQHs/X19RozZozmzp27x/sXLFigmTNn6qabbtKaNWt04oknauLEidqyZUt0n+LiYo0ePbrdZfv27dF9/H6/3nnnHZ155pnd/poAAAAAAACAruB2u6PXm5ubE1gJupph9qKhE4Zh6IUXXtDkyZOj24477jgdc8wxeuihh6LbRo0apcmTJ2vOnDmdPvYTTzyhV199VX/729/2ul9zc3PMSe73+zV48GD5fD55PJ7OvxgAAAAAAADgADU3N6uyslKS5HK5lJeXt8f9gsGgysvLJUmpqanKzs7usRp7mt/vl9frtXxel/CO2b0JBAJavXq1JkyYELN9woQJWrFiRVzH6uwYgzlz5sjr9UYvgwcPjut5AAAAAAAAgK7icrlkt9slRbKyXtRjiQPUq4PZXbt2KRQKqbCwMGZ7YWGhysrKOn0cn8+n999/Xz/84Q/3ue/s2bPl8/mil61bt8ZdNwAAAAAAANAVDMNgnEGSciS6gM5oXX2ulWma7bbtjdfr1c6dOzu1r9vtjjnZAQAAAAAAgERyuVxqaGiQJDU0NCglJSXBFaEr9OqO2by8PNnt9nbdseXl5e26aAEAAAAAAIBklJqaGr0eDAb3uE88TYzoHXp1MOtyuVRcXKzFixfHbF+8eLHGjx/frc9dWlqqoqIilZSUdOvzAAAAAAAAAHtjGIacTqekSDC7rzmzzKG1hoSPMqirq9OmTZuitzdv3qy1a9cqJydHQ4YM0axZszR16lSNHTtW48aN0yOPPKItW7Zo+vTp3VrXjBkzNGPGjOgqbwAAAAAAAECiOBwOtbS0SIqEs61BLawr4cHsBx98oFNOOSV6e9asWZKkadOmaf78+ZoyZYoqKyt1xx13aMeOHRo9erRefvllDR06NFElAwAAAAAAAD3K6XSqsbFRkhQIBAhmk4Bh0tu8V60dsz6fTx6PJ9HlAAAAAAAAoA8KBALatWuXpMj4z7y8vJj7Q6GQdu7cKUlKSUlRTk5Oj9fYU5Ilr+vVM2YBAAAAAAAARDpmHY7Il99bWloUDocTXBEOFMFsB1j8CwAAAAAAAL1F2wXATNNUKBRKcEU4UASzHZgxY4bWr1+vVatWJboUAAAAAAAAQDbbd1Ee00mtj2AWAAAAAAAAsADDMKLXdw9m294HayCYBQAAAAAAACygsx2zdNNaA8EsAAAAAAAAYAFtu2JZ/Mv6CGY7wOJfAAAAAAAA6E32NsoA1kMw2wEW/wIAAAAAAEBvQjCbXAhmAQAAAAAAAAtoO2OWUQbWRzALAAAAAAAAWADBbHIhmAUAAAAAAAAswG63R6+HQqEEVoKuQDALAAAAAAAAWIBhGNE5s83NzcyZtTiC2Q6UlpaqqKhIJSUliS4FAAAAAAAAkGEYcrlc0dtNTU0x97UisLUGgtkOzJgxQ+vXr9eqVasSXQoAAAAAAAAgSUpNTY1eDwaDCawEB4pgFgAAAAAAALCIth2zLS0tCawEB4pgFgAAAAAAALAIu90eHVtAx6y1EcwCAAAAAAAAFmEYhhwOh6RIMBsOhxNcEfYXwSwAAAAAAABgIU6nM3qdrlnrIpgFAAAAAAAALKS1Y1YimLUygtkOlJaWqqioSCUlJYkuBQAAAAAAAIhq2zHLAmDWRTDbgRkzZmj9+vVatWpVoksBAAAAAAAAovY0yqB1QTBYB8EsAAAAAAAAYCE2my0axIZCoXb3m6bZ0yVhPxDMAgAAAAAAABZjs0VivXA4nOBKsL8IZgEAAAAAAACLsdvtkiLBLB2y1kQwCwAAAAAAAFhMa8esRNesVRHMAgAAAAAAABazp2CWBcCshWAWAAAAAAAAsBg6Zq2PYLYDpaWlKioqUklJSaJLAQAAAAAAAGIQzFofwWwHZsyYofXr12vVqlWJLgUAAAAAAACI0TaYDYVCCawE+4tgFgAAAAAAALAYu90evb57MGuaZk+Xg/1AMAsAAAAAAABYjMPhiF5vaWlJYCXYXwSzAAAAAAAAgMXY7fZo12xLSwtdshZEMAsAAAAAAABYkNPplBQZXcCcWeshmAUAAAAAAAAsqO04g2AwKMMwElgN4kUwCwAAAAAAAFiQzfZdtMcoA+shmAUAAAAAAAAsqG0wGw6HE1gJ9gfBLAAAAAAAAGBBBLPWRjALAAAAAAAAWBDBrLURzHagtLRURUVFKikpSXQpAAAAAAAAQDsdBbPMm7UGw+Sd2iu/3y+v1yufzyePx5PocgAAAAAAAABJkTC2rKxMkpSSkqJAIKBwOCy73a7CwsIEV9d9kiWvo2MWAAAAAAAAsCDDMKLXGWVgPQSzAAAAAAAAgAUZhhEdZ0Awaz0EswAAAAAAAIBFtXbNhsPhmA5a9H4EswAAAAAAAIBFte2YZSkpayGYBQAAAAAAACyqNZiF9fDOAQAAAAAAABbVNpilY9ZaCGYBAAAAAAAAi9pTMEtAaw0EswAAAAAAAIBFtV3wi0DWWghmAQAAAAAAAItilIF1EcwCAAAAAAAAFkUwa10EswAAAAAAAIBFEcxaF8EsAAAAAAAAYFEEs9ZFMAsAAAAAAABYFMGsdRHMAgAAAAAAABZlGEaiS8B+IpjtQGlpqYqKilRSUpLoUgAAAAAAAIA9atsxGw6HJdE5axWGyTu1V36/X16vVz6fTx6PJ9HlAAAAAAAAADF27Ngh0zTV1NSklJQUGYah/v37J7qsbpMseR0dswAAAAAAAICFtY4zoP/SWghmAQAAAAAAAAtrHWfQVcHsJ0te15In/nxAx1jx9ye15pV/SpK2rl+n+df9Qk/eNCuuY2z99GPt2PRZ9PYXq95V1fZvD6iu7vTs7bPj2t/RTXUAAAAAAAAA6AFtFwAzTbPXLQi28Z2lOv78KTrshJPjetzW9euUmulR/xEjJUlfrlopm82u/EMO644yOyUcDslms+/xvgtvnRPXsQhmAQAAAAAAAAvb0yiDT95arI8Wv6xgS4tGlByvEy78iXzlO/WP/7lT+QcdrLJNnytv6DBNuvYGGYahTatWatmTf1F6Vo6y+g2QOz293fNsfGep3n3uadkdDnkKCjX5v25WoKlRbzw6T5XfbpNk6pRpP9fAw4qij/l06Rv6/N239c3Ha7R1/Tqd/rOr2h13++cbteSJRxUMBJSSnqEzr75e4VBIHy3+t2x2u9a9+ZpOuezn+vKD97Rtwyeyu1KU5nLqX/9zpwYdVqQtn3ysYKBZk2beqLzBQ2OO3eD36dWHHpB/V4VSMjJ0xn/+Ut6Cwj3+HBfc/ivlDx2msi8+VygU1FnX3KCcAQP1yrz7lZKRqZ2bN+mgI49R4cEjtOzJxxQOhXTQmKN18tQrZRiGSq+8WDMefarT7xvBLAAAAAAAAGBhraMMpEg46yvbrs1rPtBFv7lXhgwt+p87tf3zDUrPylHlt9t01swblTNgkJ69Y7a+3fipCocfojcf+6N+fMc9Ss/K1rN3/Lf6HzKy3fO898KzmnzDzcruP1DNDfWSpJULF2jEseM08djxqq3cpRfuvk2X/s/c6GMOP/k0bf30Yx1y3AkaXnzsHuvPHTREP779d7LZ7Nrw9hKtevF5nTLtZxpz+kSlZnp09BlnS5KGjz1Ohxx3gvIPOUxX3DdPkmR3OvWTOfdr3VuvafVLi/TD6dfGHPvd557SwMMO13nnXqCNK5bpzfl/1Hk33NLxD9OULv7t/+qL91ZoyeOP6PzZt0uSait36cJb5ijYEtBjv5yuKbfeLU9evhb9z53a9P67OuS48Z14p2IRzAIAAAAAAAAWtvvogrLPN2j7Fxv1t1/NlCS1NDWpZmeZ0rNylDNgoHIHDpYkFRw0XP6KcrlS05QzcJA8efmSpJHjvif/rop2zzNg5Cgt/lOpDjvhZB16/AmSpG8+XqPNaz7Qu889LUlqrKtVKNgSV/3N9XV6ee698u0sUzgcjtbRGSPGHi9JKhw2QhvfXtLu/m83rtf5v7rt/17XiXpr/iN7Pd7I8SdFjnvsOL3+53nR7Ycef4IMw1D19m+VM2BQtOv2sBNO1raNnxLMAgAAAAAAAH3N7jNmZUpHnnaGxl1wUcx+vvKdsjud3z3OZlM4HO70omE/uHKGdnyxUV+ufl9P3HitLrtvnmRK5914izx5Bftd/zvP/k0HH12iMadP1PbPN2r50/M7/Vi7IxJv2v7vtezLvubvdnS3w+Xe8x2m2eFj9sW2710AAAAAAAAA9FatYWPrPwsOGanP3l2upro6SZGv4TfW+jt8fM7AQaravk21lbsUDoX0+cq397ifb2eZBhw6St/78aWyOxxqqq3VkCPGaO2rL0X3Kf/6q7jrDzQ2KDM3T5K04e23ottdKakKNDZGbztTUhVoamz3+L0ZeFiRNryzVJL0+cp31G/EoXvd/7MVyyVJmz5YqcKDR7S7P3vAQFXv+Fb+inKZ4bA2rlimgYcdHldNreiYBQAAAAAAAJJIVv+BKjnnR3r29l/JNE05U1M16dobOtzf6XLr1Mv+n/5+501K82ap4KDhe9xv6d/+rJqyHTJNU4cef4Iyc/M07oKL9OZf/qjHr5+hcDisIaPH6LQrpsdV79hJ5+uVh+7Xe4v+rgGHHhbdfnDxcfrn/XP02bvLNfGq63TY+JP02iN/kBY+qzSXs8PjbfrgPe386gudcOFPNO6Ci/XqQw9o/bI3o4t/7Y3D5dJTN10XXfxrd06XW6f/7Cot+p87o4t/jSgZJ0mKt3HWMDvbq9xH+f1+eb1e+Xw+eTyeRJcDAAAAAAAAxPD7/aqrq1MwGJTH45HL5VJaWlqiy+o23ZXXLbj9Vzrt8unKG3JQ3I9t8Pv0zC3/pSse2PsM27YYZQAAAAAAAABYWOsIA4fDoZSUlKQOZXujxlq/nvr1dTrmzMlxPY6O2X2gYxYAAAAAAAC9WV1dnfz+yAzZ7OxspaamJriiPft67Wote2p+zLZBo0br1Mv/X1zHOdC8buXCBe3m6BafNVmHn3xa3Mc6EASz+0AwCwAAAAAAgN6svr5ePp9PUu8OZrtKsuR1jDIAAAAAAAAAkgQ9mNZBMAsAAAAAAABYWOuMWYlg1koIZgEAAAAAAAALI5i1JoJZAAAAAAAAwMIIZq2JYBYAAAAAAACwsLbBLKyjTwSz999/vw4//HAVFRXpmmuu4S8HAAAAAAAASErkXtaR9MFsRUWF5s6dq9WrV2vdunVavXq1Vq5cmeiyAAAAAAAAgC7BKANrciS6gJ4QDAbV1NQkSWppaVFBQUGCKwIAAAAAAAC6BsGsNSW8Y3bZsmU6++yzNWDAABmGoUWLFrXbZ968eRo2bJhSUlJUXFys5cuXd/r4+fn5uv766zVkyBANGDBAP/jBDzR8+PAufAUAAAAAAABA4hDMWlPCg9n6+nqNGTNGc+fO3eP9CxYs0MyZM3XTTTdpzZo1OvHEEzVx4kRt2bIluk9xcbFGjx7d7rJ9+3ZVV1frX//6l77++mt9++23WrFihZYtW9ZTLw8AAAAAAADoVgSz1pTwUQYTJ07UxIkTO7z/vvvu009/+lNdeeWVkqQHHnhAr776qh566CHNmTNHkrR69eoOH//3v/9dI0aMUE5OjiTprLPO0sqVK3XSSSd14asAAAAAAAAAEqNtMAvrSHjH7N4EAgGtXr1aEyZMiNk+YcIErVixolPHGDx4sFasWKGmpiaFQiEtWbJEI0eO7HD/5uZm+f3+mAsAAAAAAABgBXTMWkevDmZ37dqlUCikwsLCmO2FhYUqKyvr1DGOP/54nXnmmTr66KN15JFHavjw4TrnnHM63H/OnDnyer3Ry+DBgw/oNQAAAAAAAADdiVEG1tSrg9lWu7djm6YZV4v2b3/7W23YsEGffvqpfv/73+/1sbNnz5bP54tetm7dut91AwAAAAAAAN2NYNaaEj5jdm/y8vJkt9vbdceWl5e366LtKm63W263u1uODQAAAAAAAHQHwzBkmibBrIX06o5Zl8ul4uJiLV68OGb74sWLNX78+ARVBQAAAAAAAPQuLABmPQnvmK2rq9OmTZuitzdv3qy1a9cqJydHQ4YM0axZszR16lSNHTtW48aN0yOPPKItW7Zo+vTp3VpXaWmpSktLFQqFuvV5AAAAAAAAgAPVGszSMWsdhpngd2vJkiU65ZRT2m2fNm2a5s+fL0maN2+e7rnnHu3YsUOjR4/W/fffr5NOOqlH6vP7/fJ6vfL5fPJ4PD3ynAAAAAAAAEA8ysvLFQwGZbPZ1K9fv0SX062SJa9LeDDb2yXLGw0AAAAAAIDk1RrMGoah/v37J7qcbpUseV2vnjELAAAAAAAAYN8YZWA9BLMdKC0tVVFRkUpKShJdCgAAAAAAALBXbRf/Ipy1BkYZ7EOytEYDAAAAAAAgeVVWVqq5uVmS1K9fP9lsyduPmSx5XfK+QwAAAAAAAEAf0bZjFtZAMAsAAAAAAAAkEb4gbw0EswAAAAAAAIDFMWPWeghmO8DiXwAAAAAAALAKglnrYfGvfUiWYcIAAAAAAABIXj6fT/X19ZKkvLw8uVyuBFfUfZIlr6NjFgAAAAAAALA4Omath2AWAAAAAAAAAHoYwSwAAAAAAABgcXTMWg/BLAAAAAAAAGBxBLPWQzDbgdLSUhUVFamkpCTRpQAAAAAAAAB7RTBrPYbJO7VXybLKGwAAAAAAAJJXQ0ODampqJEler1fp6emJLagbJUteR8csAAAAAAAAYHFtO2ZhDQSzAAAAAAAAQBLhC/LWQDALAAAAAAAAWBwzZq2HYBYAAAAAAACwOIJZ6yGYBQAAAAAAACyOYNZ6CGY7UFpaqqKiIpWUlCS6FAAAAAAAAGCvWPzLegyTCH2v/H6/vF6vfD6fPB5PossBAAAAAAAA2mlpaVFFRYUkKS0tTVlZWYktqBslS15HxywAAAAAAABgcXTMWg/BLAAAAAAAAJBE+IK8NRDMAgAAAAAAAEAPI5gFAAAAAAAAgB5GMAsAAAAAAAAAPYxgFgAAAAAAALA4Fv+yHoJZAAAAAAAAIImw+Jc1EMx2oLS0VEVFRSopKUl0KQAAAAAAAACSjGESoe+V3++X1+uVz+eTx+NJdDkAAAAAAABAO6FQSDt37pQkpaSkKCcnJ8EVdZ9kyevomAUAAAAAAACAHkYwCwAAAAAAAFhc28W/+IK8NRDMAgAAAAAAAEAPI5gFAAAAAAAAgB5GMAsAAAAAAAAAPYxgFgAAAAAAALC4tjNmYQ0EswAAAAAAAEASYfEvayCYBQAAAAAAAIAeRjALAAAAAAAAAD2MYLYDpaWlKioqUklJSaJLAQAAAAAAAJBkDJOhE3vl9/vl9Xrl8/nk8XgSXQ4AAAAAAACwR9u3b5ckuVwu5eXlJbia7pMseR0dswAAAAAAAEASoQ/TGghmAQAAAAAAAKCHEcwCAAAAAAAAScAwjESXgDgQzAIAAAAAAABADyOYBQAAAAAAAJIIM2atgWAWAAAAAAAAAHoYwSwAAAAAAACQBJgxay0EswAAAAAAAADQwwhmAQAAAAAAAKCHEcwCAAAAAAAASYTFv6yBYBYAAAAAAAAAehjBLAAAAAAAAJAEWPzLWghmAQAAAAAAAKCHEcx2oLS0VEVFRSopKUl0KQAAAAAAAACSjGEyDXiv/H6/vF6vfD6fPB5PossBAAAAAAAA9mjnzp0KhUKy2Wzq169fosvpNsmS19ExCwAAAAAAAAA9jGAWAAAAAAAASAJdtfjX3z/Yqt++tP6AjnH/4s/1+IqvJUkrv6rU6fct1bml78R1jHe/rNTarTXR269+WqavKuoOqK7udONzH+ubyvpO7+/oxloAAAAAAAAA9HEvfrRdV592iM4ZMyCux638qlI56S4dNThLkvTapzvlsBnKG5jaDVV2Tihsym7bcwD+uwuOjOtYBLMAAAAAAABAEjFNU89+sFVPrvxGzcGwJhQVataEkdpa1aCf/fUDFQ3w6KOtNTqsv0dzLzpahmHotU/LNOffG5Wf6daw3HR5UtvHhi9+tF0Pvv65nHabBmWn6dFpY1XfHNTNiz7Rpoo6maZ069lFGntQTvQxz6/eppc+3qHlX1Ro5VeVuuu8I9od98Mt1frNv9arqSUsb6pTD/74KAXDpp58b4scNkPPrNqqW88u0usbdur9ryuV5rDJlpKpK/72sYqH5erdLyvV1BLS3IuP0aGFmTHHrqht1tVPf6j65pBOPjRfT773jdbcMmGPP7f7F3+urdUN+ra6UeW1zfrVxMP0w8P76e8fbNWyL3aprqlFaS6HbjzjMP3Xcx/J19iiQdmpuvc/xigrzaUpf3xXd5w7WiP7Ze7x+LsjmAUAAAAAAACSyObKRi35rFoLf3GCDEk/++sHWv1NtQoy3fqyok5zLz5Gw/PT9eNHVmrV19U6cpBXt734qZ77z/HKz3TrokdW6ughWe2OW/rmJj06rUTD8tLlb2qRJP3hzU2acHg/3Te6n3b4GnX5Y6v0ysyToo/5UfEgrfiyUmce0U+njSrcY72HFGTo79PHy24z9I+13+rhpV/plrOLdMlxQ5ST7tK08QdJkn4wqlBnHtFPJQNT5b2uVpLkttv04lXf07OrturR5V/pngvGxBz7wTc+1w9GFerKEw/WE+9+vc+f3abyOj03fbx21TXrgodW6ORD8yVJH22t0UvXfE+ZKU5dMX+Vpo4bqklHDtBDS77UA69/odvOOXyfx94dwSwAAAAAAACQBFpnzH6wtVYfflOjSX94W5LUEAhqS1W9CjLdOjgvQyMKMiRJhw/walt1g9Lddg0vyNCArMiIgLOO7K/tNY3tjl98ULb+e+E6nXvUAE08or8k6e1NFVryWbkefOMLSVJNQ4sCwXBcdfsaW/TLBWu1papBwbCpgVmdH1VwelE/SdLogV4tWvttu/tXf1Ojq089RJI0+eiBum/x53s93g8P7yeXw6YBWakaXpChL/9vpu1Jh+YpM8UpSfp4W43+PG2sJOn8Ywbq8sdWdbretghmAQAAAAAAgCRiSrro2CG69geHxGzfWtUgl8MWvW23RWammmbnjvvbyaP14ZYavbFhp876/XK9Putkmab058tK4gpTd3ff4s91ymEFuuS4ofpwS7V+9++NnX5s6+ux2wyFwu1fiGmaOpAl0Yz/e3Sq0x6z9bvjS/u75ppt37sAAAAAAAAAsIriQZn618fb5WuIjBvY4WtUdX2gw/1HFGToq4p67fA1KhgK6+V1O/a435aqBhUPzdZ//XCkXHabqhsC+t6IPP21zYiA9dv9cddb1xRUf2+KJGnRmu+6XjPcDtU1B9vctsfc7oyxB2Xrnx9HXs8/1m7f5/6vrd+pQDCsHb5GfVVRr4Pz09vtc+Qgr/79Sdn/HfNblbSZqRsPOmYBAAAAAACAJHJwboqmnzxcUx55V6Yppbvt+sPFx3S4f4rTrlvPLtIlf3pPeRluHT7Au8f9fvvSBn1T2SBTpiYe0U/9vam65rRDdOuLn+qH9y9TyDR1wvBc3X7u6Ljq/flJB+v6v3+keW99qeKh2dHtp40q0C+e/FD/+niH7p8yRmePGaBfLVyneW+asqV0vMDW4vU7tW5bjWZNGKlrTztUVz/9oV5Ys00TR/ffZy2HD/Bo6p/fU3lts245u0gpMZ2yEbedfbiuf+4j/f6NLzQwK1X/e+F3c23j6Z41TLOzzcp9k9/vl9frlc/nk8fjSXQ5AAAAAAAAwB5VV1crGAzKMAzl5eUlupxucyB53dF3vKY1t0zY4333L/48ZrGxeE24f6me+OlxKvSkdGp/OmYBAAAAAACAJJCdnb3vndAtpv3lfR0+wNvpUFaiY3af6JgFAAAAAAAADtzSzyt0924Lex03LEe3nXN4XMc50Lyuq+o4UASz+0AwCwAAAAAAAPQeyZLX2RJdAAAAAAAAAAD0NQSzAAAAAAAAANDDCGYBAAAAAAAAoIcRzAIAAAAAAABADyOYBQAAAAAAAIAeRjALAAAAAAAAAD2sTwSz9957rw4//HCNHj1af/vb3xJdDgAAAAAAAIA+zpHoArrbunXr9NRTT2n16tWSpNNOO02TJk1SVlZWYgsDAAAAAAAA0Gclfcfshg0bNH78eKWkpCglJUVHHXWUXnnllUSXBQAAAAAAAKAPS3gwu2zZMp199tkaMGCADMPQokWL2u0zb948DRs2TCkpKSouLtby5cs7ffzRo0frrbfeUk1NjWpqavTmm2/q22+/7cJXAAAAAAAAAADxSfgog/r6eo0ZM0aXX365fvSjH7W7f8GCBZo5c6bmzZunE044QX/84x81ceJErV+/XkOGDJEkFRcXq7m5ud1jX3vtNRUVFemaa67RqaeeKq/Xq5KSEjkcCX/ZAAAAAAAAAPowwzRNM9FFtDIMQy+88IImT54c3XbcccfpmGOO0UMPPRTdNmrUKE2ePFlz5syJ+zmuvPJKnXfeeTrrrLP2eH9zc3NMyOv3+zV48GD5fD55PJ64nw8AAAAAAABA1/H7/fJ6vZbP6xI+ymBvAoGAVq9erQkTJsRsnzBhglasWNHp45SXl0uSPvvsM73//vv64Q9/2OG+c+bMkdfrjV4GDx68f8UDAAAAAAAAQAd69Xf6d+3apVAopMLCwpjthYWFKisr6/RxJk+erJqaGqWnp+uxxx7b6yiD2bNna9asWdHbrR2zAAAAAAAAANBVenUw28owjJjbpmm227Y38XTXut1uud3uTu8PAAAAAAAAAPHq1aMM8vLyZLfb23XHlpeXt+uiBQAAAAAAAACr6NXBrMvlUnFxsRYvXhyzffHixRo/fny3PndpaamKiopUUlLSrc8DAAAAAAAAoO9J+CiDuro6bdq0KXp78+bNWrt2rXJycjRkyBDNmjVLU6dO1dixYzVu3Dg98sgj2rJli6ZPn96tdc2YMUMzZsyIrvIGAAAAAAAAAF0l4cHsBx98oFNOOSV6u3XhrWnTpmn+/PmaMmWKKisrdccdd2jHjh0aPXq0Xn75ZQ0dOjRRJQMAAAAAAADAATFM0zQTXURv1tox6/P55PF4El0OAAAAAAAA0KclS17Xq2fMAgAAAAAAAEAyIpjtAIt/AQAAAAAAAOgujDLYh2RpjQYAAAAAAACSQbLkdXTMAgAAAAAAAEAPI5gFAAAAAAAAgB5GMAsAAAAAAAAAPcyR6AJ6q9LSUpWWlioYDEqKzK4AAAAAAAAAkFitOZ3Vl85i8a992LZtmwYPHpzoMgAAAAAAAAC0sXXrVg0aNCjRZew3gtl9CIfD2r59uzIzM2UYRqLL6RZ+v1+DBw/W1q1bLb2SHbA3nOfoKzjX0RdwnqOv4FxHX8B5jr6Cc71rmaap2tpaDRgwQDabdSe1MspgH2w2m6WT93h4PB4+HJD0OM/RV3Cuoy/gPEdfwbmOvoDzHH0F53rX8Xq9iS7hgFk3UgYAAAAAAAAAiyKYBQAAAAAAAIAeRjALud1u3XrrrXK73YkuBeg2nOfoKzjX0RdwnqOv4FxHX8B5jr6Ccx17wuJfAAAAAAAAANDD6JgFAAAAAAAAgB5GMAsAAAAAAAAAPYxgFgAAAAAAAAB6GMFsHzdv3jwNGzZMKSkpKi4u1vLlyxNdEtBpt912mwzDiLn069cver9pmrrttts0YMAApaam6vvf/74+/fTTmGM0Nzfr6quvVl5entLT03XOOedo27ZtPf1SgBjLli3T2WefrQEDBsgwDC1atCjm/q46t6urqzV16lR5vV55vV5NnTpVNTU13fzqgIh9neeXXXZZu8/4448/PmYfznP0dnPmzFFJSYkyMzNVUFCgyZMn67PPPovZh890WF1nznM+05EMHnroIR155JHyeDzyeDwaN26c/v3vf0fv5/Mc+4Ngtg9bsGCBZs6cqZtuuklr1qzRiSeeqIkTJ2rLli2JLg3otMMPP1w7duyIXtatWxe975577tF9992nuXPnatWqVerXr59OP/101dbWRveZOXOmXnjhBT3zzDN6++23VVdXp0mTJikUCiXi5QCSpPr6eo0ZM0Zz587d4/1ddW5ffPHFWrt2rV555RW98sorWrt2raZOndrtrw+Q9n2eS9IZZ5wR8xn/8ssvx9zPeY7ebunSpZoxY4ZWrlypxYsXKxgMasKECaqvr4/uw2c6rK4z57nEZzqsb9CgQbr77rv1wQcf6IMPPtCpp56qc889Nxq+8nmO/WKizzr22GPN6dOnx2w77LDDzF/96lcJqgiIz6233mqOGTNmj/eFw2GzX79+5t133x3d1tTUZHq9XvPhhx82TdM0a2pqTKfTaT7zzDPRfb799lvTZrOZr7zySrfWDnSWJPOFF16I3u6qc3v9+vWmJHPlypXRfd59911Tkrlx48ZuflVArN3Pc9M0zWnTppnnnntuh4/hPIcVlZeXm5LMpUuXmqbJZzqS0+7nuWnymY7klZ2dbT766KN8nmO/0THbRwUCAa1evVoTJkyI2T5hwgStWLEiQVUB8fviiy80YMAADRs2TD/+8Y/11VdfSZI2b96ssrKymHPc7Xbr5JNPjp7jq1evVktLS8w+AwYM0OjRo/n3AL1WV53b7777rrxer4477rjoPscff7y8Xi/nP3qNJUuWqKCgQIceeqh+9rOfqby8PHof5zmsyOfzSZJycnIk8ZmO5LT7ed6Kz3Qkk1AopGeeeUb19fUaN24cn+fYbwSzfdSuXbsUCoVUWFgYs72wsFBlZWUJqgqIz3HHHae//vWvevXVV/WnP/1JZWVlGj9+vCorK6Pn8d7O8bKyMrlcLmVnZ3e4D9DbdNW5XVZWpoKCgnbHLygo4PxHrzBx4kQ9+eSTevPNN/W///u/WrVqlU499VQ1NzdL4jyH9ZimqVmzZul73/ueRo8eLYnPdCSfPZ3nEp/pSB7r1q1TRkaG3G63pk+frhdeeEFFRUV8nmO/ORJdABLLMIyY26ZpttsG9FYTJ06MXj/iiCM0btw4DR8+XI8//nh0MYH9Ocf59wBW0BXn9p725/xHbzFlypTo9dGjR2vs2LEaOnSoXnrpJZ1//vkdPo7zHL3VVVddpY8//lhvv/12u/v4TEey6Og85zMdyWLkyJFau3atampq9Pzzz2vatGlaunRp9H4+zxEvOmb7qLy8PNnt9nZ/cSkvL2/3Fx7AKtLT03XEEUfoiy++UL9+/SRpr+d4v379FAgEVF1d3eE+QG/TVed2v379tHPnznbHr6io4PxHr9S/f38NHTpUX3zxhSTOc1jL1VdfrRdffFFvvfWWBg0aFN3OZzqSSUfn+Z7wmQ6rcrlcGjFihMaOHas5c+ZozJgxevDBB/k8x34jmO2jXC6XiouLtXjx4pjtixcv1vjx4xNUFXBgmpubtWHDBvXv31/Dhg1Tv379Ys7xQCCgpUuXRs/x4uJiOZ3OmH127NihTz75hH8P0Gt11bk9btw4+Xw+vf/++9F93nvvPfl8Ps5/9EqVlZXaunWr+vfvL4nzHNZgmqauuuoqLVy4UG+++aaGDRsWcz+f6UgG+zrP94TPdCQL0zTV3NzM5zn2X48uNYZe5ZlnnjGdTqf55z//2Vy/fr05c+ZMMz093fz6668TXRrQKdddd525ZMkS86uvvjJXrlxpTpo0yczMzIyew3fffbfp9XrNhQsXmuvWrTMvuugis3///qbf748eY/r06eagQYPM119/3fzwww/NU0891RwzZowZDAYT9bIAs7a21lyzZo25Zs0aU5J53333mWvWrDG/+eYb0zS77tw+44wzzCOPPNJ89913zXfffdc84ogjzEmTJvX460XftLfzvLa21rzuuuvMFStWmJs3bzbfeustc9y4cebAgQM5z2Ep//mf/2l6vV5zyZIl5o4dO6KXhoaG6D58psPq9nWe85mOZDF79mxz2bJl5ubNm82PP/7Y/O///m/TZrOZr732mmmafJ5j/xDM9nGlpaXm0KFDTZfLZR5zzDHm0qVLE10S0GlTpkwx+/fvbzqdTnPAgAHm+eefb3766afR+8PhsHnrrbea/fr1M91ut3nSSSeZ69atizlGY2OjedVVV5k5OTlmamqqOWnSJHPLli09/VKAGG+99ZYpqd1l2rRppml23bldWVlpXnLJJWZmZqaZmZlpXnLJJWZ1dXUPvUr0dXs7zxsaGswJEyaY+fn5ptPpNIcMGWJOmzat3TnMeY7ebk/nuCTzsccei+7DZzqsbl/nOZ/pSBZXXHFFND/Jz883TzvttGgoa5p8nmP/GKZpmj3XnwsAAAAAAAAAYMYsAAAAAAAAAPQwglkAAAAAAAAA6GEEswAAAAAAAADQwwhmAQAAAAAAAKCHEcwCAAAAAAAAQA8jmAUAAAAAAACAHkYwCwAAAAAAAAA9jGAWAAAAAAAAAHoYwSwAAEAfZhiGFi1alNAaTNPUz3/+c+Xk5MgwDK1duzauxy9ZskSGYaimpqZb6tub3vDz68htt92mo446KtFlAAAAoAOORBcAAACA+F122WWqqanptaFgPF555RXNnz9fS5Ys0cEHH6y8vLy4Hj9+/Hjt2LFDXq+3myoEAAAAuh7BLAAAABLqyy+/VP/+/TV+/Pj9erzL5VK/fv26uKqe0dLSIqfTmegyAAAAkACMMgAAAEgC3//+93XNNdfohhtuUE5Ojvr166fbbrstZp8vvvhCJ510klJSUlRUVKTFixe3O863336rKVOmKDs7W7m5uTr33HP19ddfS5I2btyotLQ0PfXUU9H9Fy5cqJSUFK1bt67D2pYuXapjjz1Wbrdb/fv3169+9SsFg0FJkc7fq6++Wlu2bJFhGDrooIP2eIxvvvlGZ599trKzs5Wenq7DDz9cL7/8sqT2owzmz5+vrKwsvfrqqxo1apQyMjJ0xhlnaMeOHdHjBYNBXXPNNcrKylJubq5uvPFGTZs2TZMnT47uc9BBB+mBBx6IqeOoo45q93Nt68Ybb9Shhx6qtLQ0HXzwwbr55pvV0tISvb91vMBf/vIXHXzwwXK73TJNM+YYPp9PqampeuWVV2K2L1y4UOnp6aqrq+vUc+3u+9//vmbOnBmzbfLkybrsssuitwOBgG644QYNHDhQ6enpOu6447RkyZIOjwkAAID9RzALAACQJB5//HGlp6frvffe0z333KM77rgjGr6Gw2Gdf/75stvtWrlypR5++GHdeOONMY9vaGjQKaecooyMDC1btkxvv/12NNQMBAI67LDDdO+99+oXv/iFvvnmG23fvl0/+9nPdPfdd+uII47YY03ffvutzjzzTJWUlOijjz7SQw89pD//+c/6zW9+I0l68MEHdccdd2jQoEHasWOHVq1atcfjzJgxQ83NzVq2bJnWrVun3/3ud8rIyOjwZ9HQ0KB7771XTzzxhJYtW6YtW7bo+uuvj97/u9/9Tk8++aQee+wxvfPOO/L7/V0yFiIzM1Pz58/X+vXr9eCDD+pPf/qT7r///ph9Nm3apGeffVbPP//8Hufper1enXXWWXryySdjtj/11FM699xzo6+7M88Vr8svv1zvvPOOnnnmGX388cf6j//4D51xxhn64osvDui4AAAAaI9RBgAAAEniyCOP1K233ipJOuSQQzR37ly98cYbOv300/X6669rw4YN+vrrrzVo0CBJ0l133aWJEydGH//MM8/IZrPp0UcflWEYkqTHHntMWVlZWrJkiSZMmKBf/OIXevnllzV16lS5XC4VFxfr2muv7bCmefPmafDgwZo7d64Mw9Bhhx2m7du368Ybb9Qtt9wir9erzMxM2e32vY4j2LJli370ox9FA+CDDz54rz+LlpYWPfzwwxo+fLgk6aqrrtIdd9wRvf8Pf/iDZs+erfPOO0+SNHfu3GgH7oH49a9/Hb1+0EEH6brrrtOCBQt0ww03RLcHAgE98cQTys/P7/A4l1xyiS699FI1NDQoLS1Nfr9fL730kp5//vm4niseX375pZ5++mlt27ZNAwYMkCRdf/31euWVV/TYY4/prrvu2q/jAgAAYM8IZgEAAJLEkUceGXO7f//+Ki8vlyRt2LBBQ4YMiYaykjRu3LiY/VevXq1NmzYpMzMzZntTU5O+/PLL6O2//OUvOvTQQ2Wz2fTJJ59EQ9w92bBhg8aNGxezzwknnKC6ujpt27ZNQ4YM6dRru+aaa/Sf//mfeu211/SDH/xAP/rRj9q93rbS0tKioawU+7Pw+XzauXOnjj322Oj9drtdxcXFCofDnaqnI88995weeOABbdq0SXV1dQoGg/J4PDH7DB06dK+hrCSdddZZcjgcevHFF/XjH/9Yzz//vDIzMzVhwoS4niseH374oUzT1KGHHhqzvbm5Wbm5uft9XAAAAOwZowwAAACSxO6LSBmGEQ0ad59j2np/W+FwWMXFxVq7dm3M5fPPP9fFF18c3e+jjz5SfX296uvrVVZWtteaTNNs9zyttewt0N3dlVdeqa+++kpTp07VunXrNHbsWP3hD3/ocP89/Sx2/xl0VFcrm83WbtveZriuXLlSP/7xjzVx4kT961//0po1a3TTTTcpEAjE7Jeent7hMVq5XC5dcMEF0Xm+Tz31lKZMmSKHwxHXc8XzesLhsOx2u1avXh3z/m/YsEEPPvjgPmsGAABAfOiYBQAA6AOKioq0ZcsWbd++Pfo19XfffTdmn2OOOUYLFixQQUFBh52XVVVVuuyyy3TTTTeprKxMl1xyiT788EOlpqZ2+LzPP/98TEC7YsUKZWZmauDAgXG9hsGDB2v69OmaPn26Zs+erT/96U+6+uqr4zqGFJnhWlhYqPfff18nnniiJCkUCmnNmjU66qijovvl5+fHLBjm9/u1efPmDo/7zjvvaOjQobrpppui27755pu462t1ySWXaMKECfr000/11ltv6c477zyg59r99YRCIX3yySc65ZRTJElHH320QqGQysvLoz8XAAAAdB86ZgEAAPqAH/zgBxo5cqQuvfRSffTRR1q+fHlMqCdFgsC8vDyde+65Wr58uTZv3qylS5fq2muv1bZt2yRJ06dP1+DBg/XrX/9a9913n0zTjFlUa3e/+MUvtHXrVl199dXauHGj/vGPf+jWW2/VrFmzZLN1/lfRmTNn6tVXX9XmzZv14Ycf6s0339SoUaP274ch6eqrr9acOXP0j3/8Q5999pmuvfZaVVdXx3TRnnrqqXriiSe0fPlyffLJJ5o2bZrsdnuHxxwxYoS2bNmiZ555Rl9++aV+//vf64UXXtjvGk8++WQVFhbqkksu0UEHHaTjjz/+gJ7r1FNP1UsvvaSXXnpJGzdu1C9+8QvV1NRE7z/00EOjs20XLlyozZs3a9WqVfrd737XJfN3AQAAEItgFgAAoA+w2Wx64YUX1NzcrGOPPVZXXnmlfvvb38bsk5aWpmXLlmnIkCE6//zzNWrUKF1xxRVqbGyUx+PRX//6V7388st64okn5HA4lJaWpieffFKPPvpoh8HdwIED9fLLL+v999/XmDFjNH36dP30pz+NWbiqM0KhkGbMmKFRo0bpjDPO0MiRIzVv3rz9/nnceOONuuiii3TppZdq3LhxysjI0A9/+EOlpKRE95k9e7ZOOukkTZo0SWeeeaYmT54cM7d2d+eee65++ctf6qqrrtJRRx2lFStW6Oabb97vGg3D0EUXXaSPPvpIl1xyyQE/1xVXXKFp06bp0ksv1cknn6xhw4ZFu2VbPfbYY7r00kt13XXXaeTIkTrnnHP03nvvafDgwfv9OgAAALBnhrmngWMAAABAHxIOhzVq1ChdeOGFMSMDAAAAgO7CjFkAAAD0Od98841ee+01nXzyyWpubtbcuXO1efPmmEXOAAAAgO7EKAMAAAD0OTabTfPnz1dJSYlOOOEErVu3Tq+//voBza0FAAAA4sEoAwAAAAAAAADoYXTMAgAAAAAAAEAPI5gFAAAAAAAAgB5GMAsAAAAAAAAAPYxgFgAAAAAAAAB6GMEsAAAAAAAAAPQwglkAAAAAAAAA6GEEswAAAAAAAADQwwhmAQAAAAAAAKCHEcwCAAAAAAAAQA/7/06wLthZLKEyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_singular_values_energy(model.model.layers[0], top_k=4000, energy_threshold=0.80) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668f9ec9",
   "metadata": {},
   "source": [
    "## Low rank prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9991ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "class LowRankLinear(nn.Module):\n",
    "    def __init__(self, original_linear, energy_threshold=0.99):\n",
    "        super().__init__()\n",
    "        print(f\"Computing SVD for module: {original_linear}\")\n",
    "        W = original_linear.weight.data  # (out_features, in_features)\n",
    "        U, S, Vh = torch.linalg.svd(W, full_matrices=False)\n",
    "\n",
    "        S_norm = S / S.sum()\n",
    "        cumulative = torch.cumsum(S_norm, dim=0)\n",
    "        rank = int((cumulative >= energy_threshold).nonzero(as_tuple=False)[0].item()) + 1\n",
    "        \n",
    "        U_r = U[:, :rank]             # (out_features, rank)\n",
    "        S_r = torch.diag(S[:rank])    # (rank, rank)\n",
    "        Vh_r = Vh[:rank, :]           # (rank, in_features)\n",
    "        \n",
    "        self.first_linear = nn.Linear(original_linear.in_features, rank, bias=False)\n",
    "        self.first_linear.weight.data = (S_r @ Vh_r).to(W.dtype)\n",
    "        \n",
    "        self.second_linear = nn.Linear(rank, original_linear.out_features, bias=original_linear.bias is not None)\n",
    "        self.second_linear.weight.data = U_r.to(W.dtype)\n",
    "        \n",
    "        if original_linear.bias is not None:\n",
    "            self.second_linear.bias.data = original_linear.bias.data.clone()\n",
    "        \n",
    "        self.rank = rank\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.second_linear(self.first_linear(x))\n",
    "\n",
    "\n",
    "def replace_linear_with_lowrank(module, energy_threshold=0.99):\n",
    "    for name, child in list(module.named_children()):\n",
    "        if isinstance(child, nn.Linear):\n",
    "            setattr(module, name, LowRankLinear(child, energy_threshold))\n",
    "        else:\n",
    "            replace_linear_with_lowrank(child, energy_threshold)\n",
    "\n",
    "\n",
    "def make_lowrank_model(original_model, energy_threshold=0.99):\n",
    "    new_model = LlamaForCausalLM(original_model.config)\n",
    "    new_model.load_state_dict(original_model.state_dict())\n",
    "    replace_linear_with_lowrank(new_model, energy_threshold)\n",
    "    return new_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fabd7cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2571428571428571"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fraction_count_params(p, M,N):\n",
    "    K = min(p*M,p*N)\n",
    "    return (K*M + K*N) / (M*N)\n",
    "\n",
    "fraction_count_params(0.8 , 4000, 7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f4f7a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": 128009,\n",
       "  \"head_dim\": 128,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8192,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 24,\n",
       "  \"num_hidden_layers\": 28,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pad_token_id\": 128004,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 32.0,\n",
       "    \"high_freq_factor\": 4.0,\n",
       "    \"low_freq_factor\": 1.0,\n",
       "    \"original_max_position_embeddings\": 8192,\n",
       "    \"rope_type\": \"llama3\"\n",
       "  },\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.53.0\",\n",
       "  \"unsloth_fixed\": true,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2cba92ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=1024, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=8192, bias=False)\n",
      "Computing SVD for module: Linear(in_features=8192, out_features=3072, bias=False)\n",
      "Computing SVD for module: Linear(in_features=3072, out_features=128256, bias=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=820, bias=False)\n",
       "            (second_linear): Linear(in_features=820, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=357, bias=False)\n",
       "            (second_linear): Linear(in_features=357, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=613, bias=False)\n",
       "            (second_linear): Linear(in_features=613, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1374, bias=False)\n",
       "            (second_linear): Linear(in_features=1374, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2039, bias=False)\n",
       "            (second_linear): Linear(in_features=2039, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2061, bias=False)\n",
       "            (second_linear): Linear(in_features=2061, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2079, bias=False)\n",
       "            (second_linear): Linear(in_features=2079, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1171, bias=False)\n",
       "            (second_linear): Linear(in_features=1171, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=478, bias=False)\n",
       "            (second_linear): Linear(in_features=478, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=665, bias=False)\n",
       "            (second_linear): Linear(in_features=665, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1463, bias=False)\n",
       "            (second_linear): Linear(in_features=1463, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2020, bias=False)\n",
       "            (second_linear): Linear(in_features=2020, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2045, bias=False)\n",
       "            (second_linear): Linear(in_features=2045, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2071, bias=False)\n",
       "            (second_linear): Linear(in_features=2071, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (2): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1352, bias=False)\n",
       "            (second_linear): Linear(in_features=1352, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=548, bias=False)\n",
       "            (second_linear): Linear(in_features=548, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=646, bias=False)\n",
       "            (second_linear): Linear(in_features=646, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1483, bias=False)\n",
       "            (second_linear): Linear(in_features=1483, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2031, bias=False)\n",
       "            (second_linear): Linear(in_features=2031, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2051, bias=False)\n",
       "            (second_linear): Linear(in_features=2051, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2043, bias=False)\n",
       "            (second_linear): Linear(in_features=2043, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (3): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1420, bias=False)\n",
       "            (second_linear): Linear(in_features=1420, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=579, bias=False)\n",
       "            (second_linear): Linear(in_features=579, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=685, bias=False)\n",
       "            (second_linear): Linear(in_features=685, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1529, bias=False)\n",
       "            (second_linear): Linear(in_features=1529, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2031, bias=False)\n",
       "            (second_linear): Linear(in_features=2031, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2067, bias=False)\n",
       "            (second_linear): Linear(in_features=2067, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2018, bias=False)\n",
       "            (second_linear): Linear(in_features=2018, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1397, bias=False)\n",
       "            (second_linear): Linear(in_features=1397, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=584, bias=False)\n",
       "            (second_linear): Linear(in_features=584, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=671, bias=False)\n",
       "            (second_linear): Linear(in_features=671, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1496, bias=False)\n",
       "            (second_linear): Linear(in_features=1496, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2015, bias=False)\n",
       "            (second_linear): Linear(in_features=2015, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2075, bias=False)\n",
       "            (second_linear): Linear(in_features=2075, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2021, bias=False)\n",
       "            (second_linear): Linear(in_features=2021, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1347, bias=False)\n",
       "            (second_linear): Linear(in_features=1347, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=574, bias=False)\n",
       "            (second_linear): Linear(in_features=574, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=658, bias=False)\n",
       "            (second_linear): Linear(in_features=658, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1468, bias=False)\n",
       "            (second_linear): Linear(in_features=1468, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1998, bias=False)\n",
       "            (second_linear): Linear(in_features=1998, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2061, bias=False)\n",
       "            (second_linear): Linear(in_features=2061, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2017, bias=False)\n",
       "            (second_linear): Linear(in_features=2017, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1382, bias=False)\n",
       "            (second_linear): Linear(in_features=1382, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=595, bias=False)\n",
       "            (second_linear): Linear(in_features=595, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=666, bias=False)\n",
       "            (second_linear): Linear(in_features=666, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1482, bias=False)\n",
       "            (second_linear): Linear(in_features=1482, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1990, bias=False)\n",
       "            (second_linear): Linear(in_features=1990, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2059, bias=False)\n",
       "            (second_linear): Linear(in_features=2059, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2020, bias=False)\n",
       "            (second_linear): Linear(in_features=2020, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1355, bias=False)\n",
       "            (second_linear): Linear(in_features=1355, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=574, bias=False)\n",
       "            (second_linear): Linear(in_features=574, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=646, bias=False)\n",
       "            (second_linear): Linear(in_features=646, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1456, bias=False)\n",
       "            (second_linear): Linear(in_features=1456, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1986, bias=False)\n",
       "            (second_linear): Linear(in_features=1986, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2061, bias=False)\n",
       "            (second_linear): Linear(in_features=2061, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=1999, bias=False)\n",
       "            (second_linear): Linear(in_features=1999, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1329, bias=False)\n",
       "            (second_linear): Linear(in_features=1329, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=559, bias=False)\n",
       "            (second_linear): Linear(in_features=559, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=658, bias=False)\n",
       "            (second_linear): Linear(in_features=658, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1436, bias=False)\n",
       "            (second_linear): Linear(in_features=1436, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1962, bias=False)\n",
       "            (second_linear): Linear(in_features=1962, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2045, bias=False)\n",
       "            (second_linear): Linear(in_features=2045, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=1998, bias=False)\n",
       "            (second_linear): Linear(in_features=1998, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1335, bias=False)\n",
       "            (second_linear): Linear(in_features=1335, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=561, bias=False)\n",
       "            (second_linear): Linear(in_features=561, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=640, bias=False)\n",
       "            (second_linear): Linear(in_features=640, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1430, bias=False)\n",
       "            (second_linear): Linear(in_features=1430, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1952, bias=False)\n",
       "            (second_linear): Linear(in_features=1952, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2032, bias=False)\n",
       "            (second_linear): Linear(in_features=2032, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=1988, bias=False)\n",
       "            (second_linear): Linear(in_features=1988, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1340, bias=False)\n",
       "            (second_linear): Linear(in_features=1340, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=562, bias=False)\n",
       "            (second_linear): Linear(in_features=562, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=627, bias=False)\n",
       "            (second_linear): Linear(in_features=627, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1409, bias=False)\n",
       "            (second_linear): Linear(in_features=1409, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1953, bias=False)\n",
       "            (second_linear): Linear(in_features=1953, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2032, bias=False)\n",
       "            (second_linear): Linear(in_features=2032, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=1987, bias=False)\n",
       "            (second_linear): Linear(in_features=1987, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1385, bias=False)\n",
       "            (second_linear): Linear(in_features=1385, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=573, bias=False)\n",
       "            (second_linear): Linear(in_features=573, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=645, bias=False)\n",
       "            (second_linear): Linear(in_features=645, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1425, bias=False)\n",
       "            (second_linear): Linear(in_features=1425, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1960, bias=False)\n",
       "            (second_linear): Linear(in_features=1960, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2048, bias=False)\n",
       "            (second_linear): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2011, bias=False)\n",
       "            (second_linear): Linear(in_features=2011, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (12): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1290, bias=False)\n",
       "            (second_linear): Linear(in_features=1290, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=545, bias=False)\n",
       "            (second_linear): Linear(in_features=545, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=640, bias=False)\n",
       "            (second_linear): Linear(in_features=640, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1414, bias=False)\n",
       "            (second_linear): Linear(in_features=1414, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1955, bias=False)\n",
       "            (second_linear): Linear(in_features=1955, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2035, bias=False)\n",
       "            (second_linear): Linear(in_features=2035, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2014, bias=False)\n",
       "            (second_linear): Linear(in_features=2014, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1329, bias=False)\n",
       "            (second_linear): Linear(in_features=1329, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=570, bias=False)\n",
       "            (second_linear): Linear(in_features=570, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=664, bias=False)\n",
       "            (second_linear): Linear(in_features=664, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1427, bias=False)\n",
       "            (second_linear): Linear(in_features=1427, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1953, bias=False)\n",
       "            (second_linear): Linear(in_features=1953, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2044, bias=False)\n",
       "            (second_linear): Linear(in_features=2044, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2021, bias=False)\n",
       "            (second_linear): Linear(in_features=2021, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (14): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1302, bias=False)\n",
       "            (second_linear): Linear(in_features=1302, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=572, bias=False)\n",
       "            (second_linear): Linear(in_features=572, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=686, bias=False)\n",
       "            (second_linear): Linear(in_features=686, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1469, bias=False)\n",
       "            (second_linear): Linear(in_features=1469, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1945, bias=False)\n",
       "            (second_linear): Linear(in_features=1945, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2039, bias=False)\n",
       "            (second_linear): Linear(in_features=2039, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2028, bias=False)\n",
       "            (second_linear): Linear(in_features=2028, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1361, bias=False)\n",
       "            (second_linear): Linear(in_features=1361, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=582, bias=False)\n",
       "            (second_linear): Linear(in_features=582, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=691, bias=False)\n",
       "            (second_linear): Linear(in_features=691, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1467, bias=False)\n",
       "            (second_linear): Linear(in_features=1467, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1958, bias=False)\n",
       "            (second_linear): Linear(in_features=1958, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2034, bias=False)\n",
       "            (second_linear): Linear(in_features=2034, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2033, bias=False)\n",
       "            (second_linear): Linear(in_features=2033, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (16): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1422, bias=False)\n",
       "            (second_linear): Linear(in_features=1422, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=587, bias=False)\n",
       "            (second_linear): Linear(in_features=587, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=650, bias=False)\n",
       "            (second_linear): Linear(in_features=650, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1521, bias=False)\n",
       "            (second_linear): Linear(in_features=1521, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1981, bias=False)\n",
       "            (second_linear): Linear(in_features=1981, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2033, bias=False)\n",
       "            (second_linear): Linear(in_features=2033, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2024, bias=False)\n",
       "            (second_linear): Linear(in_features=2024, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (17): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1448, bias=False)\n",
       "            (second_linear): Linear(in_features=1448, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=611, bias=False)\n",
       "            (second_linear): Linear(in_features=611, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=661, bias=False)\n",
       "            (second_linear): Linear(in_features=661, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1506, bias=False)\n",
       "            (second_linear): Linear(in_features=1506, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1993, bias=False)\n",
       "            (second_linear): Linear(in_features=1993, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2034, bias=False)\n",
       "            (second_linear): Linear(in_features=2034, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2033, bias=False)\n",
       "            (second_linear): Linear(in_features=2033, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (18): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1435, bias=False)\n",
       "            (second_linear): Linear(in_features=1435, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=602, bias=False)\n",
       "            (second_linear): Linear(in_features=602, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=671, bias=False)\n",
       "            (second_linear): Linear(in_features=671, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1542, bias=False)\n",
       "            (second_linear): Linear(in_features=1542, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2003, bias=False)\n",
       "            (second_linear): Linear(in_features=2003, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2036, bias=False)\n",
       "            (second_linear): Linear(in_features=2036, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2034, bias=False)\n",
       "            (second_linear): Linear(in_features=2034, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (19): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1458, bias=False)\n",
       "            (second_linear): Linear(in_features=1458, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=618, bias=False)\n",
       "            (second_linear): Linear(in_features=618, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=688, bias=False)\n",
       "            (second_linear): Linear(in_features=688, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1535, bias=False)\n",
       "            (second_linear): Linear(in_features=1535, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2009, bias=False)\n",
       "            (second_linear): Linear(in_features=2009, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2037, bias=False)\n",
       "            (second_linear): Linear(in_features=2037, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2038, bias=False)\n",
       "            (second_linear): Linear(in_features=2038, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (20): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1488, bias=False)\n",
       "            (second_linear): Linear(in_features=1488, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=608, bias=False)\n",
       "            (second_linear): Linear(in_features=608, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=673, bias=False)\n",
       "            (second_linear): Linear(in_features=673, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1580, bias=False)\n",
       "            (second_linear): Linear(in_features=1580, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2010, bias=False)\n",
       "            (second_linear): Linear(in_features=2010, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2028, bias=False)\n",
       "            (second_linear): Linear(in_features=2028, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2019, bias=False)\n",
       "            (second_linear): Linear(in_features=2019, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (21): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1456, bias=False)\n",
       "            (second_linear): Linear(in_features=1456, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=591, bias=False)\n",
       "            (second_linear): Linear(in_features=591, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=648, bias=False)\n",
       "            (second_linear): Linear(in_features=648, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1566, bias=False)\n",
       "            (second_linear): Linear(in_features=1566, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2013, bias=False)\n",
       "            (second_linear): Linear(in_features=2013, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2029, bias=False)\n",
       "            (second_linear): Linear(in_features=2029, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2011, bias=False)\n",
       "            (second_linear): Linear(in_features=2011, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (22): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1481, bias=False)\n",
       "            (second_linear): Linear(in_features=1481, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=588, bias=False)\n",
       "            (second_linear): Linear(in_features=588, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=671, bias=False)\n",
       "            (second_linear): Linear(in_features=671, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1585, bias=False)\n",
       "            (second_linear): Linear(in_features=1585, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2016, bias=False)\n",
       "            (second_linear): Linear(in_features=2016, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2033, bias=False)\n",
       "            (second_linear): Linear(in_features=2033, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2004, bias=False)\n",
       "            (second_linear): Linear(in_features=2004, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (23): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1488, bias=False)\n",
       "            (second_linear): Linear(in_features=1488, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=594, bias=False)\n",
       "            (second_linear): Linear(in_features=594, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=704, bias=False)\n",
       "            (second_linear): Linear(in_features=704, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1548, bias=False)\n",
       "            (second_linear): Linear(in_features=1548, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2017, bias=False)\n",
       "            (second_linear): Linear(in_features=2017, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2040, bias=False)\n",
       "            (second_linear): Linear(in_features=2040, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=1991, bias=False)\n",
       "            (second_linear): Linear(in_features=1991, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (24): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1460, bias=False)\n",
       "            (second_linear): Linear(in_features=1460, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=588, bias=False)\n",
       "            (second_linear): Linear(in_features=588, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=662, bias=False)\n",
       "            (second_linear): Linear(in_features=662, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1543, bias=False)\n",
       "            (second_linear): Linear(in_features=1543, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2013, bias=False)\n",
       "            (second_linear): Linear(in_features=2013, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2043, bias=False)\n",
       "            (second_linear): Linear(in_features=2043, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=1987, bias=False)\n",
       "            (second_linear): Linear(in_features=1987, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (25): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1518, bias=False)\n",
       "            (second_linear): Linear(in_features=1518, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=635, bias=False)\n",
       "            (second_linear): Linear(in_features=635, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=692, bias=False)\n",
       "            (second_linear): Linear(in_features=692, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1548, bias=False)\n",
       "            (second_linear): Linear(in_features=1548, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1999, bias=False)\n",
       "            (second_linear): Linear(in_features=1999, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2036, bias=False)\n",
       "            (second_linear): Linear(in_features=2036, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2005, bias=False)\n",
       "            (second_linear): Linear(in_features=2005, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (26): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1338, bias=False)\n",
       "            (second_linear): Linear(in_features=1338, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=556, bias=False)\n",
       "            (second_linear): Linear(in_features=556, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=652, bias=False)\n",
       "            (second_linear): Linear(in_features=652, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1455, bias=False)\n",
       "            (second_linear): Linear(in_features=1455, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1986, bias=False)\n",
       "            (second_linear): Linear(in_features=1986, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=2024, bias=False)\n",
       "            (second_linear): Linear(in_features=2024, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2029, bias=False)\n",
       "            (second_linear): Linear(in_features=2029, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (27): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1413, bias=False)\n",
       "            (second_linear): Linear(in_features=1413, out_features=3072, bias=False)\n",
       "          )\n",
       "          (k_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=597, bias=False)\n",
       "            (second_linear): Linear(in_features=597, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=671, bias=False)\n",
       "            (second_linear): Linear(in_features=671, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1497, bias=False)\n",
       "            (second_linear): Linear(in_features=1497, out_features=3072, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1960, bias=False)\n",
       "            (second_linear): Linear(in_features=1960, out_features=8192, bias=False)\n",
       "          )\n",
       "          (up_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=3072, out_features=1993, bias=False)\n",
       "            (second_linear): Linear(in_features=1993, out_features=8192, bias=False)\n",
       "          )\n",
       "          (down_proj): LowRankLinear(\n",
       "            (first_linear): Linear(in_features=8192, out_features=2028, bias=False)\n",
       "            (second_linear): Linear(in_features=2028, out_features=3072, bias=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): LowRankLinear(\n",
       "    (first_linear): Linear(in_features=3072, out_features=2279, bias=False)\n",
       "    (second_linear): Linear(in_features=2279, out_features=128256, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowrank_model = make_lowrank_model(model, energy_threshold=0.8)\n",
    "lowrank_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe62a0",
   "metadata": {},
   "source": [
    "## lowrank check correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "daddaf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helloringgggggggggggggggappaddaddaddaddaddaddaddaddaddaddaddaddaddendendendendendendendendendendendendendendendendendendendend\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=lowrank_model, tokenizer=tokenizer, device='cuda:2')\n",
    "prompt = \"Hello\"\n",
    "answer = pipe(prompt, max_new_tokens=50, do_sample=False)[0][\"generated_text\"]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d416a2",
   "metadata": {},
   "source": [
    "## counting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d79ccebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "\n",
    "def print_param_count_table(module, simple_module_types=(nn.Linear, nn.Embedding)):\n",
    "    \"\"\"\n",
    "    Подсчитывает и выводит в табличной форме количество подмодулей \n",
    "    с определённым числом параметров.\n",
    "    \n",
    "    Args:\n",
    "        module (nn.Module): модель для анализа\n",
    "        simple_module_types (tuple): типы подмодулей для подсчёта параметров\n",
    "    \"\"\"\n",
    "    param_counts = []\n",
    "    for submodule in module.modules():\n",
    "        if type(submodule) in simple_module_types:\n",
    "            num_params = sum(p.numel() for p in submodule.parameters() if p.requires_grad)\n",
    "            param_counts.append(num_params)\n",
    "\n",
    "    total_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    \n",
    "    counter = Counter(param_counts)\n",
    "    \n",
    "    print(f\"Total trainable parameters in model: {total_params:,}\\n\")\n",
    "    print(f\"{'Parameters per submodule':>24} | {'Number of such submodules':>25}\")\n",
    "    print(\"-\" * 54)\n",
    "    for params_num, count in sorted(counter.items()):\n",
    "        print(f\"{params_num:24,} | {count:25,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "61347426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters in model: 3,212,749,824\n",
      "\n",
      "Parameters per submodule | Number of such submodules\n",
      "------------------------------------------------------\n",
      "               3,145,728 |                        56\n",
      "               9,437,184 |                        56\n",
      "              25,165,824 |                        84\n",
      "             394,002,432 |                         2\n"
     ]
    }
   ],
   "source": [
    "print_param_count_table(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "10178573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters in model: 3,235,007,232\n",
      "\n",
      "Parameters per submodule | Number of such submodules\n",
      "------------------------------------------------------\n",
      "                 365,568 |                         1\n",
      "                 489,472 |                         1\n",
      "                 558,080 |                         1\n",
      "                 561,152 |                         1\n",
      "                 569,344 |                         1\n",
      "                 572,416 |                         1\n",
      "                 574,464 |                         1\n",
      "                 575,488 |                         1\n",
      "                 583,680 |                         1\n",
      "                 585,728 |                         1\n",
      "                 586,752 |                         1\n",
      "                 587,776 |                         2\n",
      "                 592,896 |                         1\n",
      "                 595,968 |                         1\n",
      "                 598,016 |                         1\n",
      "                 601,088 |                         1\n",
      "                 602,112 |                         2\n",
      "                 605,184 |                         1\n",
      "                 608,256 |                         1\n",
      "                 609,280 |                         1\n",
      "                 611,328 |                         1\n",
      "                 616,448 |                         1\n",
      "                 622,592 |                         1\n",
      "                 625,664 |                         1\n",
      "                 627,712 |                         1\n",
      "                 632,832 |                         1\n",
      "                 642,048 |                         1\n",
      "                 650,240 |                         1\n",
      "                 655,360 |                         2\n",
      "                 660,480 |                         1\n",
      "                 661,504 |                         2\n",
      "                 663,552 |                         1\n",
      "                 665,600 |                         1\n",
      "                 667,648 |                         1\n",
      "                 673,792 |                         2\n",
      "                 676,864 |                         1\n",
      "                 677,888 |                         1\n",
      "                 679,936 |                         1\n",
      "                 680,960 |                         1\n",
      "                 681,984 |                         1\n",
      "                 687,104 |                         4\n",
      "                 689,152 |                         1\n",
      "                 701,440 |                         1\n",
      "                 702,464 |                         1\n",
      "                 704,512 |                         1\n",
      "                 707,584 |                         1\n",
      "                 708,608 |                         1\n",
      "                 720,896 |                         1\n",
      "               1,096,704 |                         1\n",
      "               1,468,416 |                         1\n",
      "               1,674,240 |                         1\n",
      "               1,683,456 |                         1\n",
      "               1,708,032 |                         1\n",
      "               1,717,248 |                         1\n",
      "               1,723,392 |                         1\n",
      "               1,726,464 |                         1\n",
      "               1,751,040 |                         1\n",
      "               1,757,184 |                         1\n",
      "               1,760,256 |                         1\n",
      "               1,763,328 |                         2\n",
      "               1,778,688 |                         1\n",
      "               1,787,904 |                         1\n",
      "               1,794,048 |                         1\n",
      "               1,803,264 |                         1\n",
      "               1,806,336 |                         2\n",
      "               1,815,552 |                         1\n",
      "               1,824,768 |                         1\n",
      "               1,827,840 |                         1\n",
      "               1,833,984 |                         1\n",
      "               1,849,344 |                         1\n",
      "               1,867,776 |                         1\n",
      "               1,876,992 |                         1\n",
      "               1,883,136 |                         1\n",
      "               1,898,496 |                         1\n",
      "               1,926,144 |                         1\n",
      "               1,950,720 |                         1\n",
      "               1,966,080 |                         2\n",
      "               1,981,440 |                         1\n",
      "               1,984,512 |                         2\n",
      "               1,990,656 |                         1\n",
      "               1,996,800 |                         1\n",
      "               2,002,944 |                         1\n",
      "               2,021,376 |                         2\n",
      "               2,030,592 |                         1\n",
      "               2,033,664 |                         1\n",
      "               2,039,808 |                         1\n",
      "               2,042,880 |                         1\n",
      "               2,045,952 |                         1\n",
      "               2,061,312 |                         4\n",
      "               2,067,456 |                         1\n",
      "               2,104,320 |                         1\n",
      "               2,107,392 |                         1\n",
      "               2,113,536 |                         1\n",
      "               2,122,752 |                         1\n",
      "               2,125,824 |                         1\n",
      "               2,162,688 |                         1\n",
      "               2,519,040 |                         2\n",
      "               3,597,312 |                         2\n",
      "               3,962,880 |                         2\n",
      "               3,999,744 |                         2\n",
      "               4,082,688 |                         4\n",
      "               4,101,120 |                         2\n",
      "               4,110,336 |                         2\n",
      "               4,116,480 |                         2\n",
      "               4,137,984 |                         2\n",
      "               4,153,344 |                         2\n",
      "               4,162,560 |                         2\n",
      "               4,180,992 |                         2\n",
      "               4,220,928 |                         2\n",
      "               4,245,504 |                         2\n",
      "               4,254,720 |                         2\n",
      "               4,291,584 |                         2\n",
      "               4,328,448 |                         2\n",
      "               4,340,736 |                         2\n",
      "               4,343,808 |                         2\n",
      "               4,362,240 |                         2\n",
      "               4,368,384 |                         2\n",
      "               4,377,600 |                         2\n",
      "               4,383,744 |                         2\n",
      "               4,392,960 |                         2\n",
      "               4,408,320 |                         2\n",
      "               4,411,392 |                         2\n",
      "               4,448,256 |                         2\n",
      "               4,469,760 |                         2\n",
      "               4,472,832 |                         4\n",
      "               4,478,976 |                         2\n",
      "               4,485,120 |                         2\n",
      "               4,494,336 |                         2\n",
      "               4,506,624 |                         2\n",
      "               4,509,696 |                         2\n",
      "               4,512,768 |                         2\n",
      "               4,549,632 |                         2\n",
      "               4,552,704 |                         2\n",
      "               4,555,776 |                         2\n",
      "               4,571,136 |                         4\n",
      "               4,595,712 |                         2\n",
      "               4,598,784 |                         2\n",
      "               4,626,432 |                         2\n",
      "               4,663,296 |                         2\n",
      "               4,672,512 |                         2\n",
      "               4,697,088 |                         2\n",
      "               4,715,520 |                         2\n",
      "               4,737,024 |                         2\n",
      "               4,740,096 |                         2\n",
      "               4,755,456 |                         4\n",
      "               4,810,752 |                         2\n",
      "               4,853,760 |                         2\n",
      "               4,869,120 |                         2\n",
      "               5,975,040 |                         1\n",
      "               5,996,544 |                         1\n",
      "               5,999,616 |                         2\n",
      "               6,005,760 |                         1\n",
      "               6,014,976 |                         1\n",
      "               6,021,120 |                         2\n",
      "               6,027,264 |                         1\n",
      "               6,085,632 |                         1\n",
      "               6,100,992 |                         2\n",
      "               6,104,064 |                         2\n",
      "               6,107,136 |                         1\n",
      "               6,113,280 |                         1\n",
      "               6,116,352 |                         1\n",
      "               6,122,496 |                         2\n",
      "               6,137,856 |                         2\n",
      "               6,140,928 |                         2\n",
      "               6,153,216 |                         1\n",
      "               6,156,288 |                         1\n",
      "               6,159,360 |                         1\n",
      "               6,171,648 |                         1\n",
      "               6,174,720 |                         1\n",
      "               6,177,792 |                         2\n",
      "               6,183,936 |                         2\n",
      "               6,187,008 |                         1\n",
      "               6,190,080 |                         1\n",
      "               6,193,152 |                         1\n",
      "               6,196,224 |                         2\n",
      "               6,199,296 |                         1\n",
      "               6,202,368 |                         1\n",
      "               6,205,440 |                         2\n",
      "               6,208,512 |                         2\n",
      "               6,217,728 |                         2\n",
      "               6,230,016 |                         3\n",
      "               6,233,088 |                         2\n",
      "               6,239,232 |                         2\n",
      "               6,242,304 |                         2\n",
      "               6,245,376 |                         4\n",
      "               6,248,448 |                         3\n",
      "               6,251,520 |                         1\n",
      "               6,254,592 |                         2\n",
      "               6,257,664 |                         1\n",
      "               6,260,736 |                         1\n",
      "               6,263,808 |                         2\n",
      "               6,266,880 |                         1\n",
      "               6,276,096 |                         2\n",
      "               6,279,168 |                         1\n",
      "               6,282,240 |                         2\n",
      "               6,291,456 |                         1\n",
      "               6,300,672 |                         1\n",
      "               6,325,248 |                         1\n",
      "               6,331,392 |                         3\n",
      "               6,349,824 |                         1\n",
      "               6,362,112 |                         1\n",
      "               6,374,400 |                         1\n",
      "               6,386,688 |                         1\n",
      "               7,001,088 |                         1\n",
      "              15,933,440 |                         1\n",
      "              15,990,784 |                         1\n",
      "              15,998,976 |                         2\n",
      "              16,015,360 |                         1\n",
      "              16,039,936 |                         1\n",
      "              16,056,320 |                         2\n",
      "              16,072,704 |                         1\n",
      "              16,228,352 |                         1\n",
      "              16,269,312 |                         2\n",
      "              16,277,504 |                         2\n",
      "              16,285,696 |                         1\n",
      "              16,302,080 |                         1\n",
      "              16,310,272 |                         1\n",
      "              16,326,656 |                         2\n",
      "              16,367,616 |                         2\n",
      "              16,375,808 |                         2\n",
      "              16,408,576 |                         1\n",
      "              16,416,768 |                         1\n",
      "              16,424,960 |                         1\n",
      "              16,457,728 |                         1\n",
      "              16,465,920 |                         1\n",
      "              16,474,112 |                         2\n",
      "              16,490,496 |                         2\n",
      "              16,498,688 |                         1\n",
      "              16,506,880 |                         1\n",
      "              16,515,072 |                         1\n",
      "              16,523,264 |                         2\n",
      "              16,531,456 |                         1\n",
      "              16,539,648 |                         1\n",
      "              16,547,840 |                         2\n",
      "              16,556,032 |                         2\n",
      "              16,580,608 |                         2\n",
      "              16,613,376 |                         3\n",
      "              16,621,568 |                         2\n",
      "              16,637,952 |                         2\n",
      "              16,646,144 |                         2\n",
      "              16,654,336 |                         4\n",
      "              16,662,528 |                         3\n",
      "              16,670,720 |                         1\n",
      "              16,678,912 |                         2\n",
      "              16,687,104 |                         1\n",
      "              16,695,296 |                         1\n",
      "              16,703,488 |                         2\n",
      "              16,711,680 |                         1\n",
      "              16,736,256 |                         2\n",
      "              16,744,448 |                         1\n",
      "              16,752,640 |                         2\n",
      "              16,777,216 |                         1\n",
      "              16,801,792 |                         1\n",
      "              16,867,328 |                         1\n",
      "              16,883,712 |                         3\n",
      "              16,932,864 |                         1\n",
      "              16,965,632 |                         1\n",
      "              16,998,400 |                         1\n",
      "              17,031,168 |                         1\n",
      "             292,295,424 |                         1\n",
      "             394,002,432 |                         1\n"
     ]
    }
   ],
   "source": [
    "print_param_count_table(lowrank_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8166d",
   "metadata": {},
   "source": [
    "## benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "18035ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   4%|▍         | 4/100 [00:06<02:39,  1.67s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcais/mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m, subtask, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m----> 7\u001b[0m     correct, count \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_slimmed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m correct\n\u001b[1;32m      9\u001b[0m     total_examples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m count\n",
      "Cell \u001b[0;32mIn[105], line 19\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, tokenizer, dataset, num_shots, verbose)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[1;32m     18\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m format_mmlu_prompt(example, preprompt\u001b[38;5;241m=\u001b[39mpre_prompt )\n\u001b[0;32m---> 19\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# raise Exception()\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# print(output)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     predicted_answer \u001b[38;5;241m=\u001b[39m extract_answer(output, example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:316\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/pipelines/base.py:1464\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1457\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1458\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1461\u001b[0m         )\n\u001b[1;32m   1462\u001b[0m     )\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/pipelines/base.py:1471\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1470\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1471\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1472\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/pipelines/base.py:1371\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1370\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1371\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:414\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    412\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 414\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    417\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/generation/utils.py:2623\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2615\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2616\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2617\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2618\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2619\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2620\u001b[0m     )\n\u001b[1;32m   2622\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2623\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2634\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2635\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2636\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2637\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2638\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2639\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2640\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/generation/utils.py:3607\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3605\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3607\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3609\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3610\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3611\u001b[0m     outputs,\n\u001b[1;32m   3612\u001b[0m     model_kwargs,\n\u001b[1;32m   3613\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3614\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:552\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    548\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    549\u001b[0m )\n\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 552\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    565\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:440\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    438\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 440\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:287\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[FlashAttentionKwargs],\n\u001b[1;32m    285\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mFloatTensor, Optional[\u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mFloatTensor, torch\u001b[38;5;241m.\u001b[39mFloatTensor]]]:\n\u001b[1;32m    286\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 287\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    291\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    292\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    300\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tinyllama-env/lib/python3.10/site-packages/accelerate/hooks.py:373\u001b[0m, in \u001b[0;36mAlignDevicesHook.post_forward\u001b[0;34m(self, module, output)\u001b[0m\n\u001b[1;32m    360\u001b[0m             set_module_tensor_to_device(\n\u001b[1;32m    361\u001b[0m                 module,\n\u001b[1;32m    362\u001b[0m                 name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m                 tied_params_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map,\n\u001b[1;32m    367\u001b[0m             )\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(\n\u001b[1;32m    370\u001b[0m         kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys\n\u001b[1;32m    371\u001b[0m     )\n\u001b[0;32m--> 373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, module, output):\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffload:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m named_module_tensors(\n\u001b[1;32m    376\u001b[0m             module,\n\u001b[1;32m    377\u001b[0m             include_buffers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffload_buffers,\n\u001b[1;32m    378\u001b[0m             recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_submodules,\n\u001b[1;32m    379\u001b[0m             remove_non_persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    380\u001b[0m         ):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "total_examples = 0\n",
    "\n",
    "for subtask in all_subtasks[:10]:\n",
    "    dataset = load_dataset(\"cais/mmlu\", subtask, split=\"test\")\n",
    "    if dataset.shape[0] < 200:\n",
    "        correct, count = evaluate_model(model_slimmed, tokenizer, dataset, verbose=False)\n",
    "        total_correct += correct\n",
    "        total_examples += count\n",
    "\n",
    "overall_accuracy = total_correct / total_examples if total_examples > 0 else 0\n",
    "print(f\"\\n🎯 Общая точность на всём MMLU: {overall_accuracy:.2f} ({total_correct}/{total_examples})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4fc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyllama-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
