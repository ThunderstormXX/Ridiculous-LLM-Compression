{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a93f36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user31/.conda/envs/tinyllama-env/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "torch.cuda.is_available()\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "\n",
    "# local_model_path = \"/home/user31/polina/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     local_model_path,\n",
    "#     device_map=\"cuda:3\",\n",
    "#     # load_in_4bit=True,            # активируем 4-bit квантование\n",
    "#     torch_dtype=\"auto\"            # автоматически выбираем тип данных\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "355b6104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from transformers import EvalPrediction\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "import warnings\n",
    "import transformers\n",
    "import os \n",
    "# 1. Подавление всех Python-предупреждений\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39d004df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = model.device\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "468d8657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3072, out_features=128256, bias=False)\n"
     ]
    }
   ],
   "source": [
    "print(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd8be50",
   "metadata": {},
   "source": [
    "## Prune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c28a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import LlamaForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "\n",
    "def prune_layers(model, layers_to_remove):\n",
    "    \"\"\"\n",
    "    Удаляет указанные слои из модели LlamaForCausalLM.\n",
    "\n",
    "    Args:\n",
    "        model (LlamaForCausalLM): Исходная модель.\n",
    "        layers_to_remove (list): Индексы слоёв, которые нужно удалить.\n",
    "\n",
    "    Returns:\n",
    "        LlamaForCausalLM: Новая модель с удалёнными слоями.\n",
    "    \"\"\"\n",
    "    new_model = LlamaForCausalLM(config=model.config)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Копируем эмбеддинги\n",
    "        new_model.model.embed_tokens.weight.data = model.model.embed_tokens.weight.data.clone()\n",
    "        \n",
    "        # Копируем оставшиеся слои энкодера\n",
    "        new_layers = [\n",
    "            layer for idx, layer in enumerate(model.model.layers) if idx not in layers_to_remove\n",
    "        ]\n",
    "        new_model.model.layers = torch.nn.ModuleList(new_layers)\n",
    "        \n",
    "        # Копируем параметры нормализации\n",
    "        new_model.model.norm.weight.data = model.model.norm.weight.data.clone()\n",
    "        \n",
    "        # Копируем lm_head\n",
    "        new_model.lm_head.weight.data = model.lm_head.weight.data.clone()\n",
    "\n",
    "    return new_model\n",
    "\n",
    "def reuse_previous_layers(model, layers_to_replace):\n",
    "    \"\"\"\n",
    "    Заменяет указанные слои ссылками на предыдущие слои (повторное использование) в модели LlamaForCausalLM.\n",
    "\n",
    "    Args:\n",
    "        model (LlamaForCausalLM): Исходная модель.\n",
    "        layers_to_replace (list): Индексы слоёв, которые нужно заменить ссылкой на предыдущий слой.\n",
    "\n",
    "    Returns:\n",
    "        LlamaForCausalLM: Новая модель с переиспользуемыми слоями.\n",
    "    \"\"\"\n",
    "    new_model = LlamaForCausalLM(config=model.config)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Копируем эмбеддинги\n",
    "        new_model.model.embed_tokens.weight = model.model.embed_tokens.weight\n",
    "\n",
    "        # Подготавливаем новые слои\n",
    "        new_layers = []\n",
    "        for idx, layer in enumerate(model.model.layers):\n",
    "            if idx in layers_to_replace:\n",
    "                if idx == 0:\n",
    "                    raise ValueError(\"Нельзя заменить нулевой слой на предыдущий — его не существует.\")\n",
    "                # Используем ссылку на предыдущий слой\n",
    "                new_layers.append(model.model.layers[idx - 1])\n",
    "            else:\n",
    "                new_layers.append(layer)\n",
    "\n",
    "        new_model.model.layers = torch.nn.ModuleList(new_layers)\n",
    "\n",
    "        # Копируем параметры нормализации\n",
    "        new_model.model.norm.weight = model.model.norm.weight\n",
    "\n",
    "        # Копируем lm_head\n",
    "        new_model.lm_head.weight = model.lm_head.weight\n",
    "\n",
    "    return new_model\n",
    "\n",
    "\n",
    "\n",
    "def print_param_count_table(module, simple_module_types=(nn.Linear, nn.Embedding)):\n",
    "    \"\"\"\n",
    "    Подсчитывает и выводит в табличной форме количество подмодулей \n",
    "    с определённым числом параметров.\n",
    "    \n",
    "    Args:\n",
    "        module (nn.Module): модель для анализа\n",
    "        simple_module_types (tuple): типы подмодулей для подсчёта параметров\n",
    "    \"\"\"\n",
    "    param_counts = []\n",
    "    for submodule in module.modules():\n",
    "        if type(submodule) in simple_module_types:\n",
    "            num_params = sum(p.numel() for p in submodule.parameters() if p.requires_grad)\n",
    "            param_counts.append(num_params)\n",
    "\n",
    "    total_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    \n",
    "    counter = Counter(param_counts)\n",
    "    \n",
    "    print(f\"Total trainable parameters in model: {total_params:,}\\n\")\n",
    "    print(f\"{'Parameters per submodule':>24} | {'Number of such submodules':>25}\")\n",
    "    print(\"-\" * 54)\n",
    "    for params_num, count in sorted(counter.items()):\n",
    "        print(f\"{params_num:24,} | {count:25,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49ab9fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters in model: 3,212,749,824\n",
      "\n",
      "Parameters per submodule | Number of such submodules\n",
      "------------------------------------------------------\n",
      "               3,145,728 |                        56\n",
      "               9,437,184 |                        56\n",
      "              25,165,824 |                        84\n",
      "             394,002,432 |                         2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, transformers.models.llama.modeling_llama.LlamaForCausalLM)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_param_count_table(model), type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04a33e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_slimmed = prune_layers(model, layers_to_remove=[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c96c78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters in model: 3,112,080,384\n",
      "\n",
      "Parameters per submodule | Number of such submodules\n",
      "------------------------------------------------------\n",
      "               3,145,728 |                        54\n",
      "               9,437,184 |                        54\n",
      "              25,165,824 |                        81\n",
      "             394,002,432 |                         2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, transformers.models.llama.modeling_llama.LlamaForCausalLM)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_param_count_table(model_slimmed), type(model_slimmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ba2292",
   "metadata": {},
   "source": [
    "## Quality of answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc22367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(model, tokenizer = tokenizer):\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=model.device )\n",
    "\n",
    "def test_quality_pipeline(pipeline, prompt = 'Hello, tell me about the situation in Russia this year'):\n",
    "    print(pipeline(prompt, max_new_tokens=200, do_sample=False)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1d88f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18f6f2209da4641847aa972636e9a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "local_model_path = \"/home/user31/polina/Llama-3.2-3B-Instruct\"\n",
    "pruned_model_path = \"/home/user31/igor/Llama-3.2-3B-Instruct-pruned\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pruned_model_path,\n",
    "    device_map=\"cuda:3\",\n",
    "    # load_in_4bit=True,            # активируем 4-bit квантование\n",
    "    torch_dtype=\"auto\"            # автоматически выбираем тип данных\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95c72125",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = get_pipeline(model, tokenizer = tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d57e6073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, tell me about the situation in Russia this year so thought I must thank you for reporting it so I must thank you for reporting it so I must thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank\n"
     ]
    }
   ],
   "source": [
    "test_quality_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d890395a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, tell me about the situation in Russia this year so thought I must thank you for reporting it so I must thank you for reporting it so I must thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank\n"
     ]
    }
   ],
   "source": [
    "test_quality_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37a229d",
   "metadata": {},
   "source": [
    "## LORA config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f0edf",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "248735c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "# Токенизируем датасет\n",
    "def tokenize_function(examples):\n",
    "    # Устанавливаем максимальную длину и задаем параметры padding и truncation\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "def format_dataset(examples):\n",
    "    # GPT-2 использует те же `input_ids` в качестве `labels`\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(format_dataset, batched=True)\n",
    "\n",
    "# Подготавливаем данные для тренировки\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "full_train_dataset = tokenized_datasets[\"train\"]\n",
    "small_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(100))\n",
    "full_eval_dataset = tokenized_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae64bb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36718, 3), (100, 3))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_dataset.shape, small_eval_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d699d4",
   "metadata": {},
   "source": [
    "## lora adpter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa972c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.peft_model.PeftModelForCausalLM"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_slimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "745d4f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,                      # размер low-rank\n",
    "    lora_alpha=32,            # масштаб\n",
    "    target_modules=[\"lm_head\"],  # важно: имя слоя внутри mlp\n",
    "    lora_dropout=0.05,        # dropout перед адаптером\n",
    "    bias=\"none\",              # bias не трогаем\n",
    "    task_type=TaskType.CAUSAL_LM  # указываем тип задачи\n",
    ")\n",
    "\n",
    "# Применяем LoRA к модели\n",
    "model_slimmed = get_peft_model(model_slimmed, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2889e77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.peft_model.PeftModelForCausalLM"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_slimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72994cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.950817584991455, 'perplexity': 1044.0029296875, 'iterations': 0, 'step_time': 3.06370210647583, 'epoch': 0}\n",
      "{'loss': 6.7499260902404785, 'perplexity': 853.9956665039062, 'iterations': 0, 'step_time': 1.3029866218566895, 'epoch': 0}\n",
      "{'loss': 6.3779, 'grad_norm': 0.5703125, 'learning_rate': 9.8e-05, 'epoch': 0.005446919766871834}\n",
      "{'loss': 0.27523621916770935, 'perplexity': 1.316841721534729, 'iterations': 50, 'step_time': 155.27723860740662, 'epoch': 0.005446919766871834}\n",
      "{'loss': 0.021796170622110367, 'perplexity': 1.0220354795455933, 'iterations': 50, 'step_time': 1.3078052997589111, 'epoch': 0.005446919766871834}\n",
      "{'loss': 0.3054, 'grad_norm': 1.1640625, 'learning_rate': 0.00019800000000000002, 'epoch': 0.010893839533743668}\n",
      "{'loss': 0.10975276678800583, 'perplexity': 1.116002082824707, 'iterations': 100, 'step_time': 155.51432919502258, 'epoch': 0.010893839533743668}\n",
      "{'loss': 0.007229093462228775, 'perplexity': 1.0072553157806396, 'iterations': 100, 'step_time': 1.2859933376312256, 'epoch': 0.010893839533743668}\n",
      "{'loss': 0.3794, 'grad_norm': 1.03125, 'learning_rate': 0.00018911111111111112, 'epoch': 0.0163407593006155}\n",
      "{'loss': 0.007305863779038191, 'perplexity': 1.007332682609558, 'iterations': 150, 'step_time': 155.6140878200531, 'epoch': 0.0163407593006155}\n",
      "{'loss': 0.1476799100637436, 'perplexity': 1.1591418981552124, 'iterations': 150, 'step_time': 1.3060064315795898, 'epoch': 0.0163407593006155}\n",
      "{'loss': 0.3934, 'grad_norm': 0.83203125, 'learning_rate': 0.00017800000000000002, 'epoch': 0.021787679067487336}\n",
      "{'loss': 0.14567577838897705, 'perplexity': 1.1568210124969482, 'iterations': 200, 'step_time': 155.64095640182495, 'epoch': 0.021787679067487336}\n",
      "{'loss': 0.2256343513727188, 'perplexity': 1.253117322921753, 'iterations': 200, 'step_time': 1.3087060451507568, 'epoch': 0.021787679067487336}\n",
      "{'loss': 0.3438, 'grad_norm': 1.1015625, 'learning_rate': 0.0001668888888888889, 'epoch': 0.02723459883435917}\n",
      "{'loss': 0.4377139210700989, 'perplexity': 1.5491615533828735, 'iterations': 250, 'step_time': 155.56684398651123, 'epoch': 0.02723459883435917}\n",
      "{'loss': 0.007086113095283508, 'perplexity': 1.0071113109588623, 'iterations': 250, 'step_time': 1.2862441539764404, 'epoch': 0.02723459883435917}\n",
      "Обучение прервано пользователем\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "# Кастомный Trainer с расширенным логгированием\n",
    "class IterationLimitedTrainer(Trainer):\n",
    "    def __init__(self, *args, max_iterations=1000, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.max_iterations = max_iterations\n",
    "        self.writer = SummaryWriter(log_dir=self.args.logging_dir)\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def training_step(self, model, inputs, num_steps=None):\n",
    "        # Ограничение по числу итераций\n",
    "        if self.state.global_step >= self.max_iterations:\n",
    "            self.control.should_training_stop = True\n",
    "            return torch.tensor(0.0).to(self.args.device)\n",
    "        \n",
    "        # Стандартный шаг обучения\n",
    "        loss = super().training_step(model, inputs, num_steps)\n",
    "        \n",
    "        # Логгирование на каждом шаге\n",
    "        if self.state.global_step % self.args.logging_steps == 0:\n",
    "            perplexity = torch.exp(loss.detach())\n",
    "            step_time = time.time() - self.start_time\n",
    "            \n",
    "            # Основные метрики\n",
    "            self.log({\n",
    "                \"loss\": loss.item(),\n",
    "                \"perplexity\": perplexity.item(),\n",
    "                \"iterations\": self.state.global_step,\n",
    "                \"step_time\": step_time\n",
    "            })\n",
    "            \n",
    "            # TensorBoard логгирование\n",
    "            self.writer.add_scalar(\"train/loss\", loss.item(), self.state.global_step)\n",
    "            self.writer.add_scalar(\"train/perplexity\", perplexity.item(), self.state.global_step)\n",
    "            self.writer.add_scalar(\"train/learning_rate\", self._get_learning_rate(), self.state.global_step)\n",
    "            \n",
    "            # Сброс таймера для следующего шага\n",
    "            self.start_time = time.time()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# Конфигурация обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    do_eval=True,\n",
    "    max_steps=1000,  # Ограничение по числу итераций\n",
    "    logging_dir=\"./logs\",\n",
    "    save_steps=400,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,  # Логгировать каждые 10 шагов\n",
    "    eval_steps=500,     # Оценка каждые 50 шагов\n",
    "\n",
    "    report_to=\"tensorboard\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=100,   # Прогрев для первых 100 шагов\n",
    ")\n",
    "\n",
    "\n",
    "# Инициализация тренера\n",
    "trainer = IterationLimitedTrainer(\n",
    "    model=model_slimmed,\n",
    "    args=training_args,\n",
    "    train_dataset=full_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_iterations=500  # Кастомное ограничение (приоритетнее max_steps)\n",
    ")\n",
    "\n",
    "# Запуск обучения\n",
    "try:\n",
    "    results = trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Обучение прервано пользователем\")\n",
    "finally:\n",
    "    # Сохранение финальной модели\n",
    "    trainer.save_model(\"final_model\")\n",
    "    trainer.writer.close()  # Важно закрыть writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "818c98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f17f0fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 319862), started 0:00:04 ago. (Use '!kill 319862' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4b00fecf6fbecc77\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4b00fecf6fbecc77\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a66e1d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75276e8c31d249eebec39287156e1818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = './output/checkpoint-375/'\n",
    "trained_model_slimmed = AutoModelForCausalLM.from_pretrained(model_path , device_map=\"cuda:3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5988369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello everyone! I'm excited to share with you my latest project, a new recipe for a delicious and healthy dessert that I like to call \"Berry Bliss Bites.\" These bite-sized treats are perfect for hot summer days when you need a sweet and refreshing pick-me-up.\n",
      "\n",
      "**Berry Bliss Bites Recipe**\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "- 1 cup rolled oats\n",
      "- 1/2 cup almond butter\n",
      "- 1/4 cup honey\n",
      "- 1/4 cup chopped fresh berries (such as blueberries, strawberries, or raspberries)\n",
      "- 1/4 cup chopped nuts (such as almonds or walnuts)\n",
      "- 1/4 cup shredded coconut (optional)\n",
      "- Pinch of salt\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. In a large mixing bowl, combine the oats and almond butter. Mix until well combined and a dough forms.\n",
      "2. Add the honey and mix until the dough is smooth and creamy.\n",
      "3. Fold in the chopped berries and nuts.\n",
      "4. If using\n"
     ]
    }
   ],
   "source": [
    "pipe =  get_pipeline(trained_model_slimmed, tokenizer = tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f4dab8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_quality_pipeline(pipe, prompt = 'Hello, how are you?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9d4b07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyllama-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
