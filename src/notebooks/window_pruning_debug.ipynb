{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window Pruning Debug Notebook\n",
    "\n",
    "This notebook tests the window pruning approach step by step for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Device selection\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "i = 0  # device number to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f'{i}'\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.pruninghealing import Trainer, DatasetLoader, WindowPruner\n",
    "from src.pruninghealing.utils import load_model_and_tokenizer, calculate_perplexity, get_model_layers\n",
    "from src.pruninghealing.logger import Logger\n",
    "\n",
    "print(f'Using GPU device {i}: {torch.cuda.get_device_name(0)}' if torch.cuda.is_available() else 'Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_PATH = \"../checkpoints/tinyllama\"  # Change to your model\n",
    "WORKSPACE = \"../../workspace/window_debug\"\n",
    "WINDOW_SIZE = 3\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"Loading model...\")\n",
    "model, tokenizer = load_model_and_tokenizer(MODEL_PATH)\n",
    "\n",
    "print(f\"Model loaded: {get_model_layers(model)} layers\")\n",
    "print(f\"Model type: {model.config.model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset_loader = DatasetLoader(tokenizer)\n",
    "dataset_loader.load_wikitext(train_size=500, eval_size=50)  # Small for debugging\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset_loader.train_dataset)} train, {len(dataset_loader.eval_dataset)} eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Baseline Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate baseline perplexity\n",
    "print(\"Calculating baseline perplexity...\")\n",
    "baseline_ppl = calculate_perplexity(model, tokenizer, max_samples=20)\n",
    "print(f\"Baseline perplexity: {baseline_ppl:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pruner, trainer, and logger\n",
    "pruner = WindowPruner(model, tokenizer, WORKSPACE)\n",
    "trainer = Trainer(model, tokenizer, WORKSPACE)\n",
    "logger = Logger(WORKSPACE)\n",
    "\n",
    "# Log baseline\n",
    "logger.log_step({\n",
    "    \"step\": 0,\n",
    "    \"action\": \"baseline\",\n",
    "    \"layers_total\": get_model_layers(model),\n",
    "    \"perplexity\": baseline_ppl\n",
    "})\n",
    "\n",
    "print(\"Components initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Question Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed test question for quality evaluation\n",
    "TEST_PROMPT = \"What is the capital of France?\"\n",
    "\n",
    "def test_model_quality(model, tokenizer, prompt=TEST_PROMPT):\n",
    "    \"\"\"Test model response quality\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=20,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test baseline\n",
    "print(f\"Test prompt: {TEST_PROMPT}\")\n",
    "baseline_response = test_model_quality(model, tokenizer)\n",
    "print(f\"Baseline response: {baseline_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Unimportant Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find least important window\n",
    "print(f\"Finding least important window of size {WINDOW_SIZE}...\")\n",
    "best_window, best_score = pruner.find_unimportant_window(WINDOW_SIZE)\n",
    "\n",
    "print(f\"Best window to remove: {best_window}\")\n",
    "print(f\"Window score: {best_score}\")\n",
    "\n",
    "# Log window selection\n",
    "logger.log_step({\n",
    "    \"step\": 1,\n",
    "    \"action\": \"window_selection\",\n",
    "    \"window_size\": WINDOW_SIZE,\n",
    "    \"selected_window\": best_window,\n",
    "    \"window_score\": best_score\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Window Importance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different windows for comparison\n",
    "num_layers = get_model_layers(model)\n",
    "print(f\"\\nTesting all possible windows of size {WINDOW_SIZE}:\")\n",
    "\n",
    "window_scores = []\n",
    "for start_idx in range(num_layers - WINDOW_SIZE + 1):\n",
    "    window = list(range(start_idx, start_idx + WINDOW_SIZE))\n",
    "    score = pruner._evaluate_window_importance(window)\n",
    "    window_scores.append((window, score))\n",
    "    print(f\"Window {window}: score = {score}\")\n",
    "\n",
    "# Sort by score\n",
    "window_scores.sort(key=lambda x: x[1])\n",
    "print(f\"\\nBest (lowest score) window: {window_scores[0][0]} with score {window_scores[0][1]}\")\n",
    "print(f\"Worst (highest score) window: {window_scores[-1][0]} with score {window_scores[-1][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune the selected window\n",
    "print(f\"\\nPruning window {best_window}...\")\n",
    "layers_before = get_model_layers(model)\n",
    "\n",
    "try:\n",
    "    pruned_model = pruner.prune_window(best_window)\n",
    "    layers_after = get_model_layers(pruned_model)\n",
    "    \n",
    "    print(f\"Layers before pruning: {layers_before}\")\n",
    "    print(f\"Layers after pruning: {layers_after}\")\n",
    "    print(f\"Layers removed: {layers_before - layers_after}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during pruning: {e}\")\n",
    "    pruned_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Perplexity After Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate perplexity after pruning\n",
    "print(\"Calculating perplexity after pruning...\")\n",
    "ppl_after_prune = calculate_perplexity(pruned_model, tokenizer, max_samples=20)\n",
    "response_after_prune = test_model_quality(pruned_model, tokenizer)\n",
    "\n",
    "print(f\"Perplexity after pruning: {ppl_after_prune:.3f}\")\n",
    "print(f\"Perplexity change: {ppl_after_prune - baseline_ppl:.3f}\")\n",
    "print(f\"Response after pruning: {response_after_prune}\")\n",
    "\n",
    "# Log pruning results\n",
    "logger.log_step({\n",
    "    \"step\": 2,\n",
    "    \"action\": \"window_pruning\",\n",
    "    \"pruned_window\": best_window,\n",
    "    \"layers_before\": layers_before,\n",
    "    \"layers_after\": layers_after,\n",
    "    \"ppl_before_prune\": baseline_ppl,\n",
    "    \"ppl_after_prune\": ppl_after_prune,\n",
    "    \"ppl_degradation\": ppl_after_prune - baseline_ppl,\n",
    "    \"response_after_prune\": response_after_prune\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune Pruned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the pruned model\n",
    "print(\"\\nFine-tuning pruned model...\")\n",
    "trainer.model = pruned_model\n",
    "finetuned_model = trainer.train(dataset_loader, max_steps=200)  # Small for debugging\n",
    "\n",
    "print(\"Fine-tuning completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Final Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final perplexity\n",
    "print(\"Calculating final perplexity...\")\n",
    "final_ppl = calculate_perplexity(finetuned_model, tokenizer, max_samples=20)\n",
    "final_response = test_model_quality(finetuned_model, tokenizer)\n",
    "\n",
    "print(f\"Final perplexity: {final_ppl:.3f}\")\n",
    "print(f\"Final response: {final_response}\")\n",
    "\n",
    "# Calculate improvements\n",
    "healing_improvement = ppl_after_prune - final_ppl\n",
    "total_change = final_ppl - baseline_ppl\n",
    "\n",
    "print(f\"\\n=== RESULTS ===\")\n",
    "print(f\"Baseline: {baseline_ppl:.3f} | {baseline_response}\")\n",
    "print(f\"After pruning: {ppl_after_prune:.3f} (change: {ppl_after_prune - baseline_ppl:+.3f}) | {response_after_prune}\")\n",
    "print(f\"After fine-tuning: {final_ppl:.3f} (healing: {healing_improvement:+.3f}) | {final_response}\")\n",
    "print(f\"Total change: {total_change:+.3f}\")\n",
    "\n",
    "# Log final results\n",
    "logger.log_step({\n",
    "    \"step\": 3,\n",
    "    \"action\": \"fine_tuning\",\n",
    "    \"ppl_before_finetune\": ppl_after_prune,\n",
    "    \"ppl_after_finetune\": final_ppl,\n",
    "    \"healing_improvement\": healing_improvement,\n",
    "    \"total_change\": total_change,\n",
    "    \"final_response\": final_response\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all logs\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(logger.logs)\n",
    "print(\"\\n=== DETAILED LOGS ===\")\n",
    "print(df)\n",
    "\n",
    "# Plot if possible\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    perplexities = [baseline_ppl, ppl_after_prune, final_ppl]\n",
    "    stages = ['Baseline', 'After Pruning', 'After Fine-tuning']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(stages, perplexities, 'bo-', linewidth=2, markersize=8)\n",
    "    plt.ylabel('Perplexity')\n",
    "    plt.title('Window Pruning and Healing Process')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (stage, ppl) in enumerate(zip(stages, perplexities)):\n",
    "        plt.annotate(f'{ppl:.3f}', (i, ppl), textcoords=\"offset points\", \n",
    "                    xytext=(0,10), ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Matplotlib not available for plotting\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}