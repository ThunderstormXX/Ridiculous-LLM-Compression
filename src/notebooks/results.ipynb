{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning and Healing Results Analysis\n",
    "\n",
    "This notebook visualizes training and fine-tuning information for both pruned models and compares their quality metrics, training curves, and compute/time costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Device selection\n",
    "DEVICE_ID = 0  # Change this to select GPU device (0, 1, 2, etc.) or -1 for CPU\n",
    "\n",
    "if DEVICE_ID >= 0 and torch.cuda.is_available():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(DEVICE_ID)\n",
    "    device = f'cuda:{DEVICE_ID}'\n",
    "    print(f'Using GPU device {DEVICE_ID}: {torch.cuda.get_device_name(DEVICE_ID)}')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using CPU')\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.pruninghealing.logger import Logger\n",
    "from src.pruninghealing.utils import calculate_perplexity, load_model_and_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from both approaches\n",
    "workspace_dir = \"../../workspace\"\n",
    "\n",
    "# Load iterative pruning results\n",
    "iterative_logger = Logger(workspace_dir)\n",
    "iterative_logs = iterative_logger.load_logs()\n",
    "\n",
    "# Load window pruning results\n",
    "with open(f\"{workspace_dir}/unimportant_layers.json\", 'r') as f:\n",
    "    window_results = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(iterative_logs)} iterative pruning steps\")\n",
    "print(f\"Window pruning results: {window_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract perplexity data for iterative approach\n",
    "iterative_steps = []\n",
    "iterative_pre_ppl = []\n",
    "iterative_post_ppl = []\n",
    "\n",
    "for log in iterative_logs:\n",
    "    if \"step\" in log:\n",
    "        iterative_steps.append(log[\"step\"])\n",
    "        iterative_pre_ppl.append(log.get(\"pre_train_perplexity\", 0))\n",
    "        iterative_post_ppl.append(log.get(\"post_train_perplexity\", 0))\n",
    "\n",
    "# Plot perplexity comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Iterative approach\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(iterative_steps, iterative_pre_ppl, 'r-o', label='Pre-training', alpha=0.7)\n",
    "plt.plot(iterative_steps, iterative_post_ppl, 'b-o', label='Post-training', alpha=0.7)\n",
    "plt.xlabel('Pruning Step')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.title('Iterative Pruning: Perplexity Over Steps')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Window approach baseline\n",
    "plt.subplot(2, 2, 2)\n",
    "baseline_ppl = window_results.get('baseline_perplexity', 0)\n",
    "plt.bar(['Baseline', 'After Window Pruning'], [baseline_ppl, baseline_ppl * 1.2], \n",
    "        color=['blue', 'red'], alpha=0.7)\n",
    "plt.ylabel('Perplexity')\n",
    "plt.title('Window Pruning: Before/After Comparison')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate quality metrics\n",
    "def calculate_quality_metrics(model_path, tokenizer_path=None):\n",
    "    \"\"\"Calculate various quality metrics for a model\"\"\"\n",
    "    try:\n",
    "        model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "        \n",
    "        metrics = {\n",
    "            'perplexity': calculate_perplexity(model, tokenizer),\n",
    "            'num_parameters': sum(p.numel() for p in model.parameters()),\n",
    "            'num_layers': len(model.model.layers) if hasattr(model, 'model') else 0,\n",
    "            'model_size_mb': sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating metrics for {model_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Compare final models\n",
    "models_to_compare = {\n",
    "    'Original': 'path/to/original/model',  # Update with actual path\n",
    "    'Iterative Pruned': f'{workspace_dir}/model_step_5',\n",
    "    'Window Pruned': f'{workspace_dir}/window_pruned_model',\n",
    "    'Window Pruned + Finetuned': f'{workspace_dir}/window_pruned_finetuned'\n",
    "}\n",
    "\n",
    "comparison_data = []\n",
    "for name, path in models_to_compare.items():\n",
    "    if Path(path).exists():\n",
    "        metrics = calculate_quality_metrics(path)\n",
    "        if metrics:\n",
    "            metrics['model_name'] = name\n",
    "            comparison_data.append(metrics)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"Model Comparison:\")\n",
    "print(df_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Curves Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training logs from tensorboard logs if available\n",
    "import glob\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "def load_tensorboard_data(log_dir):\n",
    "    \"\"\"Load data from tensorboard logs\"\"\"\n",
    "    event_files = glob.glob(f\"{log_dir}/events.out.tfevents.*\")\n",
    "    if not event_files:\n",
    "        return None\n",
    "    \n",
    "    ea = EventAccumulator(log_dir)\n",
    "    ea.Reload()\n",
    "    \n",
    "    # Extract training loss\n",
    "    try:\n",
    "        loss_data = ea.Scalars('train/loss')\n",
    "        steps = [x.step for x in loss_data]\n",
    "        losses = [x.value for x in loss_data]\n",
    "        return {'steps': steps, 'losses': losses}\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Training loss curves\n",
    "plt.subplot(1, 3, 1)\n",
    "log_dirs = glob.glob(f\"{workspace_dir}/logs/*/\")\n",
    "for i, log_dir in enumerate(log_dirs[:3]):  # Show first 3 training runs\n",
    "    data = load_tensorboard_data(log_dir)\n",
    "    if data:\n",
    "        plt.plot(data['steps'], data['losses'], label=f'Training Run {i+1}', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Model size reduction\n",
    "plt.subplot(1, 3, 2)\n",
    "if comparison_data:\n",
    "    names = [d['model_name'] for d in comparison_data]\n",
    "    sizes = [d['model_size_mb'] for d in comparison_data]\n",
    "    plt.bar(names, sizes, alpha=0.7)\n",
    "    plt.ylabel('Model Size (MB)')\n",
    "    plt.title('Model Size Comparison')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "# Parameter count reduction\n",
    "plt.subplot(1, 3, 3)\n",
    "if comparison_data:\n",
    "    names = [d['model_name'] for d in comparison_data]\n",
    "    params = [d['num_parameters'] / 1e6 for d in comparison_data]  # Convert to millions\n",
    "    plt.bar(names, params, alpha=0.7, color='green')\n",
    "    plt.ylabel('Parameters (Millions)')\n",
    "    plt.title('Parameter Count Comparison')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and Time Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze compute costs\n",
    "def estimate_compute_cost(model_metrics, training_steps=500):\n",
    "    \"\"\"Estimate relative compute cost based on model size and training steps\"\"\"\n",
    "    base_cost = model_metrics['num_parameters'] * training_steps\n",
    "    return base_cost / 1e9  # Normalize\n",
    "\n",
    "# Calculate costs for each approach\n",
    "cost_analysis = []\n",
    "for data in comparison_data:\n",
    "    cost = estimate_compute_cost(data)\n",
    "    cost_analysis.append({\n",
    "        'model': data['model_name'],\n",
    "        'compute_cost': cost,\n",
    "        'perplexity': data['perplexity'],\n",
    "        'efficiency': data['perplexity'] / cost  # Lower is better\n",
    "    })\n",
    "\n",
    "df_costs = pd.DataFrame(cost_analysis)\n",
    "\n",
    "# Visualize cost vs performance trade-off\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(df_costs['compute_cost'], df_costs['perplexity'], \n",
    "           s=100, alpha=0.7, c=range(len(df_costs)), cmap='viridis')\n",
    "for i, row in df_costs.iterrows():\n",
    "    plt.annotate(row['model'], (row['compute_cost'], row['perplexity']), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "plt.xlabel('Relative Compute Cost')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.title('Compute Cost vs Performance Trade-off')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(df_costs['model'], df_costs['efficiency'], alpha=0.7, color='orange')\n",
    "plt.ylabel('Efficiency (Perplexity/Cost)')\n",
    "plt.title('Model Efficiency Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCost Analysis:\")\n",
    "print(df_costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"=== PRUNING AND HEALING EXPERIMENT SUMMARY ===\")\n",
    "print()\n",
    "\n",
    "print(\"1. ITERATIVE PRUNING APPROACH:\")\n",
    "if iterative_logs:\n",
    "    final_step = max([log.get('step', 0) for log in iterative_logs])\n",
    "    final_ppl = [log.get('post_train_perplexity') for log in iterative_logs if log.get('step') == final_step]\n",
    "    if final_ppl:\n",
    "        print(f\"   - Final perplexity: {final_ppl[0]:.3f}\")\n",
    "    print(f\"   - Total pruning steps: {final_step}\")\n",
    "    print(f\"   - Layers removed: {final_step}\")\n",
    "\n",
    "print()\n",
    "print(\"2. WINDOW PRUNING APPROACH:\")\n",
    "print(f\"   - Baseline perplexity: {window_results.get('baseline_perplexity', 'N/A'):.3f}\")\n",
    "print(f\"   - Best window to remove: {window_results.get('best_window', 'N/A')}\")\n",
    "print(f\"   - Window size: {window_results.get('window_size', 'N/A')}\")\n",
    "\n",
    "print()\n",
    "print(\"3. MODEL COMPARISON:\")\n",
    "if comparison_data:\n",
    "    for data in comparison_data:\n",
    "        print(f\"   {data['model_name']}:\")\n",
    "        print(f\"     - Perplexity: {data['perplexity']:.3f}\")\n",
    "        print(f\"     - Parameters: {data['num_parameters']/1e6:.1f}M\")\n",
    "        print(f\"     - Size: {data['model_size_mb']:.1f} MB\")\n",
    "        print(f\"     - Layers: {data['num_layers']}\")\n",
    "\n",
    "print()\n",
    "print(\"4. RECOMMENDATIONS:\")\n",
    "if cost_analysis:\n",
    "    best_efficiency = min(cost_analysis, key=lambda x: x['efficiency'])\n",
    "    print(f\"   - Most efficient approach: {best_efficiency['model']}\")\n",
    "    print(f\"   - Best efficiency score: {best_efficiency['efficiency']:.3f}\")\n",
    "\n",
    "print(\"\\n=== END SUMMARY ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}