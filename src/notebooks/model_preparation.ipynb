{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation Notebook\n",
    "\n",
    "This notebook downloads, saves, and loads pre-trained models for use with the pruninghealing library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU device 3: NVIDIA A100-PCIE-40GB\n",
      "Checkpoints directory: /home/ThunderstormXX/Ridiculous-LLM-Compression/src/notebooks/../checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Device selection\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "i = 3  # device number to use (change this to select GPU: 0, 1, 2, etc.)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f'{i}'\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Check device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f'Using GPU device {i}: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using CPU')\n",
    "\n",
    "# Set up paths\n",
    "CHECKPOINTS_DIR = Path(\"../checkpoints\")\n",
    "CHECKPOINTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoints directory: {CHECKPOINTS_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Models\n",
    "\n",
    "Select from the following pre-trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "  llama3.1-8b: unsloth/Llama-3.1-8B-Instruct\n",
      "  llama2-13b: meta-llama/Llama-2-13b-hf\n",
      "  mistral-7b: mistralai/Mistral-7B-v0.1\n",
      "  phi2: microsoft/phi-2\n",
      "  qwen-7b: Qwen/Qwen-7B\n",
      "  tinyllama: unsloth/Llama-3.2-1B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# huggingface-cli download unsloth/Llama-3.1-8B-Instruct --local-dir ./Llama-3.1-8B-Instruct --local-dir-use-symlinks False\n",
    "\n",
    "# Available models configuration\n",
    "AVAILABLE_MODELS = {\n",
    "    \"llama3.1-8b\": \"unsloth/Llama-3.1-8B-Instruct\",\n",
    "    \"llama2-13b\": \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"mistral-7b\": \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"phi2\": \"microsoft/phi-2\",\n",
    "    \"qwen-7b\": \"Qwen/Qwen-7B\",\n",
    "    \"tinyllama\": \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "}\n",
    "\n",
    "print(\"Available models:\")\n",
    "for key, value in AVAILABLE_MODELS.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Save Model\n",
    "\n",
    "Choose a model to download and save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_save_model(model_key, force_download=False):\n",
    "    \"\"\"Download and save model and tokenizer\"\"\"\n",
    "    \n",
    "    if model_key not in AVAILABLE_MODELS:\n",
    "        print(f\"Error: {model_key} not in available models\")\n",
    "        return None, None\n",
    "    \n",
    "    model_name = AVAILABLE_MODELS[model_key]\n",
    "    save_path = CHECKPOINTS_DIR / model_key\n",
    "    \n",
    "    # Check if already exists\n",
    "    if save_path.exists() and not force_download:\n",
    "        print(f\"Model {model_key} already exists at {save_path}\")\n",
    "        print(\"Set force_download=True to re-download\")\n",
    "        return str(save_path), str(save_path)\n",
    "    \n",
    "    print(f\"Downloading {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        print(\"Loading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Load model\n",
    "        print(\"Loading model...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=device if device != 'cpu' else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Save to checkpoints\n",
    "        save_path.mkdir(exist_ok=True)\n",
    "        print(f\"Saving to {save_path}...\")\n",
    "        \n",
    "        model.save_pretrained(save_path)\n",
    "        tokenizer.save_pretrained(save_path)\n",
    "        \n",
    "        print(f\"✓ Successfully saved {model_key} to {save_path}\")\n",
    "        return str(save_path), str(save_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {model_key}: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "def load_saved_model(model_key):\n",
    "    \"\"\"Load model and tokenizer from checkpoints directory\"\"\"\n",
    "    \n",
    "    model_path = CHECKPOINTS_DIR / model_key\n",
    "    \n",
    "    if not model_path.exists():\n",
    "        print(f\"Error: Model {model_key} not found in {CHECKPOINTS_DIR}\")\n",
    "        print(\"Available models:\")\n",
    "        for p in CHECKPOINTS_DIR.iterdir():\n",
    "            if p.is_dir():\n",
    "                print(f\"  - {p.name}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading {model_key} from {model_path}...\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        \n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=device if device != 'cpu' else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Successfully loaded {model_key}\")\n",
    "        print(f\"Model type: {model.config.model_type}\")\n",
    "        print(f\"Number of layers: {len(model.model.layers)}\")\n",
    "        print(f\"Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_key}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model llama3.1-8b already exists at ../checkpoints/llama3.1-8b\n",
      "Set force_download=True to re-download\n",
      "Model saved to: ../checkpoints/llama3.1-8b\n"
     ]
    }
   ],
   "source": [
    "# Example usage - change model_key to desired model\n",
    "MODEL_TO_DOWNLOAD = \"llama3.1-8b\"#\"tinyllama\" #  # Change this to your desired model\n",
    "\n",
    "model_path, tokenizer_path = download_and_save_model(MODEL_TO_DOWNLOAD)\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Model\n",
    "\n",
    "Load a previously saved model from checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama3.1-8b from ../checkpoints/llama3.1-8b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e36d240ba3847168ab7f2eacf374e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully loaded llama3.1-8b\n",
      "Model type: llama\n",
      "Number of layers: 32\n",
      "Parameters: 8030.3M\n",
      "Loading tinyllama from ../checkpoints/tinyllama...\n",
      "✓ Successfully loaded tinyllama\n",
      "Model type: llama\n",
      "Number of layers: 16\n",
      "Parameters: 1235.8M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "MODELS_TO_LOAD = [\"llama3.1-8b\",\"tinyllama\"] #  # Change this to your desired model\n",
    "\n",
    "loaded_models = [load_saved_model(MODEL_TO_LOAD) for MODEL_TO_LOAD in MODELS_TO_LOAD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for model, _ in loaded_models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Inference\n",
    "\n",
    "Quick test to verify the loaded model works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.llama.modeling_llama.LlamaForCausalLM"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- llama3.1-8b\n",
      "Testing with prompt: 'Hello, how are you?'\n",
      "Response: Hello, how are you? It seems like a simple greeting, but the meanings and interpretations of these two questions are complex and can influence our daily lives. In this article, we’ll explore the nuances of these simple questions and how they can affect our well-being, relationships, and\n",
      "-------------------------------- tinyllama\n",
      "Testing with prompt: 'Hello, how are you?'\n",
      "Response: Hello, how are you? I'm just a simple AI assistant, I don't have feelings or emotions like humans do, but I'm here to help you with any questions or tasks you may have.\n",
      "\n",
      "You could also like to start by telling me a bit about yourself. What\n"
     ]
    }
   ],
   "source": [
    "def test_model_inference(model, tokenizer, prompt=\"Hello, how are you?\"):\n",
    "    \"\"\"Test model with simple inference\"\"\"\n",
    "    \n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Model or tokenizer not loaded\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Testing with prompt: '{prompt}'\")\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Response: {response}\")\n",
    "\n",
    "# Test the loaded model\n",
    "for model_tokenizer, name in zip(loaded_models,MODELS_TO_LOAD):\n",
    "    model, tokenizer = model_tokenizer\n",
    "    print('--------------------------------', name)\n",
    "    if model is not None:\n",
    "        test_model_inference(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Available Models\n",
    "\n",
    "Check what models are currently saved in checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models in ../checkpoints:\n",
      "  - tinyllama (2373.6 MB)\n",
      "  - llama3.1-8b (15333.0 MB)\n",
      "  - tinyllama_p_window (2048.1 MB)\n"
     ]
    }
   ],
   "source": [
    "def list_saved_models():\n",
    "    \"\"\"List all models in checkpoints directory\"\"\"\n",
    "    \n",
    "    print(f\"Models in {CHECKPOINTS_DIR}:\")\n",
    "    \n",
    "    saved_models = []\n",
    "    for path in CHECKPOINTS_DIR.iterdir():\n",
    "        if path.is_dir():\n",
    "            # Check if it contains model files\n",
    "            if (path / \"config.json\").exists():\n",
    "                size_mb = sum(f.stat().st_size for f in path.rglob('*') if f.is_file()) / (1024**2)\n",
    "                saved_models.append((path.name, size_mb))\n",
    "    \n",
    "    if saved_models:\n",
    "        for name, size in saved_models:\n",
    "            print(f\"  - {name} ({size:.1f} MB)\")\n",
    "    else:\n",
    "        print(\"  No models found\")\n",
    "    \n",
    "    return [name for name, _ in saved_models]\n",
    "\n",
    "saved_models = list_saved_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate perplexiry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # добавить родительскую папку\n",
    "\n",
    "from pruninghealing.utils import calculate_perplexity\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Загружаем закэшированный датасет\n",
    "dataset_path = \"../../cached_dataset\"\n",
    "dataset = load_from_disk(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- llama3.1-8b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perplexity:   0%|          | 0/1000 [00:00<?, ?sample/s, avg_loss=2.0562, ppl=7.82]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perplexity: 100%|██████████| 1000/1000 [01:02<00:00, 16.13sample/s, avg_loss=2.5120, ppl=12.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 12.329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perplexity: 100%|██████████| 1000/1000 [00:59<00:00, 16.67sample/s, avg_loss=2.5120, norm_loss=0.2136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Perplexity: 0.214\n",
      "-------------------------------- tinyllama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perplexity: 100%|██████████| 1000/1000 [00:25<00:00, 38.48sample/s, avg_loss=3.1310, ppl=22.90]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 22.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perplexity: 100%|██████████| 1000/1000 [00:26<00:00, 37.08sample/s, avg_loss=3.1310, norm_loss=0.2662]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Perplexity: 0.266\n"
     ]
    }
   ],
   "source": [
    "for model_tokenizer, name in zip(loaded_models,MODELS_TO_LOAD):\n",
    "    model, tokenizer = model_tokenizer\n",
    "    print('--------------------------------', name)\n",
    "\n",
    "    # Обычная перплексия\n",
    "    ppl = calculate_perplexity(model, tokenizer, dataset['validation'], normalized = False)\n",
    "    print(f\"Perplexity: {ppl:.3f}\")\n",
    "\n",
    "    # Нормализованная перплексия\n",
    "    norm_ppl = calculate_perplexity(model, tokenizer,dataset['validation'])\n",
    "    print(f\"Normalized Perplexity: {norm_ppl:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ridiculous-LLM-Compression (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
