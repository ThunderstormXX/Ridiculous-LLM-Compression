{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation Notebook\n",
    "\n",
    "This notebook downloads, saves, and loads pre-trained models for use with the pruninghealing library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints directory: /home/ThunderstormXX/Ridiculous-LLM-Compression/src/notebooks/../checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Set up paths\n",
    "CHECKPOINTS_DIR = Path(\"../checkpoints\")\n",
    "CHECKPOINTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoints directory: {CHECKPOINTS_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Models\n",
    "\n",
    "Select from the following pre-trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "  llama2-7b: meta-llama/Llama-2-7b-hf\n",
      "  llama2-13b: meta-llama/Llama-2-13b-hf\n",
      "  mistral-7b: mistralai/Mistral-7B-v0.1\n",
      "  phi2: microsoft/phi-2\n",
      "  qwen-7b: Qwen/Qwen-7B\n",
      "  tinyllama: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
     ]
    }
   ],
   "source": [
    "# Available models configuration\n",
    "AVAILABLE_MODELS = {\n",
    "    \"llama2-7b\": \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"llama2-13b\": \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"mistral-7b\": \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"phi2\": \"microsoft/phi-2\",\n",
    "    \"qwen-7b\": \"Qwen/Qwen-7B\",\n",
    "    \"tinyllama\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "}\n",
    "\n",
    "print(\"Available models:\")\n",
    "for key, value in AVAILABLE_MODELS.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Save Model\n",
    "\n",
    "Choose a model to download and save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_save_model(model_key, force_download=False):\n",
    "    \"\"\"Download and save model and tokenizer\"\"\"\n",
    "    \n",
    "    if model_key not in AVAILABLE_MODELS:\n",
    "        print(f\"Error: {model_key} not in available models\")\n",
    "        return None, None\n",
    "    \n",
    "    model_name = AVAILABLE_MODELS[model_key]\n",
    "    save_path = CHECKPOINTS_DIR / model_key\n",
    "    \n",
    "    # Check if already exists\n",
    "    if save_path.exists() and not force_download:\n",
    "        print(f\"Model {model_key} already exists at {save_path}\")\n",
    "        print(\"Set force_download=True to re-download\")\n",
    "        return str(save_path), str(save_path)\n",
    "    \n",
    "    print(f\"Downloading {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        print(\"Loading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Load model\n",
    "        print(\"Loading model...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Save to checkpoints\n",
    "        save_path.mkdir(exist_ok=True)\n",
    "        print(f\"Saving to {save_path}...\")\n",
    "        \n",
    "        model.save_pretrained(save_path)\n",
    "        tokenizer.save_pretrained(save_path)\n",
    "        \n",
    "        print(f\"✓ Successfully saved {model_key} to {save_path}\")\n",
    "        return str(save_path), str(save_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {model_key}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage - change model_key to desired model\n",
    "MODEL_TO_DOWNLOAD = \"tinyllama\"  # Change this to your desired model\n",
    "\n",
    "model_path, tokenizer_path = download_and_save_model(MODEL_TO_DOWNLOAD)\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Model\n",
    "\n",
    "Load a previously saved model from checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model(model_key):\n",
    "    \"\"\"Load model and tokenizer from checkpoints directory\"\"\"\n",
    "    \n",
    "    model_path = CHECKPOINTS_DIR / model_key\n",
    "    \n",
    "    if not model_path.exists():\n",
    "        print(f\"Error: Model {model_key} not found in {CHECKPOINTS_DIR}\")\n",
    "        print(\"Available models:\")\n",
    "        for p in CHECKPOINTS_DIR.iterdir():\n",
    "            if p.is_dir():\n",
    "                print(f\"  - {p.name}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading {model_key} from {model_path}...\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        \n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Successfully loaded {model_key}\")\n",
    "        print(f\"Model type: {model.config.model_type}\")\n",
    "        print(f\"Number of layers: {len(model.model.layers)}\")\n",
    "        print(f\"Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_key}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage\n",
    "MODEL_TO_LOAD = \"tinyllama\"  # Change this to your desired model\n",
    "\n",
    "model, tokenizer = load_saved_model(MODEL_TO_LOAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Inference\n",
    "\n",
    "Quick test to verify the loaded model works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_inference(model, tokenizer, prompt=\"Hello, how are you?\"):\n",
    "    \"\"\"Test model with simple inference\"\"\"\n",
    "    \n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Model or tokenizer not loaded\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Testing with prompt: '{prompt}'\")\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Response: {response}\")\n",
    "\n",
    "# Test the loaded model\n",
    "if model is not None:\n",
    "    test_model_inference(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Available Models\n",
    "\n",
    "Check what models are currently saved in checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_saved_models():\n",
    "    \"\"\"List all models in checkpoints directory\"\"\"\n",
    "    \n",
    "    print(f\"Models in {CHECKPOINTS_DIR}:\")\n",
    "    \n",
    "    saved_models = []\n",
    "    for path in CHECKPOINTS_DIR.iterdir():\n",
    "        if path.is_dir():\n",
    "            # Check if it contains model files\n",
    "            if (path / \"config.json\").exists():\n",
    "                size_mb = sum(f.stat().st_size for f in path.rglob('*') if f.is_file()) / (1024**2)\n",
    "                saved_models.append((path.name, size_mb))\n",
    "    \n",
    "    if saved_models:\n",
    "        for name, size in saved_models:\n",
    "            print(f\"  - {name} ({size:.1f} MB)\")\n",
    "    else:\n",
    "        print(\"  No models found\")\n",
    "    \n",
    "    return [name for name, _ in saved_models]\n",
    "\n",
    "saved_models = list_saved_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage with PruningHealing Library\n",
    "\n",
    "Example of how to use saved models with the pruninghealing library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example integration with pruninghealing library\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.pruninghealing import Trainer, DatasetLoader, IterativePruner\n",
    "from src.pruninghealing.utils import calculate_perplexity\n",
    "\n",
    "def prepare_model_for_pruning(model_key):\n",
    "    \"\"\"Load model and prepare for pruning experiments\"\"\"\n",
    "    \n",
    "    # Load model from checkpoints\n",
    "    model, tokenizer = load_saved_model(model_key)\n",
    "    \n",
    "    if model is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Calculate baseline perplexity\n",
    "    print(\"Calculating baseline perplexity...\")\n",
    "    baseline_ppl = calculate_perplexity(model, tokenizer)\n",
    "    print(f\"Baseline perplexity: {baseline_ppl:.3f}\")\n",
    "    \n",
    "    # Create dataset loader\n",
    "    dataset_loader = DatasetLoader(tokenizer)\n",
    "    dataset_loader.load_wikitext()\n",
    "    \n",
    "    # Create pruner\n",
    "    pruner = IterativePruner(model, tokenizer, workspace_dir=f\"../../workspace/{model_key}\")\n",
    "    \n",
    "    print(f\"✓ Model {model_key} ready for pruning experiments\")\n",
    "    return model, tokenizer, pruner\n",
    "\n",
    "# Example usage\n",
    "if saved_models:\n",
    "    example_model = saved_models[0]\n",
    "    print(f\"Preparing {example_model} for pruning...\")\n",
    "    model, tokenizer, pruner = prepare_model_for_pruning(example_model)\n",
    "else:\n",
    "    print(\"No saved models available. Download a model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Download Multiple Models\n",
    "\n",
    "Download multiple models at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_download_models(model_keys, force_download=False):\n",
    "    \"\"\"Download multiple models\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_key in model_keys:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing {model_key}...\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        model_path, tokenizer_path = download_and_save_model(model_key, force_download)\n",
    "        results[model_key] = {\n",
    "            'success': model_path is not None,\n",
    "            'path': model_path\n",
    "        }\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"BATCH DOWNLOAD SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for model_key, result in results.items():\n",
    "        status = \"✓\" if result['success'] else \"✗\"\n",
    "        print(f\"{status} {model_key}: {result['path'] if result['success'] else 'Failed'}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Download small models for testing\n",
    "# Uncomment to download multiple models\n",
    "# models_to_download = [\"tinyllama\", \"phi2\"]\n",
    "# batch_results = batch_download_models(models_to_download)\n",
    "\n",
    "print(\"Batch download function ready. Uncomment above lines to use.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ridiculous-LLM-Compression (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
