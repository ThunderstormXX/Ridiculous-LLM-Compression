{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation Notebook\n",
    "\n",
    "This notebook downloads, saves, and loads pre-trained models for use with the pruninghealing library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints directory: /home/ThunderstormXX/Ridiculous-LLM-Compression/src/notebooks/../checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Device selection\n",
    "DEVICE_ID = 0  # Change this to select GPU device (0, 1, 2, etc.) or -1 for CPU\n",
    "\n",
    "if DEVICE_ID >= 0 and torch.cuda.is_available():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(DEVICE_ID)\n",
    "    device = f'cuda:{DEVICE_ID}'\n",
    "    print(f'Using GPU device {DEVICE_ID}: {torch.cuda.get_device_name(DEVICE_ID)}')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using CPU')\n",
    "\n",
    "# Set up paths\n",
    "CHECKPOINTS_DIR = Path(\"../checkpoints\")\n",
    "CHECKPOINTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoints directory: {CHECKPOINTS_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Models\n",
    "\n",
    "Select from the following pre-trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "  llama3.1-8b: unsloth/Llama-3.1-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# huggingface-cli download unsloth/Llama-3.1-8B-Instruct --local-dir ./Llama-3.1-8B-Instruct --local-dir-use-symlinks False\n",
    "\n",
    "# Available models configuration\n",
    "AVAILABLE_MODELS = {\n",
    "    \"llama3.1-8b\": \"unsloth/Llama-3.1-8B-Instruct\",\n",
    "    # \"llama2-13b\": \"meta-llama/Llama-2-13b-hf\",\n",
    "    # \"mistral-7b\": \"mistralai/Mistral-7B-v0.1\",\n",
    "    # \"phi2\": \"microsoft/phi-2\",\n",
    "    # \"qwen-7b\": \"Qwen/Qwen-7B\",\n",
    "    # \"tinyllama\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "}\n",
    "\n",
    "print(\"Available models:\")\n",
    "for key, value in AVAILABLE_MODELS.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Save Model\n",
    "\n",
    "Choose a model to download and save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading unsloth/Llama-3.1-8B-Instruct...\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc34460bc0f04b09a833d35c901e2f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584bb24ccdb44fefbfef27e514fb0426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7907a53683b4cdf9830cf400ccee4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1a03c57fb94f4e9c3ce577699ea33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a28d6ad9e904dd6849f01a8e826bfd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/896 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0177f336e94940bada7b735718555b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8bb7812b294aeca5782a6ba7654c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae28d022e16b4a5d9765a5f63e390a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0274bb8a924282839bd9848d68d377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd65ecafaf6143a093aa780732a4df25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954c2a5421414fe6b810dc315085dbd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97dddafc68694b25917a8841baa51156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to ../checkpoints/llama3.1-8b...\n",
      "✓ Successfully saved llama3.1-8b to ../checkpoints/llama3.1-8b\n",
      "Model saved to: ../checkpoints/llama3.1-8b\n"
     ]
    }
   ],
   "source": [
    "def download_and_save_model(model_key, force_download=False):\n",
    "    \"\"\"Download and save model and tokenizer\"\"\"\n",
    "    \n",
    "    if model_key not in AVAILABLE_MODELS:\n",
    "        print(f\"Error: {model_key} not in available models\")\n",
    "        return None, None\n",
    "    \n",
    "    model_name = AVAILABLE_MODELS[model_key]\n",
    "    save_path = CHECKPOINTS_DIR / model_key\n",
    "    \n",
    "    # Check if already exists\n",
    "    if save_path.exists() and not force_download:\n",
    "        print(f\"Model {model_key} already exists at {save_path}\")\n",
    "        print(\"Set force_download=True to re-download\")\n",
    "        return str(save_path), str(save_path)\n",
    "    \n",
    "    print(f\"Downloading {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        print(\"Loading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Load model\n",
    "        print(\"Loading model...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=device if device != 'cpu' else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Save to checkpoints\n",
    "        save_path.mkdir(exist_ok=True)\n",
    "        print(f\"Saving to {save_path}...\")\n",
    "        \n",
    "        model.save_pretrained(save_path)\n",
    "        tokenizer.save_pretrained(save_path)\n",
    "        \n",
    "        print(f\"✓ Successfully saved {model_key} to {save_path}\")\n",
    "        return str(save_path), str(save_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {model_key}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage - change model_key to desired model\n",
    "MODEL_TO_DOWNLOAD = \"llama3.1-8b\"  # Change this to your desired model\n",
    "\n",
    "model_path, tokenizer_path = download_and_save_model(MODEL_TO_DOWNLOAD)\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Model\n",
    "\n",
    "Load a previously saved model from checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model(model_key):\n",
    "    \"\"\"Load model and tokenizer from checkpoints directory\"\"\"\n",
    "    \n",
    "    model_path = CHECKPOINTS_DIR / model_key\n",
    "    \n",
    "    if not model_path.exists():\n",
    "        print(f\"Error: Model {model_key} not found in {CHECKPOINTS_DIR}\")\n",
    "        print(\"Available models:\")\n",
    "        for p in CHECKPOINTS_DIR.iterdir():\n",
    "            if p.is_dir():\n",
    "                print(f\"  - {p.name}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading {model_key} from {model_path}...\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        \n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=device if device != 'cpu' else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Successfully loaded {model_key}\")\n",
    "        print(f\"Model type: {model.config.model_type}\")\n",
    "        print(f\"Number of layers: {len(model.model.layers)}\")\n",
    "        print(f\"Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_key}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage\n",
    "MODEL_TO_LOAD = \"tinyllama\"  # Change this to your desired model\n",
    "\n",
    "model, tokenizer = load_saved_model(MODEL_TO_LOAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Inference\n",
    "\n",
    "Quick test to verify the loaded model works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_inference(model, tokenizer, prompt=\"Hello, how are you?\"):\n",
    "    \"\"\"Test model with simple inference\"\"\"\n",
    "    \n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Model or tokenizer not loaded\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Testing with prompt: '{prompt}'\")\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Response: {response}\")\n",
    "\n",
    "# Test the loaded model\n",
    "if model is not None:\n",
    "    test_model_inference(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Available Models\n",
    "\n",
    "Check what models are currently saved in checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_saved_models():\n",
    "    \"\"\"List all models in checkpoints directory\"\"\"\n",
    "    \n",
    "    print(f\"Models in {CHECKPOINTS_DIR}:\")\n",
    "    \n",
    "    saved_models = []\n",
    "    for path in CHECKPOINTS_DIR.iterdir():\n",
    "        if path.is_dir():\n",
    "            # Check if it contains model files\n",
    "            if (path / \"config.json\").exists():\n",
    "                size_mb = sum(f.stat().st_size for f in path.rglob('*') if f.is_file()) / (1024**2)\n",
    "                saved_models.append((path.name, size_mb))\n",
    "    \n",
    "    if saved_models:\n",
    "        for name, size in saved_models:\n",
    "            print(f\"  - {name} ({size:.1f} MB)\")\n",
    "    else:\n",
    "        print(\"  No models found\")\n",
    "    \n",
    "    return [name for name, _ in saved_models]\n",
    "\n",
    "saved_models = list_saved_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage with PruningHealing Library\n",
    "\n",
    "Example of how to use saved models with the pruninghealing library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example integration with pruninghealing library\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.pruninghealing import Trainer, DatasetLoader, IterativePruner\n",
    "from src.pruninghealing.utils import calculate_perplexity\n",
    "\n",
    "def prepare_model_for_pruning(model_key):\n",
    "    \"\"\"Load model and prepare for pruning experiments\"\"\"\n",
    "    \n",
    "    # Load model from checkpoints\n",
    "    model, tokenizer = load_saved_model(model_key)\n",
    "    \n",
    "    if model is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Calculate baseline perplexity\n",
    "    print(\"Calculating baseline perplexity...\")\n",
    "    baseline_ppl = calculate_perplexity(model, tokenizer)\n",
    "    print(f\"Baseline perplexity: {baseline_ppl:.3f}\")\n",
    "    \n",
    "    # Create dataset loader\n",
    "    dataset_loader = DatasetLoader(tokenizer)\n",
    "    dataset_loader.load_wikitext()\n",
    "    \n",
    "    # Create pruner\n",
    "    pruner = IterativePruner(model, tokenizer, workspace_dir=f\"../../workspace/{model_key}\")\n",
    "    \n",
    "    print(f\"✓ Model {model_key} ready for pruning experiments\")\n",
    "    return model, tokenizer, pruner\n",
    "\n",
    "# Example usage\n",
    "if saved_models:\n",
    "    example_model = saved_models[0]\n",
    "    print(f\"Preparing {example_model} for pruning...\")\n",
    "    model, tokenizer, pruner = prepare_model_for_pruning(example_model)\n",
    "else:\n",
    "    print(\"No saved models available. Download a model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Download Multiple Models\n",
    "\n",
    "Download multiple models at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_download_models(model_keys, force_download=False):\n",
    "    \"\"\"Download multiple models\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_key in model_keys:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing {model_key}...\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        model_path, tokenizer_path = download_and_save_model(model_key, force_download)\n",
    "        results[model_key] = {\n",
    "            'success': model_path is not None,\n",
    "            'path': model_path\n",
    "        }\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"BATCH DOWNLOAD SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for model_key, result in results.items():\n",
    "        status = \"✓\" if result['success'] else \"✗\"\n",
    "        print(f\"{status} {model_key}: {result['path'] if result['success'] else 'Failed'}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Download small models for testing\n",
    "# Uncomment to download multiple models\n",
    "# models_to_download = [\"tinyllama\", \"phi2\"]\n",
    "# batch_results = batch_download_models(models_to_download)\n",
    "\n",
    "print(\"Batch download function ready. Uncomment above lines to use.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ridiculous-LLM-Compression (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
