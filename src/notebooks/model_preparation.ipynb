{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation Notebook\n",
    "\n",
    "This notebook downloads, saves, and loads pre-trained models for use with the pruninghealing library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU device 3: NVIDIA A100-PCIE-40GB\n",
      "Checkpoints directory: /home/ThunderstormXX/Ridiculous-LLM-Compression/src/notebooks/../checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Device selection\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "i = 3  # device number to use (change this to select GPU: 0, 1, 2, etc.)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f'{i}'\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Check device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f'Using GPU device {i}: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using CPU')\n",
    "\n",
    "# Set up paths\n",
    "CHECKPOINTS_DIR = Path(\"../checkpoints\")\n",
    "CHECKPOINTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoints directory: {CHECKPOINTS_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Models\n",
    "\n",
    "Select from the following pre-trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "  llama3.1-8b: unsloth/Llama-3.1-8B-Instruct\n",
      "  llama2-13b: meta-llama/Llama-2-13b-hf\n",
      "  mistral-7b: mistralai/Mistral-7B-v0.1\n",
      "  phi2: microsoft/phi-2\n",
      "  qwen-7b: Qwen/Qwen-7B\n",
      "  tinyllama: unsloth/Llama-3.2-1B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# huggingface-cli download unsloth/Llama-3.1-8B-Instruct --local-dir ./Llama-3.1-8B-Instruct --local-dir-use-symlinks False\n",
    "\n",
    "# Available models configuration\n",
    "AVAILABLE_MODELS = {\n",
    "    \"llama3.1-8b\": \"unsloth/Llama-3.1-8B-Instruct\",\n",
    "    \"llama2-13b\": \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"mistral-7b\": \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"phi2\": \"microsoft/phi-2\",\n",
    "    \"qwen-7b\": \"Qwen/Qwen-7B\",\n",
    "    \"tinyllama\": \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "}\n",
    "\n",
    "print(\"Available models:\")\n",
    "for key, value in AVAILABLE_MODELS.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Save Model\n",
    "\n",
    "Choose a model to download and save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading unsloth/Llama-3.2-1B-Instruct...\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5c74b695be4c7fac2810e3bbe34080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4dbfcdef0844e5bb63dc139dcd3e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b19591104d4c4b85e144662d2b4ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abe5880158443d4af37c8c7f063d6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85752321ae448169f32a0741bfc3bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/894 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75a9f8e8edf4fe182e748b659b119f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d298440f8e984da5b5284f1bd045f858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to ../checkpoints/tinyllama...\n",
      "✓ Successfully saved tinyllama to ../checkpoints/tinyllama\n",
      "Model saved to: ../checkpoints/tinyllama\n"
     ]
    }
   ],
   "source": [
    "def download_and_save_model(model_key, force_download=False):\n",
    "    \"\"\"Download and save model and tokenizer\"\"\"\n",
    "    \n",
    "    if model_key not in AVAILABLE_MODELS:\n",
    "        print(f\"Error: {model_key} not in available models\")\n",
    "        return None, None\n",
    "    \n",
    "    model_name = AVAILABLE_MODELS[model_key]\n",
    "    save_path = CHECKPOINTS_DIR / model_key\n",
    "    \n",
    "    # Check if already exists\n",
    "    if save_path.exists() and not force_download:\n",
    "        print(f\"Model {model_key} already exists at {save_path}\")\n",
    "        print(\"Set force_download=True to re-download\")\n",
    "        return str(save_path), str(save_path)\n",
    "    \n",
    "    print(f\"Downloading {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        print(\"Loading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Load model\n",
    "        print(\"Loading model...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=device if device != 'cpu' else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Save to checkpoints\n",
    "        save_path.mkdir(exist_ok=True)\n",
    "        print(f\"Saving to {save_path}...\")\n",
    "        \n",
    "        model.save_pretrained(save_path)\n",
    "        tokenizer.save_pretrained(save_path)\n",
    "        \n",
    "        print(f\"✓ Successfully saved {model_key} to {save_path}\")\n",
    "        return str(save_path), str(save_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {model_key}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage - change model_key to desired model\n",
    "MODEL_TO_DOWNLOAD = \"tinyllama\" #\"llama3.1-8b\"  # Change this to your desired model\n",
    "\n",
    "model_path, tokenizer_path = download_and_save_model(MODEL_TO_DOWNLOAD)\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Model\n",
    "\n",
    "Load a previously saved model from checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tinyllama from ../checkpoints/tinyllama...\n",
      "✓ Successfully loaded tinyllama\n",
      "Model type: llama\n",
      "Number of layers: 16\n",
      "Parameters: 1235.8M\n"
     ]
    }
   ],
   "source": [
    "def load_saved_model(model_key):\n",
    "    \"\"\"Load model and tokenizer from checkpoints directory\"\"\"\n",
    "    \n",
    "    model_path = CHECKPOINTS_DIR / model_key\n",
    "    \n",
    "    if not model_path.exists():\n",
    "        print(f\"Error: Model {model_key} not found in {CHECKPOINTS_DIR}\")\n",
    "        print(\"Available models:\")\n",
    "        for p in CHECKPOINTS_DIR.iterdir():\n",
    "            if p.is_dir():\n",
    "                print(f\"  - {p.name}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading {model_key} from {model_path}...\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        \n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=device if device != 'cpu' else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Successfully loaded {model_key}\")\n",
    "        print(f\"Model type: {model.config.model_type}\")\n",
    "        print(f\"Number of layers: {len(model.model.layers)}\")\n",
    "        print(f\"Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_key}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage\n",
    "MODEL_TO_LOAD = \"tinyllama\" #\"llama3.1-8b\"  # Change this to your desired model\n",
    "\n",
    "model, tokenizer = load_saved_model(MODEL_TO_LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Inference\n",
    "\n",
    "Quick test to verify the loaded model works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with prompt: 'Hello, how are you?'\n",
      "Response: Hello, how are you? I'm feeling a bit stuck and need some guidance. I've been trying to learn a new skill, but I just can't seem to get past the initial excitement and get any progress. It feels like I'm just going through the motions.\n",
      "\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "def test_model_inference(model, tokenizer, prompt=\"Hello, how are you?\"):\n",
    "    \"\"\"Test model with simple inference\"\"\"\n",
    "    \n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Model or tokenizer not loaded\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Testing with prompt: '{prompt}'\")\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Response: {response}\")\n",
    "\n",
    "# Test the loaded model\n",
    "if model is not None:\n",
    "    test_model_inference(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Available Models\n",
    "\n",
    "Check what models are currently saved in checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models in ../checkpoints:\n",
      "  - tinyllama (2373.6 MB)\n",
      "  - llama3.1-8b (15333.0 MB)\n"
     ]
    }
   ],
   "source": [
    "def list_saved_models():\n",
    "    \"\"\"List all models in checkpoints directory\"\"\"\n",
    "    \n",
    "    print(f\"Models in {CHECKPOINTS_DIR}:\")\n",
    "    \n",
    "    saved_models = []\n",
    "    for path in CHECKPOINTS_DIR.iterdir():\n",
    "        if path.is_dir():\n",
    "            # Check if it contains model files\n",
    "            if (path / \"config.json\").exists():\n",
    "                size_mb = sum(f.stat().st_size for f in path.rglob('*') if f.is_file()) / (1024**2)\n",
    "                saved_models.append((path.name, size_mb))\n",
    "    \n",
    "    if saved_models:\n",
    "        for name, size in saved_models:\n",
    "            print(f\"  - {name} ({size:.1f} MB)\")\n",
    "    else:\n",
    "        print(\"  No models found\")\n",
    "    \n",
    "    return [name for name, _ in saved_models]\n",
    "\n",
    "saved_models = list_saved_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage with PruningHealing Library\n",
    "\n",
    "Example of how to use saved models with the pruninghealing library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing llama3.1-8b for pruning...\n",
      "Loading llama3.1-8b from ../checkpoints/llama3.1-8b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc6f01d360249fdac374bba3ee7ce44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully loaded llama3.1-8b\n",
      "Model type: llama\n",
      "Number of layers: 32\n",
      "Parameters: 8030.3M\n",
      "Calculating baseline perplexity...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16d31c92f6541f981742abdb6736094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b6f3ad0e5a4013891dfe1c4273ca37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea0c10df014413daaea959f3e6da7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540eb4d1a6ee4c13a2c56e3d7b147203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae2bc2a8fab462fabbe12bd5853d702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48ed23e72d54824be0355bdec92413d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffcb6730d034b53a4d6daf9e66accc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline perplexity: 11.105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa131c0e9774844883f80d847722e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cba4c020d4427b9899f89b10d18d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f131bca23b644d1a79cbf439dff6830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e7822fc70d44378ca67dfd0351cdc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0facdd081d44f17b0b9aecc190bb03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf060a5a834d4ed88bf94d1acc2d84c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18ba06b085743d6aaa254d70b2957de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0f8dae3bb84f0bb754960f8566becb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model llama3.1-8b ready for pruning experiments\n"
     ]
    }
   ],
   "source": [
    "# Example integration with pruninghealing library\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.pruninghealing import Trainer, DatasetLoader, IterativePruner\n",
    "from src.pruninghealing.utils import calculate_perplexity\n",
    "\n",
    "def prepare_model_for_pruning(model_key):\n",
    "    \"\"\"Load model and prepare for pruning experiments\"\"\"\n",
    "    \n",
    "    # Load model from checkpoints\n",
    "    model, tokenizer = load_saved_model(model_key)\n",
    "    \n",
    "    if model is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Calculate baseline perplexity\n",
    "    print(\"Calculating baseline perplexity...\")\n",
    "    baseline_ppl = calculate_perplexity(model, tokenizer)\n",
    "    print(f\"Baseline perplexity: {baseline_ppl:.3f}\")\n",
    "    \n",
    "    # Create dataset loader\n",
    "    dataset_loader = DatasetLoader(tokenizer)\n",
    "    dataset_loader.load_wikitext()\n",
    "    \n",
    "    # Create pruner\n",
    "    pruner = IterativePruner(model, tokenizer, workspace_dir=f\"../../workspace/{model_key}\")\n",
    "    \n",
    "    print(f\"✓ Model {model_key} ready for pruning experiments\")\n",
    "    return model, tokenizer, pruner\n",
    "\n",
    "# Example usage\n",
    "if saved_models:\n",
    "    example_model = saved_models[0]\n",
    "    print(f\"Preparing {example_model} for pruning...\")\n",
    "    model, tokenizer, pruner = prepare_model_for_pruning(example_model)\n",
    "else:\n",
    "    print(\"No saved models available. Download a model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Download Multiple Models\n",
    "\n",
    "Download multiple models at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch download function ready. Uncomment above lines to use.\n"
     ]
    }
   ],
   "source": [
    "def batch_download_models(model_keys, force_download=False):\n",
    "    \"\"\"Download multiple models\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_key in model_keys:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing {model_key}...\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        model_path, tokenizer_path = download_and_save_model(model_key, force_download)\n",
    "        results[model_key] = {\n",
    "            'success': model_path is not None,\n",
    "            'path': model_path\n",
    "        }\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"BATCH DOWNLOAD SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for model_key, result in results.items():\n",
    "        status = \"✓\" if result['success'] else \"✗\"\n",
    "        print(f\"{status} {model_key}: {result['path'] if result['success'] else 'Failed'}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Download small models for testing\n",
    "# Uncomment to download multiple models\n",
    "# models_to_download = [\"tinyllama\", \"phi2\"]\n",
    "# batch_results = batch_download_models(models_to_download)\n",
    "\n",
    "print(\"Batch download function ready. Uncomment above lines to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ridiculous-LLM-Compression (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
