{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Pruning Debug Notebook\n",
    "\n",
    "This notebook tests the iterative pruning approach step by step for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU device 2: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Device selection\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "i = 2  # device number to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f'{i}'\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.pruninghealing import Trainer, DatasetLoader, IterativePruner\n",
    "from src.pruninghealing.utils import load_model_and_tokenizer, calculate_perplexity, get_model_layers\n",
    "from src.pruninghealing.logger import Logger\n",
    "\n",
    "print(f'Using GPU device {i}: {torch.cuda.get_device_name(0)}' if torch.cuda.is_available() else 'Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d09cace2614fc2989f83fd1bc31312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_PATH = \"../checkpoints/llama3.1-8b\"  # Change to your model\n",
    "WORKSPACE = \"../../workspace/iterative_debug\"\n",
    "NUM_LAYERS_TO_PRUNE = 3\n",
    "START_LAYER = 17\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"Loading model...\")\n",
    "model, tokenizer = load_model_and_tokenizer(MODEL_PATH)\n",
    "\n",
    "print(f\"Model loaded: {get_model_layers(model)} layers\")\n",
    "print(f\"Model type: {model.config.model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded: 1000 train, 100 eval\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset_loader = DatasetLoader(tokenizer)\n",
    "dataset_loader.load_wikitext(train_size=500, eval_size=50)  # Small for debugging\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset_loader.train_dataset)} train, {len(dataset_loader.eval_dataset)} eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Baseline Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating baseline perplexity...\n",
      "Baseline perplexity: 10.907\n"
     ]
    }
   ],
   "source": [
    "# Calculate baseline perplexity\n",
    "print(\"Calculating baseline perplexity...\")\n",
    "baseline_ppl = calculate_perplexity(model, tokenizer, max_samples=20)\n",
    "print(f\"Baseline perplexity: {baseline_ppl:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize pruner, trainer, and logger\n",
    "pruner = IterativePruner(model, tokenizer, WORKSPACE)\n",
    "trainer = Trainer(model, tokenizer, WORKSPACE)\n",
    "logger = Logger(WORKSPACE)\n",
    "\n",
    "# Log baseline\n",
    "logger.log_step({\n",
    "    \"step\": 0,\n",
    "    \"action\": \"baseline\",\n",
    "    \"layers_remaining\": get_model_layers(model),\n",
    "    \"perplexity\": baseline_ppl\n",
    "})\n",
    "\n",
    "print(\"Components initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Question Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prompt: What is the capital of France?\n",
      "Baseline response: What is the capital of France? Paris\n",
      "What is the capital of France?\n",
      "The capital of France is Paris. Paris is the largest\n"
     ]
    }
   ],
   "source": [
    "# Fixed test question for quality evaluation\n",
    "TEST_PROMPT = \"What is the capital of France?\"\n",
    "\n",
    "def test_model_quality(model, tokenizer, prompt=TEST_PROMPT):\n",
    "    \"\"\"Test model response quality\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=20,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test baseline\n",
    "print(f\"Test prompt: {TEST_PROMPT}\")\n",
    "baseline_response = test_model_quality(model, tokenizer)\n",
    "print(f\"Baseline response: {baseline_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Remove First Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 1: Removing layer 17 ===\n",
      "Layers remaining: 31\n"
     ]
    }
   ],
   "source": [
    "# Initialize current model and step\n",
    "current_model = model\n",
    "step = 0\n",
    "layer_idx = START_LAYER + step\n",
    "\n",
    "print(f\"=== Step {step+1}: Removing layer {layer_idx} ===\")\n",
    "\n",
    "# Remove layer\n",
    "current_model = pruner._remove_layer(current_model, layer_idx)\n",
    "layers_remaining = get_model_layers(current_model)\n",
    "\n",
    "print(f\"Layers remaining: {layers_remaining}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity after pruning: 12.589 (change: +1.682)\n",
      "Response after pruning: What is the capital of France? Paris\n",
      "What is the capital of France?\n",
      "The capital of France is Paris. Paris is a city\n"
     ]
    }
   ],
   "source": [
    "# Test after pruning\n",
    "ppl_after_prune = calculate_perplexity(current_model, tokenizer, max_samples=20)\n",
    "response_after_prune = test_model_quality(current_model, tokenizer)\n",
    "\n",
    "print(f\"Perplexity after pruning: {ppl_after_prune:.3f} (change: {ppl_after_prune - baseline_ppl:+.3f})\")\n",
    "print(f\"Response after pruning: {response_after_prune}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA to layer 17...\n",
      "Perplexity after LoRA: 12.589\n",
      "Response after LoRA: What is the capital of France? Paris\n",
      "What is the capital of France?\n",
      "The capital of France is Paris. Paris is a city\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA\n",
    "print(f\"Applying LoRA to layer {layer_idx}...\")\n",
    "current_model = pruner._apply_lora(current_model, layer_idx)\n",
    "\n",
    "# Test after LoRA\n",
    "ppl_before_train = calculate_perplexity(current_model, tokenizer, max_samples=20)\n",
    "response_before_train = test_model_quality(current_model, tokenizer)\n",
    "\n",
    "print(f\"Perplexity after LoRA: {ppl_before_train:.3f}\")\n",
    "print(f\"Response after LoRA: {response_before_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.peft_model.PeftModelForCausalLM"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(current_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ThunderstormXX/Ridiculous-LLM-Compression/src/pruninghealing/trainer.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `IterationLimitedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:20, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.800822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>10.457600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity after training: 11.443 (improvement: +1.146)\n",
      "Response after training: What is the capital of France? Paris\n",
      "What is the capital of France?\n",
      "The capital of France is Paris. Paris is a city\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"Training model...\")\n",
    "trainer.model = current_model\n",
    "current_model = trainer.train(dataset_loader, max_steps=50)\n",
    "\n",
    "# Test after training\n",
    "ppl_after_train = calculate_perplexity(current_model, tokenizer, max_samples=20)\n",
    "response_after_train = test_model_quality(current_model, tokenizer)\n",
    "\n",
    "print(f\"Perplexity after training: {ppl_after_train:.3f} (improvement: {ppl_before_train - ppl_after_train:+.3f})\")\n",
    "print(f\"Response after training: {response_after_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 completed!\n"
     ]
    }
   ],
   "source": [
    "# Log step 1\n",
    "logger.log_step({\"action\": \"prune\", \"step\": step + 1, \"layer\": layer_idx, \"ppl\": ppl_after_prune})\n",
    "logger.log_step({\"action\": \"train\", \"step\": step + 1, \"layer\": layer_idx, \"ppl\": ppl_after_train})\n",
    "\n",
    "print(f\"Step {step+1} completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Remove Second Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 2: Removing layer 18 ===\n",
      "Layers remaining: 30\n"
     ]
    }
   ],
   "source": [
    "# Step 2\n",
    "step = 1\n",
    "layer_idx = START_LAYER + step\n",
    "\n",
    "print(f\"=== Step {step+1}: Removing layer {layer_idx} ===\")\n",
    "\n",
    "# Remove layer\n",
    "current_model = pruner._remove_layer(current_model, layer_idx)\n",
    "layers_remaining = get_model_layers(current_model)\n",
    "\n",
    "print(f\"Layers remaining: {layers_remaining}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity after pruning: 12.878\n",
      "Response after pruning: What is the capital of France? Paris\n",
      "What is the capital of Australia? Canberra\n",
      "What is the capital of China? Beijing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test after pruning\n",
    "ppl_after_prune = calculate_perplexity(current_model, tokenizer, max_samples=20)\n",
    "response_after_prune = test_model_quality(current_model, tokenizer)\n",
    "\n",
    "print(f\"Perplexity after pruning: {ppl_after_prune:.3f}\")\n",
    "print(f\"Response after pruning: {response_after_prune}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ThunderstormXX/Ridiculous-LLM-Compression/.venv/lib/python3.10/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/ThunderstormXX/Ridiculous-LLM-Compression/.venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/home/ThunderstormXX/Ridiculous-LLM-Compression/src/pruninghealing/trainer.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `IterationLimitedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:19, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.795442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>10.460700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity after training: 11.480\n",
      "Response after training: What is the capital of France? Paris\n",
      "What is the capital of Australia? Canberra\n",
      "What is the capital of China? Beijing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA and train\n",
    "current_model = pruner._apply_lora(current_model, layer_idx)\n",
    "ppl_before_train = calculate_perplexity(current_model, tokenizer, max_samples=20)\n",
    "\n",
    "trainer.model = current_model\n",
    "current_model = trainer.train(dataset_loader, max_steps=50)\n",
    "\n",
    "ppl_after_train = calculate_perplexity(current_model, tokenizer, max_samples=20)\n",
    "response_after_train = test_model_quality(current_model, tokenizer)\n",
    "\n",
    "print(f\"Perplexity after training: {ppl_after_train:.3f}\")\n",
    "print(f\"Response after training: {response_after_train}\")\n",
    "\n",
    "# Log step 2\n",
    "logger.log_step({\"action\": \"prune\", \"step\": step + 1, \"layer\": layer_idx, \"ppl\": ppl_after_prune})\n",
    "logger.log_step({\"action\": \"train\", \"step\": step + 1, \"layer\": layer_idx, \"ppl\": ppl_after_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Remove Third Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 3: Removing layer 19 ===\n",
      "Layers remaining: 29\n"
     ]
    }
   ],
   "source": [
    "# Step 3\n",
    "step = 2\n",
    "layer_idx = START_LAYER + step\n",
    "\n",
    "print(f\"=== Step {step+1}: Removing layer {layer_idx} ===\")\n",
    "\n",
    "# Remove layer\n",
    "current_model = pruner._remove_layer(current_model, layer_idx)\n",
    "layers_remaining = get_model_layers(current_model)\n",
    "\n",
    "print(f\"Layers remaining: {layers_remaining}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ThunderstormXX/Ridiculous-LLM-Compression/.venv/lib/python3.10/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/ThunderstormXX/Ridiculous-LLM-Compression/.venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/home/ThunderstormXX/Ridiculous-LLM-Compression/src/pruninghealing/trainer.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `IterationLimitedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:19, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.790966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>10.461600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 13.532 → 12.162\n",
      "Final response: What is the capital of France? Paris\n",
      "What is the population of France? 8,645,000\n",
      "What is the area\n"
     ]
    }
   ],
   "source": [
    "# Test and train\n",
    "ppl_after_prune = calculate_perplexity(current_model, tokenizer, max_samples=20)\n",
    "response_after_prune = test_model_quality(current_model, tokenizer)\n",
    "\n",
    "current_model = pruner._apply_lora(current_model, layer_idx)\n",
    "trainer.model = current_model\n",
    "current_model = trainer.train(dataset_loader, max_steps=50)\n",
    "\n",
    "ppl_after_train = calculate_perplexity(current_model, tokenizer, max_samples=20)\n",
    "response_after_train = test_model_quality(current_model, tokenizer)\n",
    "\n",
    "print(f\"Perplexity: {ppl_after_prune:.3f} → {ppl_after_train:.3f}\")\n",
    "print(f\"Final response: {response_after_train}\")\n",
    "\n",
    "# Log step 3\n",
    "logger.log_step({\"action\": \"prune\", \"step\": step + 1, \"layer\": layer_idx, \"ppl\": ppl_after_prune})\n",
    "logger.log_step({\"action\": \"train\", \"step\": step + 1, \"layer\": layer_idx, \"ppl\": ppl_after_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mlogger\u001b[49m\u001b[38;5;241m.\u001b[39mlogs)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Filter out baseline and get prune/train data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m prune_data \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprune\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(logger.logs)\n",
    "\n",
    "# Filter out baseline and get prune/train data\n",
    "prune_data = df[df['action'] == 'prune'].reset_index(drop=True)\n",
    "train_data = df[df['action'] == 'train'].reset_index(drop=True)\n",
    "assert len(prune_data) == len(train_data), \"Количество prune и train должно совпадать\"\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot baseline\n",
    "x_vals = [0]\n",
    "y_vals = [baseline_ppl]\n",
    "plt.plot(x_vals, y_vals, 'bo', markersize=8, label=f'Baseline ({baseline_ppl:.3f})')\n",
    "\n",
    "for i in range(len(prune_data)):\n",
    "    # Prune (ухудшение: стрелка вверх)\n",
    "    prune_ppl = prune_data.iloc[i]['ppl']\n",
    "    x_vals.append(i + 0.5)\n",
    "    y_vals.append(prune_ppl)\n",
    "    plt.annotate('', xy=(i + 0.5, prune_ppl), xytext=(i, y_vals[-2]),\n",
    "                 arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "    plt.plot(i + 0.5, prune_ppl, 'ro', markersize=8, label='After Pruning' if i == 0 else '')\n",
    "    plt.text(i + 0.5, prune_ppl + 0.2, f'{prune_ppl:.2f}', ha='center', fontsize=9)\n",
    "\n",
    "    # Train (улучшение: стрелка вниз)\n",
    "    train_ppl = train_data.iloc[i]['ppl']\n",
    "    x_vals.append(i + 1)\n",
    "    y_vals.append(train_ppl)\n",
    "    plt.annotate('', xy=(i + 1, train_ppl), xytext=(i + 0.5, prune_ppl),\n",
    "                 arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "    plt.plot(i + 1, train_ppl, 'go', markersize=8, label='After Training' if i == 0 else '')\n",
    "    plt.text(i + 1, train_ppl - 0.3, f'{train_ppl:.2f}', ha='center', fontsize=9)\n",
    "\n",
    "# Ось X: шаги\n",
    "plt.xticks([0] + [i + 0.5 for i in range(len(prune_data))] + [i + 1 for i in range(len(train_data))],\n",
    "           ['Baseline'] + [f'Prune {i+1}' for i in range(len(prune_data))] + [f'Train {i+1}' for i in range(len(train_data))],\n",
    "           rotation=45)\n",
    "\n",
    "plt.xlabel('Pruning/Training Steps')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.title('Iterative Pruning and Healing Process')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== DETAILED LOGS ===\")\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ridiculous-LLM-Compression (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
