{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation Notebook\n",
    "\n",
    "This notebook downloads, saves, and loads pre-trained models for use with the pruninghealing library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU device 3: NVIDIA A100-PCIE-40GB\n",
      "Checkpoints directory: /home/ThunderstormXX/Ridiculous-LLM-Compression/polina_experiments/results/../checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Device selection\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "i = 3  # device number to use (change this to select GPU: 0, 1, 2, etc.)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f'{i}'\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Check device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f'Using GPU device {i}: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using CPU')\n",
    "\n",
    "# Set up paths\n",
    "CHECKPOINTS_DIR = Path(\"../checkpoints\")\n",
    "CHECKPOINTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoints directory: {CHECKPOINTS_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Models\n",
    "\n",
    "Select from the following pre-trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "  llama3.1-8b: unsloth/Llama-3.1-8B-Instruct\n",
      "  llama2-13b: meta-llama/Llama-2-13b-hf\n",
      "  mistral-7b: mistralai/Mistral-7B-v0.1\n",
      "  phi2: microsoft/phi-2\n",
      "  qwen-7b: Qwen/Qwen-7B\n",
      "  tinyllama: unsloth/Llama-3.2-1B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# huggingface-cli download unsloth/Llama-3.1-8B-Instruct --local-dir ./Llama-3.1-8B-Instruct --local-dir-use-symlinks False\n",
    "\n",
    "# Available models configuration\n",
    "AVAILABLE_MODELS = {\n",
    "    \"llama3.1-8b\": \"unsloth/Llama-3.1-8B-Instruct\",\n",
    "    \"llama2-13b\": \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"mistral-7b\": \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"phi2\": \"microsoft/phi-2\",\n",
    "    \"qwen-7b\": \"Qwen/Qwen-7B\",\n",
    "    \"tinyllama\": \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "}\n",
    "\n",
    "print(\"Available models:\")\n",
    "for key, value in AVAILABLE_MODELS.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Save Model\n",
    "\n",
    "Choose a model to download and save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading unsloth/Llama-3.2-1B-Instruct...\n",
      "Loading tokenizer...\n",
      "Loading model...\n",
      "Saving to ../checkpoints/tinyllama...\n",
      "✓ Successfully saved tinyllama to ../checkpoints/tinyllama\n",
      "Model saved to: ../checkpoints/tinyllama\n"
     ]
    }
   ],
   "source": [
    "def download_and_save_model(model_key, force_download=False):\n",
    "    \"\"\"Download and save model and tokenizer\"\"\"\n",
    "    \n",
    "    if model_key not in AVAILABLE_MODELS:\n",
    "        print(f\"Error: {model_key} not in available models\")\n",
    "        return None, None\n",
    "    \n",
    "    model_name = AVAILABLE_MODELS[model_key]\n",
    "    save_path = CHECKPOINTS_DIR / model_key\n",
    "    \n",
    "    # Check if already exists\n",
    "    if save_path.exists() and not force_download:\n",
    "        print(f\"Model {model_key} already exists at {save_path}\")\n",
    "        print(\"Set force_download=True to re-download\")\n",
    "        return str(save_path), str(save_path)\n",
    "    \n",
    "    print(f\"Downloading {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        print(\"Loading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Load model\n",
    "        print(\"Loading model...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=device if device != 'cpu' else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Save to checkpoints\n",
    "        save_path.mkdir(exist_ok=True)\n",
    "        print(f\"Saving to {save_path}...\")\n",
    "        \n",
    "        model.save_pretrained(save_path)\n",
    "        tokenizer.save_pretrained(save_path)\n",
    "        \n",
    "        print(f\"✓ Successfully saved {model_key} to {save_path}\")\n",
    "        return str(save_path), str(save_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {model_key}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage - change model_key to desired model\n",
    "MODEL_TO_DOWNLOAD = \"tinyllama\" #\"llama3.1-8b\"  # Change this to your desired model\n",
    "\n",
    "model_path, tokenizer_path = download_and_save_model(MODEL_TO_DOWNLOAD)\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Model\n",
    "\n",
    "Load a previously saved model from checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tinyllama from ../checkpoints/tinyllama...\n",
      "✓ Successfully loaded tinyllama\n",
      "Model type: llama\n",
      "Number of layers: 16\n",
      "Parameters: 1235.8M\n"
     ]
    }
   ],
   "source": [
    "def load_saved_model(model_key):\n",
    "    \"\"\"Load model and tokenizer from checkpoints directory\"\"\"\n",
    "    \n",
    "    model_path = CHECKPOINTS_DIR / model_key\n",
    "    \n",
    "    if not model_path.exists():\n",
    "        print(f\"Error: Model {model_key} not found in {CHECKPOINTS_DIR}\")\n",
    "        print(\"Available models:\")\n",
    "        for p in CHECKPOINTS_DIR.iterdir():\n",
    "            if p.is_dir():\n",
    "                print(f\"  - {p.name}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading {model_key} from {model_path}...\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        \n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=device if device != 'cpu' else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Successfully loaded {model_key}\")\n",
    "        print(f\"Model type: {model.config.model_type}\")\n",
    "        print(f\"Number of layers: {len(model.model.layers)}\")\n",
    "        print(f\"Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_key}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage\n",
    "MODEL_TO_LOAD = \"tinyllama\" #\"llama3.1-8b\"  # Change this to your desired model\n",
    "\n",
    "model, tokenizer = load_saved_model(MODEL_TO_LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaDecoderLayer(\n",
       "  (self_attn): LlamaAttention(\n",
       "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "    (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "    (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  )\n",
       "  (mlp): LlamaMLP(\n",
       "    (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "    (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "    (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "    (act_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "  (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = model.model.layers[1]\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": 128009,\n",
       "  \"head_dim\": 64,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8192,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 16,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pad_token_id\": 128004,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 32.0,\n",
       "    \"high_freq_factor\": 4.0,\n",
       "    \"low_freq_factor\": 1.0,\n",
       "    \"original_max_position_embeddings\": 8192,\n",
       "    \"rope_type\": \"llama3\"\n",
       "  },\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.54.0\",\n",
       "  \"unsloth_fixed\": true,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class LlamaDecoderLayer(GradientCheckpointingLayer):\n",
      "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
      "        super().__init__()\n",
      "        self.hidden_size = config.hidden_size\n",
      "\n",
      "        self.self_attn = LlamaAttention(config=config, layer_idx=layer_idx)\n",
      "\n",
      "        self.mlp = LlamaMLP(config)\n",
      "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
      "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Cache] = None,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n",
      "        **kwargs: Unpack[TransformersKwargs],\n",
      "    ) -> tuple[torch.Tensor]:\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.input_layernorm(hidden_states)\n",
      "        # Self Attention\n",
      "        hidden_states, _ = self.self_attn(\n",
      "            hidden_states=hidden_states,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_value=past_key_value,\n",
      "            use_cache=use_cache,\n",
      "            cache_position=cache_position,\n",
      "            position_embeddings=position_embeddings,\n",
      "            **kwargs,\n",
      "        )\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        # Fully Connected\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "        hidden_states = self.mlp(hidden_states)\n",
      "        hidden_states = residual + hidden_states\n",
      "        return hidden_states\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "layer_class = layer.__class__\n",
    "\n",
    "print(inspect.getsource(layer_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = layer.self_attn\n",
    "config = attn.config\n",
    "\n",
    "# Попробуй так:\n",
    "num_heads = getattr(config, 'num_attention_heads', getattr(config, 'n_heads', None))\n",
    "assert num_heads is not None, \"Не найдено число голов в config\"\n",
    "head_dim = config.hidden_size // num_heads\n",
    "\n",
    "# 14-я голова (индекс 13)\n",
    "idx = 13\n",
    "q_proj_weight = attn.q_proj.weight  # [embed_dim, hidden_size]\n",
    "q_weight_14 = q_proj_weight[idx*head_dim:(idx+1)*head_dim, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 2048])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_proj_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2048])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_weight_14.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ridiculous-LLM-Compression (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
